[
  {
    "objectID": "posts/2022-11-20_tf_modelling/index.html",
    "href": "posts/2022-11-20_tf_modelling/index.html",
    "title": "TensorFlow Modelling — Sequential Vs Functional",
    "section": "",
    "text": "“A computer system modeled on the human brain and nervous system.” You can google it and this is the answer you will get. It’s a simple answer but does not explain why it is used in Deep Learning.\n\n\n\n\n\nNeural Network also known as an Artificial Neural Network, mimics the same architecture of our human brain. It’s the core of any Deep Learning or Machine Learning Algorithm. It consists of three essential factors also called Nodes or Layers, that make up the neural network architecture. First, the Input Layer is the entry point for any data. After the data passes through the input layer it moves toward multiple Hidden Layers, this is where patterns are learned by the neural network using weights and biases. Once the network has reached its mathematical conclusion in the hidden layers it passes to the Output Layer.\nNow, that we understand a neural network, how do we practically use them? Here comes TensorFlow, an open-source Python library for Deep Learning. TensorFlow helps us in creating neural network models that are capable to solve different types of deep learning problems, due to the complexity of neural networks TensorFlow uses the Keras library to make it simple and easy to use.\nTensorflow and Keras provide us with two different methods to initiate and implement our neural network architecture, they are as follows:\n\nThe Sequential API Model\nThe Functional API Model\n\n\n\n\n\nFig 1: Sequential API Vs Functional API Architecture\n\n\n\n\n\nThe very first model that TensorFlow teaches us is the Sequential Model because it is the fastest and the simplest way to create a neural network. Now you might be wondering when to use a sequential model. You use it when you have one input and one output, just like everything moves in a sequence, one at a time.\nSequential Model Example:\nimport tensorflow as tf\n\n# Sequential Model \nmodel = tf.keras.Sequential(\n    [\n        # Input Layer\n        tf.keras.Input(shape=(224, 224, 3), name=\"input_layer\"),\n        # 4 Hidden Layers\n        tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_layer1\"),\n        tf.keras.layers.Dense(128, activation=\"relu\", name=\"hidden_layer2\"),\n        tf.keras.layers.Dense(64, activation=\"relu\", name=\"hidden_layer3\"),\n        tf.keras.layers.Dense(32, activation=\"relu\", name=\"hidden_layer4\"),\n        # Output Layer\n        tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"output_layer\")\n    ]\n)\n\n# The above code can also be written using add() method.\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(shape=(224, 224, 3), name=\"input_layer\"))\nmodel.add(tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_layer1\"))\nmodel.add(tf.keras.layers.Dense(128, activation=\"relu\", name=\"hidden_layer2\"))\nmodel.add(tf.keras.layers.Dense(64, activation=\"relu\", name=\"hidden_layer3\"))\nmodel.add(tf.keras.layers.Dense(32, activation=\"relu\", name=\"hidden_layer4\"))\nmodel.add(tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"output_layer\"))\n\n\n\n\nFig. 2: Model Architecture of the above Sequential Model example.\n\n\n\nIf you are seeing this for the first time you might get confused, and questions might arise like what shape, Dense, activation, relu, sigmoid, and multiple different numbers? Let’s understand everything line by line.\n\nThe first line initiates the sequential function within the name ‘model’. You can click and check the documentation for the sequential function.\nWithin the sequential model, there is a list with multiple layers. The first layer is the Input layer with a shape 224, 224, 3. This tells us that the data is an image array, here you can change the shape to whatever your data is been processed in.\nAfter the input layer, there are 4 fully dense hidden layers. In the sequential model, the output of the layer is the input for the next layer. Layers are basically Keras functions that apply certain mathematical calculations to learn patterns. In the hidden layer part, you can even add a sequential model as a layer, this way you can nest a model within a model.\nOnce we get an output from the last hidden layer, it goes through the output layer which is activated using sigmoid(another mathematical calculation for binary classification) this gives us an output result for our problem.\n\n\n\n\nKeras understood that the sequential model had some limitations like it couldn’t handle multiple inputs and outputs. It is not flexible enough to build a neural network that has shared layers or a non-linear branched topology. Although, sequential models were capable to produce great results. It was not capable enough to solve complex problems like object detection in computer vision, speech-to-text recognition, or vice versa, and many more. Finally, Keras came up with Functional API, here everything is isolated and combined however you see fit. A neural network that is now customizable layer-to-layer.\nFunctional API Example:\n# Taking similar example as above and making it more complex\n# importing packages\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, concatenate\n\n# Input Layer\ninputs = Input(shape=(224, 224, 3), name=\"input_layer\")\n\n# Two branches of hidden layers\n# First branch\nlayer_x1 = Dense(256, activation=\"relu\", name=\"hidden_layer1\")(inputs)\nlayer_x2 = Dense(128, activation=\"relu\", name=\"hidden_layer2\")(layer_x1)\n# Second branch\nlayer_y1 = Dense(64, activation=\"relu\", name=\"hidden_layer3\")(inputs)\nlayer_y2 = Dense(32, activation=\"relu\", name=\"hidden_layer4\")(layer_y1)\n# Concatenate layer\ncat_layer = concatenate([layer_x2, layer_y2])\n\n# Output Layer\noutputs = Dense(2, activation=\"sigmoid\", name=\"output_layer\")(cat_layer)\n\n# Defining the model\nmodel = Model(inputs = inputs, outputs = outputs)\n\n\n\n\nFig. 3: Model Architecture of the above Functional API example.\n\n\n\nNow, that you understand sequential architecture, the above functional API example should be easier for you. Most of the layers used in this example are similar to the sequential example. Just the way to represent the model has changed, but this change is powerful enough to build some of the famous pre-trained models(e.g. AlexNet, VGG, ResNet, Densenet, and many more). Let’s understand the changes that are happening in the functional API model.\n\nSimilar to the sequential model example, we initialize the model by importing the packages and creating our first layer — the input layer, but unlike the sequential model, we do not need to use any Keras function and can directly name the layers using a variable.\nNow the fun part starts, we have used 4 hidden layers separated into two different branches. In Functional API, layers communicate by letting them know which layer is connected. If you look at the hidden layer “layer_x1”, at the end of the Dense layer statement it mentions the layer connected i.e. the input layer. This way you can chain any layer with each other and forward pass the model.\nNow that we have created two branches but need the output from a single layer, we can use the concatenate layer. The last layer of the branches is the input for the concatenate layer which is finally connected with the output layer to give us the output probabilities.\nAll the layers are connected, so it is time to define the model, for this we will be using the Keras Model function. The Model function needs two Arguments inputs and outputs. It basically tells the start and end point and the flow of the model is created."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "All My Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nRiding the Waves - PyTorch and Lightning 2.0\n\n\n\n\n\n\n\nPyTorch\n\n\nLightning\n\n\nFabric\n\n\nAI\n\n\nVideo Classification\n\n\n\n\nIn this post, I will be talking about the new release by PyTorch and PyTorch Lightning. Also testing the potential of the new updates.\n\n\n\n\n\n\nApr 1, 2023\n\n\nJohn Pinto\n\n\n39 min\n\n\n\n\n\n\n  \n\n\n\n\nTensorFlow Modelling — Sequential Vs Functional\n\n\n\n\n\n\n\nTensorflow\n\n\nKeras\n\n\nAI\n\n\nNeural Network\n\n\nArchitecture\n\n\n\n\nIn this post, I introduce you to the Tensorflow neural network architectures that will help you in building AI Models.\n\n\n\n\n\n\nNov 20, 2022\n\n\nJohn Pinto\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-11-20_tf_modelling/index.html#the-sequential-api-model",
    "href": "posts/2022-11-20_tf_modelling/index.html#the-sequential-api-model",
    "title": "TensorFlow Modelling — Sequential Vs Functional",
    "section": "",
    "text": "The very first model that TensorFlow teaches us is the Sequential Model because it is the fastest and the simplest way to create a neural network. Now you might be wondering when to use a sequential model. You use it when you have one input and one output, just like everything moves in a sequence, one at a time.\nSequential Model Example:\nimport tensorflow as tf\n\n# Sequential Model \nmodel = tf.keras.Sequential(\n    [\n        # Input Layer\n        tf.keras.Input(shape=(224, 224, 3), name=\"input_layer\"),\n        # 4 Hidden Layers\n        tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_layer1\"),\n        tf.keras.layers.Dense(128, activation=\"relu\", name=\"hidden_layer2\"),\n        tf.keras.layers.Dense(64, activation=\"relu\", name=\"hidden_layer3\"),\n        tf.keras.layers.Dense(32, activation=\"relu\", name=\"hidden_layer4\"),\n        # Output Layer\n        tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"output_layer\")\n    ]\n)\n\n# The above code can also be written using add() method.\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(shape=(224, 224, 3), name=\"input_layer\"))\nmodel.add(tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_layer1\"))\nmodel.add(tf.keras.layers.Dense(128, activation=\"relu\", name=\"hidden_layer2\"))\nmodel.add(tf.keras.layers.Dense(64, activation=\"relu\", name=\"hidden_layer3\"))\nmodel.add(tf.keras.layers.Dense(32, activation=\"relu\", name=\"hidden_layer4\"))\nmodel.add(tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"output_layer\"))\n\n\n\n\nFig. 2: Model Architecture of the above Sequential Model example.\n\n\n\nIf you are seeing this for the first time you might get confused, and questions might arise like what shape, Dense, activation, relu, sigmoid, and multiple different numbers? Let’s understand everything line by line.\n\nThe first line initiates the sequential function within the name ‘model’. You can click and check the documentation for the sequential function.\nWithin the sequential model, there is a list with multiple layers. The first layer is the Input layer with a shape 224, 224, 3. This tells us that the data is an image array, here you can change the shape to whatever your data is been processed in.\nAfter the input layer, there are 4 fully dense hidden layers. In the sequential model, the output of the layer is the input for the next layer. Layers are basically Keras functions that apply certain mathematical calculations to learn patterns. In the hidden layer part, you can even add a sequential model as a layer, this way you can nest a model within a model.\nOnce we get an output from the last hidden layer, it goes through the output layer which is activated using sigmoid(another mathematical calculation for binary classification) this gives us an output result for our problem."
  },
  {
    "objectID": "posts/2022-11-20_tf_modelling/index.html#the-functional-api-model",
    "href": "posts/2022-11-20_tf_modelling/index.html#the-functional-api-model",
    "title": "TensorFlow Modelling — Sequential Vs Functional",
    "section": "",
    "text": "Keras understood that the sequential model had some limitations like it couldn’t handle multiple inputs and outputs. It is not flexible enough to build a neural network that has shared layers or a non-linear branched topology. Although, sequential models were capable to produce great results. It was not capable enough to solve complex problems like object detection in computer vision, speech-to-text recognition, or vice versa, and many more. Finally, Keras came up with Functional API, here everything is isolated and combined however you see fit. A neural network that is now customizable layer-to-layer.\nFunctional API Example:\n# Taking similar example as above and making it more complex\n# importing packages\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, concatenate\n\n# Input Layer\ninputs = Input(shape=(224, 224, 3), name=\"input_layer\")\n\n# Two branches of hidden layers\n# First branch\nlayer_x1 = Dense(256, activation=\"relu\", name=\"hidden_layer1\")(inputs)\nlayer_x2 = Dense(128, activation=\"relu\", name=\"hidden_layer2\")(layer_x1)\n# Second branch\nlayer_y1 = Dense(64, activation=\"relu\", name=\"hidden_layer3\")(inputs)\nlayer_y2 = Dense(32, activation=\"relu\", name=\"hidden_layer4\")(layer_y1)\n# Concatenate layer\ncat_layer = concatenate([layer_x2, layer_y2])\n\n# Output Layer\noutputs = Dense(2, activation=\"sigmoid\", name=\"output_layer\")(cat_layer)\n\n# Defining the model\nmodel = Model(inputs = inputs, outputs = outputs)\n\n\n\n\nFig. 3: Model Architecture of the above Functional API example.\n\n\n\nNow, that you understand sequential architecture, the above functional API example should be easier for you. Most of the layers used in this example are similar to the sequential example. Just the way to represent the model has changed, but this change is powerful enough to build some of the famous pre-trained models(e.g. AlexNet, VGG, ResNet, Densenet, and many more). Let’s understand the changes that are happening in the functional API model.\n\nSimilar to the sequential model example, we initialize the model by importing the packages and creating our first layer — the input layer, but unlike the sequential model, we do not need to use any Keras function and can directly name the layers using a variable.\nNow the fun part starts, we have used 4 hidden layers separated into two different branches. In Functional API, layers communicate by letting them know which layer is connected. If you look at the hidden layer “layer_x1”, at the end of the Dense layer statement it mentions the layer connected i.e. the input layer. This way you can chain any layer with each other and forward pass the model.\nNow that we have created two branches but need the output from a single layer, we can use the concatenate layer. The last layer of the branches is the input for the concatenate layer which is finally connected with the output layer to give us the output probabilities.\nAll the layers are connected, so it is time to define the model, for this we will be using the Keras Model function. The Model function needs two Arguments inputs and outputs. It basically tells the start and end point and the flow of the model is created."
  },
  {
    "objectID": "posts/2022-11-20_tf_modelling/index.html#closing",
    "href": "posts/2022-11-20_tf_modelling/index.html#closing",
    "title": "TensorFlow Modelling — Sequential Vs Functional",
    "section": "",
    "text": "We have covered both the Sequential and Functional Model architecture in this article. Experimented with both and understood the pros and cons, with this you are now capable enough to create your neural network. By the way, there’s a third method that I haven’t mentioned in this article. It’s called the Model subclassing method. I do not recommend this for a beginner to begin their journey in deep learning, it is much more complex and difficult to understand however powerful enough to create your own custom layers and models.\nI hope that you enjoyed this article and that it serves you well. Please reach out to me or you can comment down if you run into any trouble with what I wrote. I am more than happy to improve my article so that it can help more people."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello and Welcome to My Blog!",
    "section": "",
    "text": "Hello and Welcome to My Blog!\n\n\n\n\n\n\n\nBlog\n\n\nI enjoy exploring Data Analytics and Machine Learning and picking up new  tricks on my way. I share what I discover and learn.\n\nStart reading\n\n\n\n\n\n\n\nProjects\n\n\nA collection of my ongoing personal data-endeavors and programming projects.\n\n Take me there"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "John Pinto",
    "section": "",
    "text": "👋 Hey there!\n\nI’m John Pinto. I’m a Management and I.T. graduate who is self-motivated and passionate about the field of Data Analytics, Data Visualization and Data Science. I have a great interest in open and reproducible research and deveploment in machine learning and deep learning using open and reproducible tools like Python and R. This is my personal blog where I share on topics that I learn and explore."
  },
  {
    "objectID": "about.html#fa-solid-address-card-about",
    "href": "about.html#fa-solid-address-card-about",
    "title": "John Pinto",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html",
    "title": "Riding the 2.0 Waves, PyTorch and Lightning 2.0",
    "section": "",
    "text": "Quick Facts on PyTorch and Lightning 2.0 Release\nPyTorch and Lightning were released on 15 March, 2023. The main focus being the improvement in the speed, even the release article title says it\n\n“PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever”\n\nyou can read it over here.\nThis speed that they mention comes with just a single line of code. After you have created your model you just use this code and the code modifies your model to perform at it’s best level.\ntorch.compile()\nThey have even mentioned the details while testing the new functionality and how the torch.compile() can improve the speed of the models from different sources [HuggingFace, TIMM and TorchBench].\nTo run this amazing compiled model, PyTorch introduces new technologies - TorchDynamo, AOTAutograd, PrimTorch and TorchInductor. All of this new technologies are working in a flow and they are broken down in three phases - Graph Acquisition, Graph Lowering and Graph Compilation. You can check all of this over here, they have explained complex system in easy to understand way.\n\n\n\n\n\n\nTip\n\n\n\nRead the above linked article, especially the section on technology overview, this will help you understand the hyper-parameters within the torch.compile()."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#now-the-caveats-part-of-the-release",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#now-the-caveats-part-of-the-release",
    "title": "Riding the 2.0 Waves, PyTorch and Lightning 2.0",
    "section": "Now the Caveats Part of the Release",
    "text": "Now the Caveats Part of the Release\nPyTorch and PyTorch Lightning 2.0 are the stable release but there are some information that needs attention.\n\nHardware:"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#caveats-part-of-the-release",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#caveats-part-of-the-release",
    "title": "Riding the Waves - PyTorch and Lightning 2.0",
    "section": "Caveats Part of the Release",
    "text": "Caveats Part of the Release\nPyTorch and PyTorch Lightning 2.0 are the stable release but there are some information that needs attention.\n\nHardware: This speed up performance that the PyTorch team speaks of are based on specific hardware, broadly they have mentioned that NVIDIA Volta and Ampere server class GPUs are capable to produce decent results. So desktop GPUs will need to wait for later releases.\nModel Saving/Exporting: Right now the compile model can only be saved using the model.state_dict() method. You won’t me able to save the object of the model, which returns an error if you try to. You can read the serialization part of the article that I have mentioned above. Along with the save part, team will also introduce torch.export() mode in the later release."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#importing-libraries",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#importing-libraries",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\nTorch version: 2.0.0+cu117\nTorchvision version: 0.15.1+cu117\nLightning version: 2.0.1\nGPU score: (8, 6)\n\n\n\n\ndevice(type='cuda')"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#downloading-the-dataset",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#downloading-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Downloading the Dataset",
    "text": "Downloading the Dataset\nDownloading the data from the dataset origin.\n\n\n[INFO] Downloading data.\n[INFO] Data is been downloaded!\n\n\nExtracting the data in the raw_data directory\n\n\n[INFO] Extracting data.\n[INFO] Data extraction done!\n\n\nExtracting all the raw data in the data directory and deleting the raw data from the system..\n\n\n[INFO] Extracting all the classes data.\n[INFO] Data extraction done for: \"raw_data/clap.rar\"!\n[INFO] Data extraction done for: \"raw_data/talk.rar\"!\n[INFO] Data extraction done for: \"raw_data/smoke.rar\"!\n[INFO] Data extraction done for: \"raw_data/draw_sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/flic_flac.rar\"!\n[INFO] Data extraction done for: \"raw_data/dive.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_bow.rar\"!\n[INFO] Data extraction done for: \"raw_data/drink.rar\"!\n[INFO] Data extraction done for: \"raw_data/hug.rar\"!\n[INFO] Data extraction done for: \"raw_data/pour.rar\"!\n[INFO] Data extraction done for: \"raw_data/pushup.rar\"!\n[INFO] Data extraction done for: \"raw_data/hit.rar\"!\n[INFO] Data extraction done for: \"raw_data/wave.rar\"!\n[INFO] Data extraction done for: \"raw_data/run.rar\"!\n[INFO] Data extraction done for: \"raw_data/walk.rar\"!\n[INFO] Data extraction done for: \"raw_data/fall_floor.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_horse.rar\"!\n[INFO] Data extraction done for: \"raw_data/turn.rar\"!\n[INFO] Data extraction done for: \"raw_data/situp.rar\"!\n[INFO] Data extraction done for: \"raw_data/jump.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/dribble.rar\"!\n[INFO] Data extraction done for: \"raw_data/sit.rar\"!\n[INFO] Data extraction done for: \"raw_data/chew.rar\"!\n[INFO] Data extraction done for: \"raw_data/pick.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick.rar\"!\n[INFO] Data extraction done for: \"raw_data/eat.rar\"!\n[INFO] Data extraction done for: \"raw_data/fencing.rar\"!\n[INFO] Data extraction done for: \"raw_data/swing_baseball.rar\"!\n[INFO] Data extraction done for: \"raw_data/stand.rar\"!\n[INFO] Data extraction done for: \"raw_data/handstand.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb.rar\"!\n[INFO] Data extraction done for: \"raw_data/kiss.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/push.rar\"!\n[INFO] Data extraction done for: \"raw_data/throw.rar\"!\n[INFO] Data extraction done for: \"raw_data/cartwheel.rar\"!\n[INFO] Data extraction done for: \"raw_data/pullup.rar\"!\n[INFO] Data extraction done for: \"raw_data/brush_hair.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb_stairs.rar\"!\n[INFO] Data extraction done for: \"raw_data/laugh.rar\"!\n[INFO] Data extraction done for: \"raw_data/punch.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_bike.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword_exercise.rar\"!\n[INFO] Data extraction done for: \"raw_data/somersault.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_gun.rar\"!\n[INFO] Data extraction done for: \"raw_data/catch.rar\"!\n[INFO] Data extraction done for: \"raw_data/smile.rar\"!\n[INFO] Data extraction done for: \"raw_data/golf.rar\"!\n[INFO] Data extraction done for: \"raw_data/shake_hands.rar\"!\n\n[INFO] Deleted raw data.\n\n\nChecking some info on the data that is been extracted.\n\n\nThere are 51 directories and 0 files in \"data\".\nThere are 0 directories and 143 files in \"data/swing_baseball\".\nThere are 0 directories and 232 files in \"data/run\".\nThere are 0 directories and 105 files in \"data/situp\".\nThere are 0 directories and 162 files in \"data/shake_hands\".\nThere are 0 directories and 107 files in \"data/flic_flac\".\nThere are 0 directories and 127 files in \"data/hit\".\nThere are 0 directories and 102 files in \"data/kiss\".\nThere are 0 directories and 120 files in \"data/talk\".\nThere are 0 directories and 164 files in \"data/drink\".\nThere are 0 directories and 104 files in \"data/wave\".\nThere are 0 directories and 140 files in \"data/somersault\".\nThere are 0 directories and 116 files in \"data/push\".\nThere are 0 directories and 116 files in \"data/ride_horse\".\nThere are 0 directories and 128 files in \"data/kick_ball\".\nThere are 0 directories and 105 files in \"data/golf\".\nThere are 0 directories and 112 files in \"data/shoot_bow\".\nThere are 0 directories and 116 files in \"data/fencing\".\nThere are 0 directories and 102 files in \"data/smile\".\nThere are 0 directories and 145 files in \"data/dribble\".\nThere are 0 directories and 154 files in \"data/stand\".\nThere are 0 directories and 130 files in \"data/clap\".\nThere are 0 directories and 548 files in \"data/walk\".\nThere are 0 directories and 102 files in \"data/throw\".\nThere are 0 directories and 151 files in \"data/jump\".\nThere are 0 directories and 103 files in \"data/shoot_gun\".\nThere are 0 directories and 131 files in \"data/shoot_ball\".\nThere are 0 directories and 102 files in \"data/catch\".\nThere are 0 directories and 103 files in \"data/draw_sword\".\nThere are 0 directories and 142 files in \"data/sit\".\nThere are 0 directories and 127 files in \"data/sword_exercise\".\nThere are 0 directories and 108 files in \"data/climb\".\nThere are 0 directories and 104 files in \"data/pullup\".\nThere are 0 directories and 126 files in \"data/punch\".\nThere are 0 directories and 103 files in \"data/pushup\".\nThere are 0 directories and 127 files in \"data/sword\".\nThere are 0 directories and 108 files in \"data/eat\".\nThere are 0 directories and 109 files in \"data/chew\".\nThere are 0 directories and 128 files in \"data/laugh\".\nThere are 0 directories and 103 files in \"data/ride_bike\".\nThere are 0 directories and 118 files in \"data/hug\".\nThere are 0 directories and 107 files in \"data/cartwheel\".\nThere are 0 directories and 106 files in \"data/pick\".\nThere are 0 directories and 130 files in \"data/kick\".\nThere are 0 directories and 113 files in \"data/handstand\".\nThere are 0 directories and 240 files in \"data/turn\".\nThere are 0 directories and 112 files in \"data/climb_stairs\".\nThere are 0 directories and 127 files in \"data/dive\".\nThere are 0 directories and 107 files in \"data/brush_hair\".\nThere are 0 directories and 106 files in \"data/pour\".\nThere are 0 directories and 109 files in \"data/smoke\".\nThere are 0 directories and 136 files in \"data/fall_floor\"."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#visualizing-the-dataset",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#visualizing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Visualizing the dataset",
    "text": "Visualizing the dataset\nNow that the data is downloaded, lets view the data with the labels."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#preprocessing-the-dataset",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#preprocessing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Preprocessing the Dataset",
    "text": "Preprocessing the Dataset\nCreating pytorch Dataset.\nIn this steps we will be creating a custom dataset class using the functionality provided by PyTorch. 1. Read the video file. 2. Extracting the frames after every distance for a max sequence length. 3. Assigning label index for all the classes. 4. Performing torchvision.transform on the video data. 5. Returning tuple of video and label data as a tensor.\nBefore creating the dataset, first lets clean it. Here we will check whether the videos are readable and the frame count needs to be more than the Sequence Length frames.\n\n\n[]\n\n\n\n\nThere are no files to delete.\n\n\n\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n         [ 2.,  2.,  0.,  ..., 11., 11., 11.],\n         ...,\n         [ 2.,  2.,  0.,  ..., 55., 55., 55.],\n         [ 5.,  5.,  3.,  ..., 62., 62., 62.],\n         [ 5.,  5.,  2.,  ..., 34., 34., 34.]],\n\n        [[ 4.,  4.,  5.,  ...,  2.,  1.,  1.],\n         [ 5.,  5.,  5.,  ...,  3.,  3.,  3.],\n         [ 4.,  4.,  5.,  ..., 17., 17., 17.],\n         ...,\n         [ 2.,  2.,  2.,  ..., 41., 41., 41.],\n         [ 0.,  0.,  1.,  ..., 49., 49., 49.],\n         [ 0.,  0.,  0.,  ..., 21., 21., 21.]],\n\n        [[ 3.,  3.,  4.,  ...,  2.,  1.,  1.],\n         [ 4.,  4.,  4.,  ...,  3.,  3.,  3.],\n         [ 1.,  1.,  1.,  ...,  8.,  8.,  8.],\n         ...,\n         [ 0.,  0.,  0.,  ..., 16., 14., 14.],\n         [ 0.,  0.,  0.,  ..., 28., 28., 28.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([16, 3, 240, 320]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#creating-a-pretrained-model",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#creating-a-pretrained-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating A Pretrained Model",
    "text": "Creating A Pretrained Model\n\n\n==================================================================================================================================\nLayer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n==================================================================================================================================\nMViT (MViT)                                        [1, 3, 16, 224, 224] [1, 20]              --                   Partial\n├─Conv3d (conv_proj)                               [1, 3, 16, 224, 224] [1, 96, 8, 56, 56]   (42,432)             False\n├─PositionalEncoding (pos_encoding)                [1, 25088, 96]       [1, 25089, 96]       (96)                 False\n├─ModuleList (blocks)                              --                   --                   --                   False\n│    └─MultiscaleBlock (0)                         [1, 25089, 96]       [1, 25089, 96]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 25089, 96]       (68,352)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MLP (mlp)                              [1, 25089, 96]       [1, 25089, 96]       (74,208)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    └─MultiscaleBlock (1)                         [1, 25089, 96]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 6273, 192]       (113,280)            False\n│    │    └─Linear (project)                       [1, 25089, 96]       [1, 25089, 192]      (18,624)             False\n│    │    └─Pool (pool_skip)                       [1, 25089, 192]      [1, 6273, 192]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (2)                         [1, 6273, 192]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 6273, 192]       (168,576)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (3)                         [1, 6273, 192]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 1569, 384]       (385,152)            False\n│    │    └─Linear (project)                       [1, 6273, 192]       [1, 6273, 384]       (74,112)             False\n│    │    └─Pool (pool_skip)                       [1, 6273, 384]       [1, 1569, 384]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (4)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (5)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (6)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (7)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (8)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (9)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (10)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (11)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (12)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (13)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (14)                        [1, 1569, 384]       [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 393, 768]        (1,492,608)          False\n│    │    └─Linear (project)                       [1, 1569, 384]       [1, 1569, 768]       (295,680)            False\n│    │    └─Pool (pool_skip)                       [1, 1569, 768]       [1, 393, 768]        --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    └─MultiscaleBlock (15)                        [1, 393, 768]        [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MultiscaleAttention (attn)             [1, 393, 768]        [1, 393, 768]        (2,374,656)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n├─LayerNorm (norm)                                 [1, 393, 768]        [1, 393, 768]        (1,536)              False\n├─Sequential (head)                                [1, 768]             [1, 20]              --                   True\n│    └─Dropout (0)                                 [1, 768]             [1, 768]             --                   --\n│    └─Linear (1)                                  [1, 768]             [1, 20]              15,380               True\n==================================================================================================================================\nTotal params: 34,245,524\nTrainable params: 15,380\nNon-trainable params: 34,230,144\nTotal mult-adds (G): 1.64\n==================================================================================================================================\nInput size (MB): 9.63\nForward/backward pass size (MB): 1658.93\nParams size (MB): 136.46\nEstimated Total Size (MB): 1805.02\n==================================================================================================================================\n\n\n\n\nPretrained Model Transforms:\nVideoClassification(\n    crop_size=[224, 224]\n    resize_size=[256]\n    mean=[0.45, 0.45, 0.45]\n    std=[0.225, 0.225, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 2.4604e+02,  2.3502e+02,  2.2911e+02,  ...,  2.8675e+02,\n           2.8675e+02,  2.8684e+02],\n         [ 2.3070e+02,  2.2561e+02,  2.1813e+02,  ...,  3.1508e+02,\n           3.1508e+02,  3.1508e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1425e+02,\n           3.1425e+02,  3.1425e+02],\n         ...,\n         [ 3.2401e+00,  2.9817e+00, -2.0000e+00,  ...,  1.6333e+02,\n           1.2992e+02,  9.1900e+01],\n         [-1.5833e+00, -1.7232e+00, -2.0000e+00,  ...,  4.4867e+02,\n           3.7443e+02,  2.8330e+02],\n         [ 2.4444e+00,  9.5210e-01, -2.0000e+00,  ...,  6.6754e+02,\n           6.3805e+02,  5.6134e+02]],\n\n        [[ 2.2467e+02,  2.2616e+02,  2.2911e+02,  ...,  3.0883e+02,\n           3.0883e+02,  3.0902e+02],\n         [ 2.2869e+02,  2.2478e+02,  2.1813e+02,  ...,  3.1314e+02,\n           3.1314e+02,  3.1581e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1356e+02,\n           3.1356e+02,  3.1402e+02],\n         ...,\n         [ 9.1056e+00,  6.2105e+00, -2.0000e+00,  ...,  9.9700e+01,\n           7.6676e+01,  2.8844e+01],\n         [ 1.9746e+01,  1.7561e+01, -1.1667e+00,  ...,  2.1729e+02,\n           1.6955e+02,  1.1592e+02],\n         [ 6.6111e+00,  6.6111e+00,  6.6111e+00,  ...,  5.4941e+02,\n           5.2508e+02,  4.3940e+02]],\n\n        [[-2.0000e+00,  5.3684e+00,  2.1125e+01,  ...,  3.2244e+02,\n           3.1922e+02,  3.2077e+02],\n         [-3.9952e-01,  4.0798e+00,  9.3575e+00,  ...,  3.2330e+02,\n           3.1922e+02,  3.1025e+02],\n         [ 2.0259e+00,  2.6776e+00,  5.3857e+00,  ...,  3.3464e+02,\n           3.3047e+02,  3.1790e+02],\n         ...,\n         [ 1.7578e+02,  1.7578e+02,  1.7719e+02,  ...,  2.3105e+02,\n           2.2658e+02,  2.1946e+02],\n         [ 1.7803e+02,  1.7884e+02,  1.8227e+02,  ...,  2.3562e+02,\n           2.2931e+02,  2.2306e+02],\n         [ 1.9990e+02,  2.0851e+02,  2.2140e+02,  ...,  2.2817e+02,\n           2.1964e+02,  2.2187e+02]],\n\n        ...,\n\n        [[ 4.2491e+02,  4.3142e+02,  4.2619e+02,  ...,  1.5639e+01,\n           1.5639e+01,  1.5639e+01],\n         [ 4.0218e+02,  4.0793e+02,  4.1056e+02,  ...,  6.0095e+00,\n           7.7222e+00,  2.3716e+00],\n         [ 4.0126e+02,  4.0792e+02,  4.0992e+02,  ...,  8.7490e+00,\n           1.0639e+01,  2.2439e+00],\n         ...,\n         [ 1.2787e+02,  1.2839e+02,  1.3253e+02,  ...,  9.5269e-01,\n           1.7500e+00,  1.7500e+00],\n         [ 1.3349e+02,  1.2883e+02,  1.3904e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 1.3694e+02,  1.3320e+02,  1.3960e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.5630e+02,  4.6579e+02,  4.6640e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3329e+02,  4.3661e+02,  4.3776e+02,  ...,  1.1714e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4439e+02,  4.4481e+02,  4.4500e+02,  ...,  1.4995e+00,\n          -7.8133e-01, -2.0000e+00],\n         ...,\n         [ 1.4467e+02,  1.5315e+02,  1.7425e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.3056e+00],\n         [ 1.4625e+02,  1.5174e+02,  1.6260e+02,  ..., -2.0000e+00,\n           9.2336e-01,  2.0278e+00],\n         [ 1.6148e+02,  1.6227e+02,  1.5224e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.2527e+02,  4.4359e+02,  4.6669e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3037e+02,  4.3994e+02,  4.5392e+02,  ...,  2.0278e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4064e+02,  4.4451e+02,  4.5152e+02,  ...,  2.4444e+00,\n          -7.8133e-01,  4.9081e-01],\n         ...,\n         [ 1.4709e+02,  1.5459e+02,  1.5781e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.7668e+00],\n         [ 1.4016e+02,  1.5057e+02,  1.5565e+02,  ..., -1.5833e+00,\n           1.3400e+00,  4.5890e-02],\n         [ 1.3139e+02,  1.3590e+02,  1.4467e+02,  ...,  2.4444e+00,\n           2.4444e+00,  5.3965e+00]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([3, 16, 224, 224]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#creating-dataloaders",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#creating-dataloaders",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating DataLoaders",
    "text": "Creating DataLoaders\n\n\nTrain Dataset Length: 1898 and Test Dataset length: 632\n\n\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\n\n\nLabel Distribution and Zero Rule Benchmark\n\n\nTraining Dataset Distribution:\n [(0, 78), (1, 92), (2, 83), (3, 79), (4, 94), (5, 72), (6, 77), (7, 79), (8, 85), (9, 103), (10, 90), (11, 106), (12, 88), (13, 78), (14, 180), (15, 70), (16, 87), (17, 100), (18, 171), (19, 76)]\n\n\n\n\nTesting Dataset Distribution:\n [(0, 30), (1, 32), (2, 24), (3, 23), (4, 29), (5, 32), (6, 30), (7, 24), (8, 27), (9, 27), (10, 17), (11, 35), (12, 37), (13, 24), (14, 58), (15, 33), (16, 29), (17, 26), (18, 60), (19, 27)]\n\n\n\n\nThe Major class in Testing DataLoader is: Class Name: hug, Class Index: 18 and Total Number of Data: 60\nZero Rule Basline Benchmark Accuracy for predicting the major class: 9.62%"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#pytorch---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#pytorch---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch - Model Training and Testing",
    "text": "PyTorch - Model Training and Testing\n\nExp 1: PyTorch Before 2.0 (PyTorch Eager Mode)\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.4238 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5183 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.4201 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5181 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.3959 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.5176 min\n\nTotal Model Training Time: 5.7940 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[INFO] results directory is been created.\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.423781\n0.518263\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.420147\n0.518100\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.395912\n0.517590\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n\n\n\n\n\n\n\nExp 2: PyTorch 2.0 (PyTorch Compile Mode)\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.3885 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0454 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.6187 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5678 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.6008 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5694 min\n\nTotal Model Training Time: 7.7908 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.388515\n1.045425\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.618742\n0.567790\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.600783\n0.569392\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#pytorch-lightning---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#pytorch-lightning---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch Lightning - Model Training and Testing",
    "text": "PyTorch Lightning - Model Training and Testing\n\nExp 3: PyTorch Lightning\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 7.6887 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n\n\n\n\n\n\n\nExp 4: PyTorch Lightning - Compile Mode\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 10.5425 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#lightning-fabric---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#lightning-fabric---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Lightning Fabric - Model training and Testing",
    "text": "Lightning Fabric - Model training and Testing\n\nExp 5: Fabric Lightning\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5801 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5829 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.5967 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5939 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5900 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.6074 min\n\nTotal Model Training Time: 6.5512 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.580098\n0.582881\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.596745\n0.593947\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.590004\n0.607377\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n\n\n\n\n\n\n\nExp 6: Fabric Lightning - Compile mode\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.1276 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0502 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.7023 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5721 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.5845 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5688 min\n\nTotal Model Training Time: 7.6056 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.127585\n1.050178\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.702294\n0.572100\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.584458\n0.568837\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n\n\n\n\n\n\n\nExp 7: Fabric + Precision\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5438 min | test loss: 2.6781 | test_acc: 0.4519 | test_time: 0.5552 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4168 | train_time: 1.5081 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5554 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5786 min | test loss: 2.2094 | test_acc: 0.5817 | test_time: 0.5744 min\n\nTotal Model Training Time: 6.3157 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853492\n0.198623\n2.678145\n0.451923\n1.543753\n0.555179\n\n\n1\n2.599140\n0.416843\n2.427695\n0.540064\n1.508068\n0.555399\n\n\n2\n2.360222\n0.523835\n2.209447\n0.581731\n1.578585\n0.574411\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n\n\n\n\n\n\n\nExp 8: Fabric + Precision + Compile\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 3.0326 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.1394 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.5740 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5764 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5095 | train_time: 1.5562 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5639 min\n\nTotal Model Training Time: 8.4427 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845062\n0.201271\n2.678693\n0.450321\n3.032648\n1.139419\n\n\n1\n2.590762\n0.445975\n2.427031\n0.548077\n1.574046\n0.576357\n\n\n2\n2.366282\n0.509534\n2.209138\n0.580128\n1.556243\n0.563871\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n7\nFabric + Precision + Compile\n8.442741"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Training and Testing on Full Dataset and Deploying the Model",
    "text": "Training and Testing on Full Dataset and Deploying the Model\n\nCreating the model\n\n\nGetting the Full Dataset and Dataloaders\n\n\nTrain Dataset Length: 5075 and Test Dataset length: 1691\nDataloaders Length: 317 for a batch size of: 16\nDataloaders Length: 105 for a batch size of: 16\n\n\n\n\nTraining Model for Full Dataset\n\n\n\n\n\n\n\n\n\n\n\nEpoch: [1/10] | train_loss: 2.6020 | train_acc: 0.4720 | train_time: 4.8366 min | test loss: 1.8906 | test_acc: 0.6238 | test_time: 1.6950 min\nEpoch: [2/10] | train_loss: 1.8662 | train_acc: 0.6327 | train_time: 5.4221 min | test loss: 1.7076 | test_acc: 0.6607 | test_time: 1.5391 min\nEpoch: [3/10] | train_loss: 1.7319 | train_acc: 0.6642 | train_time: 4.8773 min | test loss: 1.6484 | test_acc: 0.6756 | test_time: 1.5405 min\nEpoch: [4/10] | train_loss: 1.6819 | train_acc: 0.6733 | train_time: 4.5695 min | test loss: 1.6199 | test_acc: 0.6887 | test_time: 1.5152 min\nEpoch: [5/10] | train_loss: 1.6426 | train_acc: 0.6936 | train_time: 4.3718 min | test loss: 1.6051 | test_acc: 0.6970 | test_time: 1.5019 min\nEpoch: [6/10] | train_loss: 1.6085 | train_acc: 0.6981 | train_time: 4.7581 min | test loss: 1.5919 | test_acc: 0.7000 | test_time: 1.5833 min\nEpoch: [7/10] | train_loss: 1.5967 | train_acc: 0.7049 | train_time: 4.9686 min | test loss: 1.5807 | test_acc: 0.7071 | test_time: 1.5994 min\nEpoch: [8/10] | train_loss: 1.5680 | train_acc: 0.7173 | train_time: 6.3247 min | test loss: 1.5763 | test_acc: 0.7137 | test_time: 1.5196 min\nEpoch: [9/10] | train_loss: 1.5577 | train_acc: 0.7179 | train_time: 5.6291 min | test loss: 1.5734 | test_acc: 0.7083 | test_time: 1.5351 min\nEpoch: [10/10] | train_loss: 1.5540 | train_acc: 0.7194 | train_time: 5.6172 min | test loss: 1.5605 | test_acc: 0.7185 | test_time: 1.5643 min\n\nTotal Model Training Time: 66.9688 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStoring Files for Demo App.\n\n\n[INFO] \"demo/human_action_recognition\" and \"demo/human_action_recognition/examples\" directory is been created.\n\n\n\n\ndata/shoot_bow/6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_3.avi File is been copied.\ndata/golf/Golf_Tips_-_Hit_The_Driver_300+_Yards!!!_golf_f_nm_np1_fr_med_0.avi File is been copied.\ndata/climb/(HQ)_Rock_Climbing_-_Free_Solo_Speed_Climb_-_Dan_Osman_climb_f_cm_np1_le_med_2.avi File is been copied.\ndata/punch/AdvancedBoxingTechniquesandExercises_punch_u_nm_np1_ba_med_23.avi File is been copied.\ndata/swing_baseball/BaseballHitinSlowMotion_swing_baseball_f_nm_np1_fr_bad_0.avi File is been copied.\n\n\n\n\n[INFO] Saving class names to: \"demo/human_action_recognition/class_names.txt\"\n\n\n\n\n['smoke', 'dive', 'cartwheel', 'throw', 'hit']\n\n\n\n\nSaving and Loading the Model\n\n\n[INFO] Model is been saved to: \"demo/human_action_recognition/mvit_v2_pretrained_model_hmdb51.pth\"\n\n\n\n\n&lt;All keys matched successfully&gt;\n\n\n\n\nCreating a text file for Modules and Packages\n\n\nWriting demo/human_action_recognition/requirements.txt\n\n\n\n\nCreating a Python Script to Preprocess the Data\n\n\nWriting demo/human_action_recognition/utils.py\n\n\n\n\nCreating a Python Script to Create the Model\n\n\nWriting demo/human_action_recognition/model.py\n\n\n\n\nCreating a Python Script for Gradio App\n\n\nWriting demo/human_action_recognition/app.py\n\n\n\n\nDeploying on HuggingFace"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#results",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#results",
    "title": "Riding the Waves - PyTorch and Lightning 2.0",
    "section": "Results",
    "text": "Results\nSo now that all the experiments are completed, it’s time to check the overall comparison of all the experiment and which one has performed the best.\n\n\n\n\n\n\nNvidia RTX A4000\n\n\n\n\n\n\n\nNvidia Tesla T4"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#caveats-part-of-the-release",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#caveats-part-of-the-release",
    "title": "Riding the Waves - PyTorch and Lightning 2.0",
    "section": "Caveats Part of the Release",
    "text": "Caveats Part of the Release\nPyTorch and PyTorch Lightning 2.0 are stable releases but there is some information that needs attention.\n\nHardware: This speed-up performance that the PyTorch team speaks of is based on specific hardware, broadly they have mentioned that NVIDIA Volta and Ampere server-class GPUs are capable to produce decent results. So desktop GPUs will need to wait for later releases.\nModel Saving/Exporting: Right now the compiled model can only be saved using the model.state_dict() method. You won’t be able to save the object of the model, which returns an error if you try to. You can read the serialization part of the article that I have mentioned above. Along with the save part, the team will also introduce torch.export() mode in the later release."
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/blog_figures.html",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/blog_figures.html",
    "title": "John Pinto",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nexp1, exp2 = pd.read_csv('/content/exp1_results.csv'), pd.read_csv('/content/exp2_results.csv')\nexp3, exp4 = pd.read_csv('/content/lightning.csv'), pd.read_csv('/content/lightning_compile.csv')\nexp5, exp6 = pd.read_csv('/content/exp5_results.csv'), pd.read_csv('/content/exp6_results.csv')\nexp7, exp8 = pd.read_csv('/content/exp7_results.csv'), pd.read_csv('/content/exp8_results.csv')\n\n\nepochs = np.array(range(len(exp1)))\nlabel_fs = 14\ntitle_fs = 16\nstitle_fs = 18\n\n\nplt.figure(figsize=(25, 8))\nplt.subplot(1, 3, 1)\nplt.plot(epochs, exp1['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp1['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp2['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp2['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 2)\nplt.plot(epochs, exp1['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp1['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp2['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp2['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 3)\nplt.plot(epochs, exp1['train_epoch_time(min)'], label='eager_train_time')\nplt.plot(epochs, exp1['test_epoch_time(min)'], label='eager_val_time')\nplt.plot(epochs, exp2['train_epoch_time(min)'], label='compile_train_time')\nplt.plot(epochs, exp2['test_epoch_time(min)'], label='compile_val_time')\nplt.title('Time (min)', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend()\nplt.suptitle('PyTorch Eager and Compile Mode', fontsize=stitle_fs, fontweight='bold');\n\n\n\n\nFigure 1: Benchmark Comparison between PyTorch Eager and Compile Mode for 3 Epochs.\n\n\n\n\n\nexp3_dict = []\nfor i, dfg in exp3.groupby('epoch'):\n    agg = dict(dfg.mean())\n    agg['epoch'] = i\n    exp3_dict.append(agg)\nexp4_dict = []\nfor i, dfg in exp3.groupby('epoch'):\n    agg = dict(dfg.mean())\n    agg['epoch'] = i\n    exp4_dict.append(agg)\nexp3, exp4 = pd.DataFrame(exp3_dict), pd.DataFrame(exp4_dict)\n\n\nplt.figure(figsize=(20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, exp3['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp3['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp4['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp4['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=label_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 2, 2)\nplt.plot(epochs, exp3['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp3['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp4['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp4['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=label_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.suptitle('PyTorch Lightning Eager and Compile Mode', fontsize=title_fs, fontweight='bold');\n\n\n\n\nFigure 2: Benchmark Comparison between PyTorch Lightning Eager and Compile Mode for 3 Epochs.\n\n\n\n\n\nplt.figure(figsize=(25, 8))\nplt.subplot(1, 3, 1)\nplt.plot(epochs, exp5['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp5['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp6['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp6['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 2)\nplt.plot(epochs, exp5['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp5['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp6['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp6['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 3)\nplt.plot(epochs, exp5['train_epoch_time(min)'], label='eager_train_time')\nplt.plot(epochs, exp5['test_epoch_time(min)'], label='eager_val_time')\nplt.plot(epochs, exp6['train_epoch_time(min)'], label='compile_train_time')\nplt.plot(epochs, exp6['test_epoch_time(min)'], label='compile_val_time')\nplt.title('Time (min)', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend()\nplt.suptitle('Lightning Fabric Eager and Compile Mode', fontsize=stitle_fs, fontweight='bold');\n\n\n\n\nFigure 3: Benchmark Comparison between Lightning Fabric Eager and Compile Mode for 3 Epochs.\n\n\n\n\n\nplt.figure(figsize=(25, 8))\nplt.subplot(1, 3, 1)\nplt.plot(epochs, exp7['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp7['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp8['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp8['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 2)\nplt.plot(epochs, exp7['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp7['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp8['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp8['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 3)\nplt.plot(epochs, exp7['train_epoch_time(min)'], label='eager_train_time')\nplt.plot(epochs, exp7['test_epoch_time(min)'], label='eager_val_time')\nplt.plot(epochs, exp8['train_epoch_time(min)'], label='compile_train_time')\nplt.plot(epochs, exp8['test_epoch_time(min)'], label='compile_val_time')\nplt.title('Time (min)', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend()\nplt.suptitle('Lightning Fabric (TF16 Mixed Precision) Eager and Compile Mode', fontsize=stitle_fs, fontweight='bold');\n\n\n\n\nFigure 4: Benchmark Comparison between Lightning Fabric (TF16 Mixed Precision) Eager and Compile Mode for 3 Epochs."
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/blog_pytorch2_0.html",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/blog_pytorch2_0.html",
    "title": "John Pinto",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nexp1, exp2 = pd.read_csv('/content/exp1_results.csv'), pd.read_csv('/content/exp2_results.csv')\nexp3, exp4 = pd.read_csv('/content/lightning.csv'), pd.read_csv('/content/lightning_compile.csv')\nexp5, exp6 = pd.read_csv('/content/exp5_results.csv'), pd.read_csv('/content/exp6_results.csv')\nexp7, exp8 = pd.read_csv('/content/exp7_results.csv'), pd.read_csv('/content/exp8_results.csv')\n\n\nepochs = np.array(range(len(exp1)))\nlabel_fs = 14\ntitle_fs = 16\nstitle_fs = 18\n\n\nplt.figure(figsize=(25, 8))\nplt.subplot(1, 3, 1)\nplt.plot(epochs, exp1['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp1['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp2['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp2['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 2)\nplt.plot(epochs, exp1['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp1['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp2['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp2['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 3)\nplt.plot(epochs, exp1['train_epoch_time(min)'], label='eager_train_time')\nplt.plot(epochs, exp1['test_epoch_time(min)'], label='eager_val_time')\nplt.plot(epochs, exp2['train_epoch_time(min)'], label='compile_train_time')\nplt.plot(epochs, exp2['test_epoch_time(min)'], label='compile_val_time')\nplt.title('Time (min)', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend()\nplt.suptitle('PyTorch Eager and Compile Mode', fontsize=stitle_fs, fontweight='bold');\n\n\n\n\nFigure 1: Benchmark Comparison between PyTorch Eager and Compile Mode for 3 Epochs.\n\n\n\n\n\nexp3_dict = []\nfor i, dfg in exp3.groupby('epoch'):\n    agg = dict(dfg.mean())\n    agg['epoch'] = i\n    exp3_dict.append(agg)\nexp4_dict = []\nfor i, dfg in exp3.groupby('epoch'):\n    agg = dict(dfg.mean())\n    agg['epoch'] = i\n    exp4_dict.append(agg)\nexp3, exp4 = pd.DataFrame(exp3_dict), pd.DataFrame(exp4_dict)\n\n\nplt.figure(figsize=(20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, exp3['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp3['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp4['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp4['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=label_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 2, 2)\nplt.plot(epochs, exp3['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp3['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp4['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp4['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=label_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.suptitle('PyTorch Lightning Eager and Compile Mode', fontsize=title_fs, fontweight='bold');\n\n\n\n\nFigure 2: Benchmark Comparison between PyTorch Lightning Eager and Compile Mode for 3 Epochs.\n\n\n\n\n\nplt.figure(figsize=(25, 8))\nplt.subplot(1, 3, 1)\nplt.plot(epochs, exp5['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp5['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp6['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp6['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 2)\nplt.plot(epochs, exp5['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp5['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp6['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp6['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 3)\nplt.plot(epochs, exp5['train_epoch_time(min)'], label='eager_train_time')\nplt.plot(epochs, exp5['test_epoch_time(min)'], label='eager_val_time')\nplt.plot(epochs, exp6['train_epoch_time(min)'], label='compile_train_time')\nplt.plot(epochs, exp6['test_epoch_time(min)'], label='compile_val_time')\nplt.title('Time (min)', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend()\nplt.suptitle('Lightning Fabric Eager and Compile Mode', fontsize=stitle_fs, fontweight='bold');\n\n\n\n\nFigure 3: Benchmark Comparison between Lightning Fabric Eager and Compile Mode for 3 Epochs.\n\n\n\n\n\nplt.figure(figsize=(25, 8))\nplt.subplot(1, 3, 1)\nplt.plot(epochs, exp7['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp7['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp8['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp8['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 2)\nplt.plot(epochs, exp7['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp7['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp8['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp8['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 3)\nplt.plot(epochs, exp7['train_epoch_time(min)'], label='eager_train_time')\nplt.plot(epochs, exp7['test_epoch_time(min)'], label='eager_val_time')\nplt.plot(epochs, exp8['train_epoch_time(min)'], label='compile_train_time')\nplt.plot(epochs, exp8['test_epoch_time(min)'], label='compile_val_time')\nplt.title('Time (min)', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend()\nplt.suptitle('Lightning Fabric (TF16 Mixed Precision) Eager and Compile Mode', fontsize=stitle_fs, fontweight='bold');\n\n\n\n\nFigure 4: Benchmark Comparison between Lightning Fabric (TF16 Mixed Precision) Eager and Compile Mode for 3 Epochs."
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "",
    "text": "!pip install -U -q -r requirements.txt\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport os\nimport shutil\nimport rarfile\nimport random\nimport requests\nimport numpy as np\nimport pandas as pd\nimport gradio as gr\nfrom tqdm.auto import tqdm\nfrom timeit import default_timer as timer\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\nimport cv2\nfrom IPython.display import Video, IFrame\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torchmetrics\nfrom torchinfo import summary\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, utils\nfrom torchvision.io import read_video\nfrom torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights\nimport lightning as L\nfrom lightning.fabric import Fabric\n\n\nprint(f'Torch version: {torch.__version__}')\nprint(f'Torchvision version: {torchvision.__version__}')\nprint(f'Lightning version: {L.__version__}')\nprint(f'GPU score: {torch.cuda.get_device_capability()}')\n\nTorch version: 2.0.0+cu117\nTorchvision version: 0.15.1+cu117\nLightning version: 2.0.1\nGPU score: (8, 6)\n\n\n\n# Setting seed for randomness\ndef set_seed(seed: int = 42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\nset_seed(42)\n\n# setting the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cuda')"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#importing-libraries",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#importing-libraries",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "",
    "text": "!pip install -U -q -r requirements.txt\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport os\nimport shutil\nimport rarfile\nimport random\nimport requests\nimport numpy as np\nimport pandas as pd\nimport gradio as gr\nfrom tqdm.auto import tqdm\nfrom timeit import default_timer as timer\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\nimport cv2\nfrom IPython.display import Video, IFrame\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torchmetrics\nfrom torchinfo import summary\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, utils\nfrom torchvision.io import read_video\nfrom torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights\nimport lightning as L\nfrom lightning.fabric import Fabric\n\n\nprint(f'Torch version: {torch.__version__}')\nprint(f'Torchvision version: {torchvision.__version__}')\nprint(f'Lightning version: {L.__version__}')\nprint(f'GPU score: {torch.cuda.get_device_capability()}')\n\nTorch version: 2.0.0+cu117\nTorchvision version: 0.15.1+cu117\nLightning version: 2.0.1\nGPU score: (8, 6)\n\n\n\n# Setting seed for randomness\ndef set_seed(seed: int = 42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\nset_seed(42)\n\n# setting the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cuda')"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#downloading-the-dataset",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#downloading-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Downloading the Dataset",
    "text": "Downloading the Dataset\nDownloading the data from the dataset origin.\n\nDATA_URL = 'http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar'\nRAW_DATA_PATH = 'raw_data'\nDATA_PATH = 'data'\nHMDB_DATA_PATH = os.path.join(DATA_PATH, 'HMDB51')\n\n\nif os.path.isfile('hmdb51_org.rar'):\n    print('[INFO] Data already exists!')\nelse:\n    print('[INFO] Downloading data.')\n    r = requests.get(DATA_URL)\n    with open('hmdb51_org.rar', 'wb') as file:\n        file.write(r.content)\n        file.close()\n    print('[INFO] Data is been downloaded!')\n\n[INFO] Downloading data.\n[INFO] Data is been downloaded!\n\n\nExtracting the data in the raw_data directory\n\nif os.path.exists(RAW_DATA_PATH):\n    print('[INFO] Data path already exists!')\nelse:\n    print('[INFO] Extracting data.')\n    os.mkdir(RAW_DATA_PATH)\n    r = rarfile.RarFile('hmdb51_org.rar')\n    r.extractall(RAW_DATA_PATH)\n    r.close()\n    print('[INFO] Data extraction done!')\n\n[INFO] Extracting data.\n[INFO] Data extraction done!\n\n\nExtracting all the raw data in the data directory and deleting the raw data from the system..\n\nif os.path.exists(DATA_PATH):\n    print('[INFO] Data path already exists!')\nelse:\n    print('[INFO] Extracting all the classes data.')\n    os.mkdir(DATA_PATH)\n    for data in os.listdir(RAW_DATA_PATH):\n        r = rarfile.RarFile(os.path.join(RAW_DATA_PATH, data))\n        r.extractall(DATA_PATH)\n        r.close()\n        print(f'[INFO] Data extraction done for: \"{os.path.join(RAW_DATA_PATH, data)}\"!')\n    os.remove('hmdb51_org.rar')\n    shutil.rmtree(RAW_DATA_PATH)\n    print(f'\\n[INFO] Deleted raw data.')\n\n[INFO] Extracting all the classes data.\n[INFO] Data extraction done for: \"raw_data/clap.rar\"!\n[INFO] Data extraction done for: \"raw_data/talk.rar\"!\n[INFO] Data extraction done for: \"raw_data/smoke.rar\"!\n[INFO] Data extraction done for: \"raw_data/draw_sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/flic_flac.rar\"!\n[INFO] Data extraction done for: \"raw_data/dive.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_bow.rar\"!\n[INFO] Data extraction done for: \"raw_data/drink.rar\"!\n[INFO] Data extraction done for: \"raw_data/hug.rar\"!\n[INFO] Data extraction done for: \"raw_data/pour.rar\"!\n[INFO] Data extraction done for: \"raw_data/pushup.rar\"!\n[INFO] Data extraction done for: \"raw_data/hit.rar\"!\n[INFO] Data extraction done for: \"raw_data/wave.rar\"!\n[INFO] Data extraction done for: \"raw_data/run.rar\"!\n[INFO] Data extraction done for: \"raw_data/walk.rar\"!\n[INFO] Data extraction done for: \"raw_data/fall_floor.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_horse.rar\"!\n[INFO] Data extraction done for: \"raw_data/turn.rar\"!\n[INFO] Data extraction done for: \"raw_data/situp.rar\"!\n[INFO] Data extraction done for: \"raw_data/jump.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/dribble.rar\"!\n[INFO] Data extraction done for: \"raw_data/sit.rar\"!\n[INFO] Data extraction done for: \"raw_data/chew.rar\"!\n[INFO] Data extraction done for: \"raw_data/pick.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick.rar\"!\n[INFO] Data extraction done for: \"raw_data/eat.rar\"!\n[INFO] Data extraction done for: \"raw_data/fencing.rar\"!\n[INFO] Data extraction done for: \"raw_data/swing_baseball.rar\"!\n[INFO] Data extraction done for: \"raw_data/stand.rar\"!\n[INFO] Data extraction done for: \"raw_data/handstand.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb.rar\"!\n[INFO] Data extraction done for: \"raw_data/kiss.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/push.rar\"!\n[INFO] Data extraction done for: \"raw_data/throw.rar\"!\n[INFO] Data extraction done for: \"raw_data/cartwheel.rar\"!\n[INFO] Data extraction done for: \"raw_data/pullup.rar\"!\n[INFO] Data extraction done for: \"raw_data/brush_hair.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb_stairs.rar\"!\n[INFO] Data extraction done for: \"raw_data/laugh.rar\"!\n[INFO] Data extraction done for: \"raw_data/punch.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_bike.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword_exercise.rar\"!\n[INFO] Data extraction done for: \"raw_data/somersault.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_gun.rar\"!\n[INFO] Data extraction done for: \"raw_data/catch.rar\"!\n[INFO] Data extraction done for: \"raw_data/smile.rar\"!\n[INFO] Data extraction done for: \"raw_data/golf.rar\"!\n[INFO] Data extraction done for: \"raw_data/shake_hands.rar\"!\n\n[INFO] Deleted raw data.\n\n\nChecking some info on the data that is been extracted.\n\nfor dirpath, dirnames, filenames in os.walk(DATA_PATH):\n    print(f'There are {len(dirnames)} directories and {len(filenames)} files in \"{dirpath}\".')\n\nThere are 51 directories and 0 files in \"data\".\nThere are 0 directories and 143 files in \"data/swing_baseball\".\nThere are 0 directories and 232 files in \"data/run\".\nThere are 0 directories and 105 files in \"data/situp\".\nThere are 0 directories and 162 files in \"data/shake_hands\".\nThere are 0 directories and 107 files in \"data/flic_flac\".\nThere are 0 directories and 127 files in \"data/hit\".\nThere are 0 directories and 102 files in \"data/kiss\".\nThere are 0 directories and 120 files in \"data/talk\".\nThere are 0 directories and 164 files in \"data/drink\".\nThere are 0 directories and 104 files in \"data/wave\".\nThere are 0 directories and 140 files in \"data/somersault\".\nThere are 0 directories and 116 files in \"data/push\".\nThere are 0 directories and 116 files in \"data/ride_horse\".\nThere are 0 directories and 128 files in \"data/kick_ball\".\nThere are 0 directories and 105 files in \"data/golf\".\nThere are 0 directories and 112 files in \"data/shoot_bow\".\nThere are 0 directories and 116 files in \"data/fencing\".\nThere are 0 directories and 102 files in \"data/smile\".\nThere are 0 directories and 145 files in \"data/dribble\".\nThere are 0 directories and 154 files in \"data/stand\".\nThere are 0 directories and 130 files in \"data/clap\".\nThere are 0 directories and 548 files in \"data/walk\".\nThere are 0 directories and 102 files in \"data/throw\".\nThere are 0 directories and 151 files in \"data/jump\".\nThere are 0 directories and 103 files in \"data/shoot_gun\".\nThere are 0 directories and 131 files in \"data/shoot_ball\".\nThere are 0 directories and 102 files in \"data/catch\".\nThere are 0 directories and 103 files in \"data/draw_sword\".\nThere are 0 directories and 142 files in \"data/sit\".\nThere are 0 directories and 127 files in \"data/sword_exercise\".\nThere are 0 directories and 108 files in \"data/climb\".\nThere are 0 directories and 104 files in \"data/pullup\".\nThere are 0 directories and 126 files in \"data/punch\".\nThere are 0 directories and 103 files in \"data/pushup\".\nThere are 0 directories and 127 files in \"data/sword\".\nThere are 0 directories and 108 files in \"data/eat\".\nThere are 0 directories and 109 files in \"data/chew\".\nThere are 0 directories and 128 files in \"data/laugh\".\nThere are 0 directories and 103 files in \"data/ride_bike\".\nThere are 0 directories and 118 files in \"data/hug\".\nThere are 0 directories and 107 files in \"data/cartwheel\".\nThere are 0 directories and 106 files in \"data/pick\".\nThere are 0 directories and 130 files in \"data/kick\".\nThere are 0 directories and 113 files in \"data/handstand\".\nThere are 0 directories and 240 files in \"data/turn\".\nThere are 0 directories and 112 files in \"data/climb_stairs\".\nThere are 0 directories and 127 files in \"data/dive\".\nThere are 0 directories and 107 files in \"data/brush_hair\".\nThere are 0 directories and 106 files in \"data/pour\".\nThere are 0 directories and 109 files in \"data/smoke\".\nThere are 0 directories and 136 files in \"data/fall_floor\"."
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#visualizing-the-dataset",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#visualizing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Visualizing the dataset",
    "text": "Visualizing the dataset\nNow that the data is downloaded, lets view the data with the labels.\n\nplt.figure(figsize=(20, 20))\nCLASSES = sorted(os.listdir(DATA_PATH))\n\n# Selecting 20 random categories for visualization\nrandom_range = random.sample(range(len(CLASSES)), 20)\n\nfor i, rand_i in enumerate(random_range, 1):\n    class_name = CLASSES[rand_i]\n\n    # Looking for the video files in the category and selecting it randomly\n    video_files_list = os.listdir(os.path.join(DATA_PATH, class_name))\n    video_file_name = random.choice(video_files_list)\n\n    # Reading the video file and displaying the first frame with label.\n    video_reader = cv2.VideoCapture(os.path.join(DATA_PATH, class_name, video_file_name))\n    _, frame = video_reader.read()\n    video_reader.release()\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    cv2.putText(rgb_frame, class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n    plt.subplot(5, 4, i)\n    plt.title(f'Category: {class_name},\\nShape: {rgb_frame.shape}')\n    plt.imshow(rgb_frame)\n    plt.axis(False);"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#preprocessing-the-dataset",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#preprocessing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Preprocessing the Dataset",
    "text": "Preprocessing the Dataset\nCreating pytorch Dataset.\nIn this steps we will be creating a custom dataset class using the functionality provided by PyTorch. 1. Read the video file. 2. Extracting the frames after every distance for a max sequence length. 3. Assigning label index for all the classes. 4. Performing torchvision.transform on the video data. 5. Returning tuple of video and label data as a tensor.\n\n# Defining variables\nSEQUENCE_LENGTH = 16 # Frames feeded to the model as a single sequence\nNUM_CLASSES = 20 # Selecting 20 classes from 51 classes\n\nBefore creating the dataset, first lets clean it. Here we will check whether the videos are readable and the frame count needs to be more than the Sequence Length frames.\n\nfiles_list = [os.path.join(DATA_PATH, dir, video) for dir in CLASSES for video in os.listdir(DATA_PATH + \"/\" + dir)]\nremove_file_list = []\nfor file in files_list:\n    vframes, _, _ = read_video(filename=file, pts_unit='sec', output_format='TCHW')\n    if len(vframes) &lt;= SEQUENCE_LENGTH:\n        print(f'File Name: {file} and Total Frames Count: {len(vframes)}')\n        remove_file_list.append(file)\nremove_file_list\n\n[]\n\n\n\n# Deleting the video files\nif len(remove_file_list) &gt; 0:\n    for file_path in remove_file_list:\n        try: \n            os.remove(path=file_path)\n            print(f'File \"{file_path}\" is been deleted')\n        except FileNotFoundError:\n            print('File is missing or is been already deleted')\nelse:\n    print('There are no files to delete.')\n\nThere are no files to delete.\n\n\n\nclass HumanActionDataset(Dataset):\n    \"\"\"\n    A class to create a PyTorch dataset using the video files directory,\n    transforming the video data and returning the data and labels for every index.\n    \n    Parameters:\n        video_dir: str, A string containing the path for all the video categories or classes.\n        seq_len: int(default: 20), A integer specifing the required total sequence frames.\n        num_classes: int(default: 20), Selecting random classes and the max allowed is 51 classes for this dataset.\n        transform: default: None, A torchvision transforms for applying multiple transformation on every frame.\n    Returns:\n        selected_frame, label: tuple, A tuple containing the video \"TCHW\" and label data in tensor format. \n    \"\"\"\n    def __init__(self, video_dir: str, seq_len: int, num_classes: int, transform=None):\n        self.video_dir = video_dir\n        self.seq_len = seq_len\n        self.num_classes = num_classes\n        self.transform = transform \n        \n        # Creating a list of all the video files path and classes.\n        set_seed(seed=42)\n        if self.num_classes &gt; 51:\n            self.num_classes = 51\n        self.classes = random.sample(sorted(os.listdir(video_dir)), k=self.num_classes)\n        self.files_list = [os.path.join(video_dir, dir, video) for dir in self.classes for video in os.listdir(video_dir + \"/\" + dir)]\n        \n    def __len__(self):\n        return len(self.files_list)\n    \n    def __getitem__(self, index):\n        video_path = self.files_list[index]\n        \n        # Reading the video file\n        vframes, _, _ = read_video(filename=video_path, pts_unit='sec', output_format='TCHW')\n        vframes = vframes.type(torch.float32)\n        vframes_count = len(vframes)\n        \n        # Selecting frames at certain interval\n        skip_frames = max(int(vframes_count/self.seq_len), 1)\n        selected_frame = vframes[0].unsqueeze(0)\n        \n        # Creating a new sequence of frames upto the defined sequence length.\n        for i in range(1, self.seq_len):\n            selected_frame = torch.concat((selected_frame, vframes[i * skip_frames].unsqueeze(0)))\n            \n        # Video label as per the classes list.\n        label = torch.tensor(self.classes.index(video_path.split('/')[1]))\n        \n        # Applying transformation to the frames\n        if self.transform:\n            return self.transform(selected_frame), label\n        else:\n            return selected_frame, label\n\n\n# Creating the dataset\ndataset = HumanActionDataset(video_dir=DATA_PATH, seq_len=SEQUENCE_LENGTH, num_classes=NUM_CLASSES)\nprint(f'Length of the dataset: {len(dataset)}')\n\n# Checking the dataset shape\nframes, label = dataset[2000]\nprint(f'Video tensor(first frame):\\n{frames[0]}\\nlabel tensor: {label}')\nprint(f'Shape of the video tensor: {frames.shape} and shape of the label: {label.shape}')\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n         [ 2.,  2.,  0.,  ..., 11., 11., 11.],\n         ...,\n         [ 2.,  2.,  0.,  ..., 55., 55., 55.],\n         [ 5.,  5.,  3.,  ..., 62., 62., 62.],\n         [ 5.,  5.,  2.,  ..., 34., 34., 34.]],\n\n        [[ 4.,  4.,  5.,  ...,  2.,  1.,  1.],\n         [ 5.,  5.,  5.,  ...,  3.,  3.,  3.],\n         [ 4.,  4.,  5.,  ..., 17., 17., 17.],\n         ...,\n         [ 2.,  2.,  2.,  ..., 41., 41., 41.],\n         [ 0.,  0.,  1.,  ..., 49., 49., 49.],\n         [ 0.,  0.,  0.,  ..., 21., 21., 21.]],\n\n        [[ 3.,  3.,  4.,  ...,  2.,  1.,  1.],\n         [ 4.,  4.,  4.,  ...,  3.,  3.,  3.],\n         [ 1.,  1.,  1.,  ...,  8.,  8.,  8.],\n         ...,\n         [ 0.,  0.,  0.,  ..., 16., 14., 14.],\n         [ 0.,  0.,  0.,  ..., 28., 28., 28.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([16, 3, 240, 320]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-a-pretrained-model",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-a-pretrained-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating A Pretrained Model",
    "text": "Creating A Pretrained Model\n\ndef create_model(num_classes: int, device: torch.device, seed: int = 42):\n    \"\"\"\n    A function to create a model.\n    Parameters:\n        num_classes: int, A integer for toal number of classes.\n        device: torch.device, A torch device to set the tensors to cpu or cuda.\n        seed: int(default: 42), A random seed value.\n    Returns: \n        model: A feature extracted model for video classification.\n        transforms: A torchvision transform is returned which was used in the pretrained model.    \n    \"\"\"\n    # Creating model, weights and transforms\n    weights = MViT_V2_S_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = mvit_v2_s(weights=weights)\n    \n    # Freezing the model layers\n    for params in model.parameters():\n        params.requires_grad = False\n        \n    # Changing the fully Conncected head layer\n    set_seed(seed)\n    dropout_layer = model.head[0]\n    in_features = model.head[1].in_features\n    model.head = torch.nn.Sequential(\n        dropout_layer,\n        torch.nn.Linear(in_features=in_features, out_features=num_classes, bias=True, device=device)\n    )\n    return model.to(device), transforms\n\n\n# Creating the model and transforms using the function.\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\n\n\nsummary(model, \n        input_size=(1, 3, 16, 224, 224),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])\n\n/usr/local/lib/python3.9/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  action_fn=lambda data: sys.getsizeof(data.storage()),\n/usr/local/lib/python3.9/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return super().__sizeof__() + self.nbytes()\n\n\n==================================================================================================================================\nLayer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n==================================================================================================================================\nMViT (MViT)                                        [1, 3, 16, 224, 224] [1, 20]              --                   Partial\n├─Conv3d (conv_proj)                               [1, 3, 16, 224, 224] [1, 96, 8, 56, 56]   (42,432)             False\n├─PositionalEncoding (pos_encoding)                [1, 25088, 96]       [1, 25089, 96]       (96)                 False\n├─ModuleList (blocks)                              --                   --                   --                   False\n│    └─MultiscaleBlock (0)                         [1, 25089, 96]       [1, 25089, 96]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 25089, 96]       (68,352)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MLP (mlp)                              [1, 25089, 96]       [1, 25089, 96]       (74,208)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    └─MultiscaleBlock (1)                         [1, 25089, 96]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 6273, 192]       (113,280)            False\n│    │    └─Linear (project)                       [1, 25089, 96]       [1, 25089, 192]      (18,624)             False\n│    │    └─Pool (pool_skip)                       [1, 25089, 192]      [1, 6273, 192]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (2)                         [1, 6273, 192]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 6273, 192]       (168,576)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (3)                         [1, 6273, 192]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 1569, 384]       (385,152)            False\n│    │    └─Linear (project)                       [1, 6273, 192]       [1, 6273, 384]       (74,112)             False\n│    │    └─Pool (pool_skip)                       [1, 6273, 384]       [1, 1569, 384]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (4)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (5)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (6)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (7)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (8)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (9)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (10)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (11)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (12)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (13)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (14)                        [1, 1569, 384]       [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 393, 768]        (1,492,608)          False\n│    │    └─Linear (project)                       [1, 1569, 384]       [1, 1569, 768]       (295,680)            False\n│    │    └─Pool (pool_skip)                       [1, 1569, 768]       [1, 393, 768]        --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    └─MultiscaleBlock (15)                        [1, 393, 768]        [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MultiscaleAttention (attn)             [1, 393, 768]        [1, 393, 768]        (2,374,656)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n├─LayerNorm (norm)                                 [1, 393, 768]        [1, 393, 768]        (1,536)              False\n├─Sequential (head)                                [1, 768]             [1, 20]              --                   True\n│    └─Dropout (0)                                 [1, 768]             [1, 768]             --                   --\n│    └─Linear (1)                                  [1, 768]             [1, 20]              15,380               True\n==================================================================================================================================\nTotal params: 34,245,524\nTrainable params: 15,380\nNon-trainable params: 34,230,144\nTotal mult-adds (G): 1.64\n==================================================================================================================================\nInput size (MB): 9.63\nForward/backward pass size (MB): 1658.93\nParams size (MB): 136.46\nEstimated Total Size (MB): 1805.02\n==================================================================================================================================\n\n\n\n# Checking the model transforms and applying it on the dataset\nprint(f'Pretrained Model Transforms:\\n{transforms}\\n')\ndataset = HumanActionDataset(video_dir=DATA_PATH, seq_len=SEQUENCE_LENGTH, num_classes=NUM_CLASSES, transform=transforms)\nprint(f'Length of the dataset: {len(dataset)}')\n\n# Checking the dataset and shape\nframes, label = dataset[2000]\nprint(f'Video tensor(first frame):\\n{frames[0]}\\nlabel tensor: {label}')\nprint(f'Shape of the video tensor: {frames.shape} and shape of the label: {label.shape}')\n\nPretrained Model Transforms:\nVideoClassification(\n    crop_size=[224, 224]\n    resize_size=[256]\n    mean=[0.45, 0.45, 0.45]\n    std=[0.225, 0.225, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 2.4604e+02,  2.3502e+02,  2.2911e+02,  ...,  2.8675e+02,\n           2.8675e+02,  2.8684e+02],\n         [ 2.3070e+02,  2.2561e+02,  2.1813e+02,  ...,  3.1508e+02,\n           3.1508e+02,  3.1508e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1425e+02,\n           3.1425e+02,  3.1425e+02],\n         ...,\n         [ 3.2401e+00,  2.9817e+00, -2.0000e+00,  ...,  1.6333e+02,\n           1.2992e+02,  9.1900e+01],\n         [-1.5833e+00, -1.7232e+00, -2.0000e+00,  ...,  4.4867e+02,\n           3.7443e+02,  2.8330e+02],\n         [ 2.4444e+00,  9.5210e-01, -2.0000e+00,  ...,  6.6754e+02,\n           6.3805e+02,  5.6134e+02]],\n\n        [[ 2.2467e+02,  2.2616e+02,  2.2911e+02,  ...,  3.0883e+02,\n           3.0883e+02,  3.0902e+02],\n         [ 2.2869e+02,  2.2478e+02,  2.1813e+02,  ...,  3.1314e+02,\n           3.1314e+02,  3.1581e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1356e+02,\n           3.1356e+02,  3.1402e+02],\n         ...,\n         [ 9.1056e+00,  6.2105e+00, -2.0000e+00,  ...,  9.9700e+01,\n           7.6676e+01,  2.8844e+01],\n         [ 1.9746e+01,  1.7561e+01, -1.1667e+00,  ...,  2.1729e+02,\n           1.6955e+02,  1.1592e+02],\n         [ 6.6111e+00,  6.6111e+00,  6.6111e+00,  ...,  5.4941e+02,\n           5.2508e+02,  4.3940e+02]],\n\n        [[-2.0000e+00,  5.3684e+00,  2.1125e+01,  ...,  3.2244e+02,\n           3.1922e+02,  3.2077e+02],\n         [-3.9952e-01,  4.0798e+00,  9.3575e+00,  ...,  3.2330e+02,\n           3.1922e+02,  3.1025e+02],\n         [ 2.0259e+00,  2.6776e+00,  5.3857e+00,  ...,  3.3464e+02,\n           3.3047e+02,  3.1790e+02],\n         ...,\n         [ 1.7578e+02,  1.7578e+02,  1.7719e+02,  ...,  2.3105e+02,\n           2.2658e+02,  2.1946e+02],\n         [ 1.7803e+02,  1.7884e+02,  1.8227e+02,  ...,  2.3562e+02,\n           2.2931e+02,  2.2306e+02],\n         [ 1.9990e+02,  2.0851e+02,  2.2140e+02,  ...,  2.2817e+02,\n           2.1964e+02,  2.2187e+02]],\n\n        ...,\n\n        [[ 4.2491e+02,  4.3142e+02,  4.2619e+02,  ...,  1.5639e+01,\n           1.5639e+01,  1.5639e+01],\n         [ 4.0218e+02,  4.0793e+02,  4.1056e+02,  ...,  6.0095e+00,\n           7.7222e+00,  2.3716e+00],\n         [ 4.0126e+02,  4.0792e+02,  4.0992e+02,  ...,  8.7490e+00,\n           1.0639e+01,  2.2439e+00],\n         ...,\n         [ 1.2787e+02,  1.2839e+02,  1.3253e+02,  ...,  9.5269e-01,\n           1.7500e+00,  1.7500e+00],\n         [ 1.3349e+02,  1.2883e+02,  1.3904e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 1.3694e+02,  1.3320e+02,  1.3960e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.5630e+02,  4.6579e+02,  4.6640e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3329e+02,  4.3661e+02,  4.3776e+02,  ...,  1.1714e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4439e+02,  4.4481e+02,  4.4500e+02,  ...,  1.4995e+00,\n          -7.8133e-01, -2.0000e+00],\n         ...,\n         [ 1.4467e+02,  1.5315e+02,  1.7425e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.3056e+00],\n         [ 1.4625e+02,  1.5174e+02,  1.6260e+02,  ..., -2.0000e+00,\n           9.2336e-01,  2.0278e+00],\n         [ 1.6148e+02,  1.6227e+02,  1.5224e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.2527e+02,  4.4359e+02,  4.6669e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3037e+02,  4.3994e+02,  4.5392e+02,  ...,  2.0278e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4064e+02,  4.4451e+02,  4.5152e+02,  ...,  2.4444e+00,\n          -7.8133e-01,  4.9081e-01],\n         ...,\n         [ 1.4709e+02,  1.5459e+02,  1.5781e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.7668e+00],\n         [ 1.4016e+02,  1.5057e+02,  1.5565e+02,  ..., -1.5833e+00,\n           1.3400e+00,  4.5890e-02],\n         [ 1.3139e+02,  1.3590e+02,  1.4467e+02,  ...,  2.4444e+00,\n           2.4444e+00,  5.3965e+00]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([3, 16, 224, 224]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-dataloaders",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-dataloaders",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating DataLoaders",
    "text": "Creating DataLoaders\n\n# Spliting the dataset in train and test dataset for ratio of 75:25.\ndataset = HumanActionDataset(video_dir=DATA_PATH, \n                             seq_len=SEQUENCE_LENGTH,\n                             num_classes=NUM_CLASSES,\n                             transform=transforms)\ntrain_dataset, test_dataset = random_split(dataset=dataset,\n                                           lengths=[0.75, 0.25], \n                                           generator=torch.Generator().manual_seed(42))\nprint(f'Train Dataset Length: {len(train_dataset)} and Test Dataset length: {len(test_dataset)}')\n\nTrain Dataset Length: 1898 and Test Dataset length: 632\n\n\n\n# Creating train and test dataloaders.\ndef create_dataloaders(dataset, batch, shuffle, workers):\n    \"\"\"\n    A function to create pytorch dataloaders.\n    \n    Parameters:\n        dataset: A pytorch dataset.\n        batch: A integer for batch size.\n        shuffle: A boolean for shuffling the data.\n        workers: A integer to set the workers in dataloaders.\n        \n    Returns: dataloader.\n    \"\"\"\n    dataloader = DataLoader(dataset=dataset, \n                            batch_size=batch, \n                            shuffle=shuffle, \n                            num_workers=workers,\n                            pin_memory=True,\n                            drop_last=True)\n    print(f'Dataloaders Length: {len(dataloader)} for a batch size of: {batch}')\n    return dataloader\n\nset_seed(42)\nBATCH_SIZE = 16\nWORKERS = os.cpu_count()\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\n\n\nLabel Distribution and Zero Rule Benchmark\n\ntrain_counter = Counter()\nfor frames, labels in train_dataloader:\n    train_counter.update(labels.tolist())\nprint('Training Dataset Distribution:\\n', sorted(train_counter.items()))\n\nTraining Dataset Distribution:\n [(0, 78), (1, 92), (2, 83), (3, 79), (4, 94), (5, 72), (6, 77), (7, 79), (8, 85), (9, 103), (10, 90), (11, 106), (12, 88), (13, 78), (14, 180), (15, 70), (16, 87), (17, 100), (18, 171), (19, 76)]\n\n\n\ntest_counter = Counter()\nfor frames, labels in test_dataloader:\n    test_counter.update(labels.tolist())\nprint('Testing Dataset Distribution:\\n', sorted(test_counter.items()))\n\nTesting Dataset Distribution:\n [(0, 30), (1, 32), (2, 24), (3, 23), (4, 29), (5, 32), (6, 30), (7, 24), (8, 27), (9, 27), (10, 17), (11, 35), (12, 37), (13, 24), (14, 58), (15, 33), (16, 29), (17, 26), (18, 60), (19, 27)]\n\n\n\n# Zero Rule Benchmark\nmajor_class = test_counter.most_common(1)[0]\nprint(f'The Major class in Testing DataLoader is: Class Name: {CLASSES[major_class[0]]}, Class Index: {major_class[0]} and Total Number of Data: {major_class[1]}')\n\nzero_rule_acc = (major_class[1] / sum(test_counter.values())) * 100\nprint(f'Zero Rule Basline Benchmark Accuracy for predicting the major class: {zero_rule_acc:.2f}%')\n\nThe Major class in Testing DataLoader is: Class Name: hug, Class Index: 18 and Total Number of Data: 60\nZero Rule Basline Benchmark Accuracy for predicting the major class: 9.62%"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch---model-training-and-testing",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch - Model Training and Testing",
    "text": "PyTorch - Model Training and Testing\n\ndef train_step(model, dataloader, loss_fn, optimizer, device, num_classes):\n    \"\"\"\n    Trains a pytorch model by going into train mode and applying forward pass,\n    loss calculation and optimizer step.\n    \n    Parameters:\n        model: A pytorch model for training.\n        dataloader: A pytorch dataloader for training.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        device: A torch device to allocate tensors on 'cpu' or 'cuda'.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of training loss and training accuracy.\n        \n    \"\"\"\n    # Model on training mode\n    model.train()\n    \n    # Setting train loss and accuracy \n    train_loss = 0\n    train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(device)\n    \n    # Looping the dataloaders\n    for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Training', total=len(dataloader), unit='batch'):\n        X, y = X.to(device), y.to(device)\n        \n        # 5 step to train a model\n        y_pred = model(X) # 1. Forward pass\n        loss = loss_fn(y_pred, y) # 2. Calculate loss\n        train_loss += loss.item() \n        optimizer.zero_grad() # 3. Initiate optimizer\n        loss.backward() # 4. Backward pass\n        optimizer.step() # 5. Updating the model parameters\n        \n        # Calculating the training accuracy\n        y_pred_labels = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc.compute()\n    return train_loss, train_acc\n\ndef test_step(model, dataloader, loss_fn, device, num_classes):\n    \"\"\"\n    Test a pytorch model by going into eval mode and applying forward pass,\n    and loss calculation.\n    \n    Parameters:\n        model: A pytorch model for testing.\n        dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        device: A torch device to allocate tensors on 'cpu' or 'cuda'.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of testing loss and testing accuracy.\n    \"\"\"\n    # Model on evaluation mode\n    model.eval()\n    \n    # Setting train loss and accuracy \n    test_loss = 0\n    test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(device)\n    \n    # Using inference mode\n    with torch.no_grad():\n        # Looping the dataloaders\n        for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Evaluation', total=len(dataloader), unit='batch'):\n            X, y = X.to(device), y.to(device)\n            \n            # Forward pass\n            y_pred = model(X)\n            \n            # Calculate loss\n            loss = loss_fn(y_pred, y)\n            test_loss += loss.item()\n            \n            # Calculate accuracy\n            y_pred_labels = y_pred.argmax(dim=1)\n            test_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc.compute()\n    return test_loss, test_acc\n\ndef model_train(epochs, model, train_dataloader, test_dataloader, optimizer, loss_fn, device, num_classes):\n    \"\"\"\n    Trains a pytorch model for a certain number of epochs going through the model training \n    and testing stage, and accumalating the loss, accuracy, and training and testing time.\n    \n    Parameters:\n        epochs: A integer to run the training and testing stage. \n        model: A pytorch model for training and testing.\n        train_dataloader: A pytorch dataloader for training.\n        test_dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        device: A torch device to allocate tensors on 'cpu' or 'cuda'.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of accumaleted results in dict and total training time in float datatype.\n    \"\"\"\n    # Create empty result\n    results = {'train_loss': [],\n               'train_acc': [],\n               'test_loss': [],\n               'test_acc': [],\n               'train_epoch_time(min)': [],\n               'test_epoch_time(min)': []}\n    \n    # Loop through training and testing steps\n    model_train_start_time = timer()\n    for epoch in tqdm(range(epochs), desc=f'Training and Evaluation for {epochs} Epochs', unit='epochs'):\n        # Training the model and timing it.\n        train_epoch_start_time = timer()\n        train_loss, train_acc = train_step(model=model, \n                                           dataloader=train_dataloader, \n                                           loss_fn=loss_fn, \n                                           optimizer=optimizer, \n                                           device=device, \n                                           num_classes=num_classes)\n        train_epoch_stop_time = timer()\n        train_epoch_time = (train_epoch_stop_time - train_epoch_start_time)/60\n        \n        # Testing the model and timing it\n        test_epoch_start_time = timer()\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        device=device,\n                                        num_classes=num_classes)\n        test_epoch_stop_time = timer()\n        test_epoch_time = (test_epoch_stop_time - test_epoch_start_time)/60\n        \n        # Print the model result\n        print(f'Epoch: [{epoch+1}/{epochs}] | train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | train_time: {train_epoch_time:.4f} min | '\n              f'test loss: {test_loss:.4f} | test_acc: {test_acc:.4f} | test_time: {test_epoch_time:.4f} min')\n        \n        # Saving the results\n        results['train_loss'].append(train_loss)\n        results['train_acc'].append(train_acc.detach().cpu().item())\n        results['test_loss'].append(test_loss)\n        results['test_acc'].append(test_acc.detach().cpu().item())\n        results['train_epoch_time(min)'].append(train_epoch_time)\n        results['test_epoch_time(min)'].append(test_epoch_time)\n        \n    # Calculating total model training time\n    model_train_end_time = timer()\n    total_train_time = (model_train_end_time - model_train_start_time)/60\n    print(f'\\nTotal Model Training Time: {total_train_time:.4f} min')\n    return results, total_train_time\n\n\nExp 1: PyTorch Before 2.0 (PyTorch Eager Mode)\n\nset_seed(42)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\nmodel.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp1_results, exp1_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  device=device, \n                                                  num_classes=len(dataset.classes))\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.4238 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5183 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.4201 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5181 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.3959 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.5176 min\n\nTotal Model Training Time: 5.7940 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Creating a result directory\nRESULTS_DIR = 'results'\nif os.path.exists(RESULTS_DIR):\n    print('[INFO] results directory exists.')\nelse:\n    os.mkdir(RESULTS_DIR)\n    print('[INFO] results directory is been created.')\n\n# Saving the result in csv format\nexp1_results_filename = 'exp1_results.csv'\nexp1_results_df = pd.DataFrame(exp1_results)\nexp1_results_df.to_csv(os.path.join(RESULTS_DIR, exp1_results_filename), index=False)\nexp1_results_df\n\n[INFO] results directory is been created.\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.423781\n0.518263\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.420147\n0.518100\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.395912\n0.517590\n\n\n\n\n\n\n\n\n# Creating and saving a dataframe for model train time \nmodel_train_time_filename = 'model_train_time.csv'\nmodel_train_time_df = pd.DataFrame({'model': ['Pytorch Eager'], 'training_time(min)': [exp1_total_train_time]})\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n\n\n\n\n\n\n\nExp 2: PyTorch 2.0 (PyTorch Compile Mode)\n\nset_seed(42)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\nmodel.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp2_results, exp2_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model,\n                                                  train_dataloader=train_dataloader,\n                                                  test_dataloader=test_dataloader,\n                                                  optimizer=optimizer,\n                                                  loss_fn=loss_fn,\n                                                  device=device,\n                                                  num_classes=len(dataset.classes))\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.3885 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0454 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.6187 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5678 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.6008 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5694 min\n\nTotal Model Training Time: 7.7908 min\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.9/dist-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n  warnings.warn(\n[2023-04-06 06:39:50,385] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp2_results_filename = 'exp2_results.csv'\nexp2_results_df = pd.DataFrame(exp2_results)\nexp2_results_df.to_csv(os.path.join(RESULTS_DIR, exp2_results_filename), index=False)\nexp2_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.388515\n1.045425\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.618742\n0.567790\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.600783\n0.569392\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Pytorch Compile', exp2_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch-lightning---model-training-and-testing",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch-lightning---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch Lightning - Model Training and Testing",
    "text": "PyTorch Lightning - Model Training and Testing\n\nclass PyLightHMDB51(L.LightningModule):\n    \"\"\"\n    A Lightning Module containing Model training and validation step.\n    Parameters: \n        model: A PyTorch Model.\n        loss_fn: A PyTorch loss function.\n        optimizer: A Pytorch Optimizer.\n        num_classes: A integer for total number of classes in the dataset.\n    \"\"\"\n    def __init__(self, model, loss_fn, optimizer, num_classes):\n        super().__init__()\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.num_classes = num_classes\n        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n        self.test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def training_step(self, train_batch, batch_idx):\n        X, y = train_batch\n        y_preds = self.forward(X)\n        loss = self.loss_fn(y_preds, y)\n        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n        y_pred_labels = torch.argmax(torch.softmax(y_preds, dim=1), dim=1)\n        self.train_acc.update(y_pred_labels, y)\n        self.log('train_acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n        return loss\n    \n    def validation_step(self, val_batch, batch_idx):\n        X, y = val_batch\n        y_preds = self.forward(X)\n        loss = self.loss_fn(y_preds, y)\n        self.log('test_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n        y_pred_labels = torch.argmax(torch.softmax(y_preds, dim=1), dim=1)\n        self.test_acc.update(y_pred_labels, y)\n        self.log('test_acc', self.test_acc, prog_bar=True, on_step=False, on_epoch=True)\n    \n    def configure_optimizers(self):\n        optimizers = self.optimizer\n        return optimizers\n\n\nExp 3: PyTorch Lightning\n\nset_seed(42)\n\n# Creating the pytorch lightning trainer\nNUM_EPOCHS = 3\nlogger = L.pytorch.loggers.CSVLogger(save_dir=RESULTS_DIR, \n                                     name=\"pytorch_lightning\")\ntrainer = L.Trainer(max_epochs=NUM_EPOCHS, \n                    logger=logger)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Initializing the lightning module class\nmodel = PyLightHMDB51(model=model, \n                      loss_fn=loss_fn, \n                      optimizer=optimizer, \n                      num_classes=len(dataset.classes))\n\n# Fiting the model to trainer.\nstart_time = timer()\ntrainer.fit(model=model, \n            train_dataloaders=train_dataloader, \n            val_dataloaders=test_dataloader)\nend_time = timer()\nexp3_total_train_time = (end_time - start_time)/60\nprint(f'Total Time to train the model: {exp3_total_train_time:.4f} min')\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nMissing logger folder: results/pytorch_lightning\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type               | Params\n-------------------------------------------------\n0 | model     | MViT               | 34.2 M\n1 | loss_fn   | CrossEntropyLoss   | 0     \n2 | train_acc | MulticlassAccuracy | 0     \n3 | test_acc  | MulticlassAccuracy | 0     \n-------------------------------------------------\n15.4 K    Trainable params\n34.2 M    Non-trainable params\n34.2 M    Total params\n136.982   Total estimated model params size (MB)\n`Trainer.fit` stopped: `max_epochs=3` reached.\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 7.6887 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Pytorch Lightning', exp3_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n\n\n\n\n\n\n\nExp 4: PyTorch Lightning - Compile Mode\n\nset_seed(42)\n\n# Creating the pytorch lightning trainer\nNUM_EPOCHS = 3\nlogger = L.pytorch.loggers.CSVLogger(save_dir=RESULTS_DIR, \n                                     name=\"pytorch_lightning_compile_mode\")\ntrainer = L.Trainer(max_epochs=NUM_EPOCHS, \n                    logger=logger)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Initializing the lightning module class\nmodel = PyLightHMDB51(model=model, \n                      loss_fn=loss_fn, \n                      optimizer=optimizer, \n                      num_classes=len(dataset.classes))\n\n# Fiting the model to trainer.\nstart_time = timer()\ntrainer.fit(model=model, \n            train_dataloaders=train_dataloader, \n            val_dataloaders=test_dataloader)\nend_time = timer()\nexp4_total_train_time = (end_time - start_time)/60\nprint(f'Total Time to train the model: {exp4_total_train_time:.4f} min')\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nMissing logger folder: results/pytorch_lightning_compile_mode\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type               | Params\n-------------------------------------------------\n0 | model     | OptimizedModule    | 34.2 M\n1 | loss_fn   | CrossEntropyLoss   | 0     \n2 | train_acc | MulticlassAccuracy | 0     \n3 | test_acc  | MulticlassAccuracy | 0     \n-------------------------------------------------\n15.4 K    Trainable params\n34.2 M    Non-trainable params\n34.2 M    Total params\n136.982   Total estimated model params size (MB)\n[2023-04-06 06:56:20,261] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n`Trainer.fit` stopped: `max_epochs=3` reached.\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 10.5425 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['PyTorch Lightning + Compile', exp4_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#lightning-fabric---model-training-and-testing",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#lightning-fabric---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Lightning Fabric - Model training and Testing",
    "text": "Lightning Fabric - Model training and Testing\n\ndef train_step(model, dataloader, loss_fn, optimizer, fabric, num_classes):\n    \"\"\"\n    Trains a pytorch model by going into train mode and applying forward pass,\n    loss calculation and optimizer step.\n    \n    Parameters:\n        model: A pytorch model for training.\n        dataloader: A pytorch dataloader for training.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        fabric: A Fabric function to setup device for the tensors and gradients.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of training loss and training accuracy.\n        \n    \"\"\"\n    # Model on training mode\n    model.train()\n    \n    # Setting train loss and accuracy \n    train_loss = 0\n    train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(fabric.device) # New by Fabric\n    \n    # Looping the dataloaders\n    for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Training', total=len(dataloader), unit='batch'):\n        # X, y = X.to(device), y.to(device) # New by Fabric\n        \n        # 5 step to train a model\n        y_pred = model(X) # 1. Forward pass\n        loss = loss_fn(y_pred, y) # 2. Calculate loss\n        train_loss += loss.item() \n        optimizer.zero_grad() # 3. Initiate optimizer\n        #loss.backward() # 4. Backward pass\n        fabric.backward(loss) # New by Fabric\n        optimizer.step() # 5. Updating the model parameters\n        \n        # Calculating the training accuracy\n        y_pred_labels = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc.compute()\n    return train_loss, train_acc\n\ndef test_step(model, dataloader, loss_fn, fabric, num_classes):\n    \"\"\"\n    Test a pytorch model by going into eval mode and applying forward pass,\n    and loss calculation.\n    \n    Parameters:\n        model: A pytorch model for testing.\n        dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        fabric: A Fabric function to setup device for the tensors and gradients.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of testing loss and testing accuracy.\n    \"\"\"\n    # Model on evaluation mode\n    model.eval()\n    \n    # Setting train loss and accuracy \n    test_loss = 0\n    test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(fabric.device) # New by Fabric\n    \n    # Using inference mode\n    with torch.no_grad():\n        # Looping the dataloaders\n        for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Evaluation', total=len(dataloader), unit='batch'):\n            # X, y = X.to(device), y.to(device) # New by Fabric\n            \n            # Forward pass\n            y_pred = model(X)\n            \n            # Calculate loss\n            loss = loss_fn(y_pred, y)\n            test_loss += loss.item()\n            \n            # Calculate accuracy\n            y_pred_labels = y_pred.argmax(dim=1)\n            test_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc.compute()\n    return test_loss, test_acc\n\ndef model_train(epochs, model, train_dataloader, test_dataloader, optimizer, loss_fn, fabric, num_classes):\n    \"\"\"\n    Trains a pytorch model for a certain number of epochs going through the model training \n    and testing stage, and accumalating the loss, accuracy, and training and testing time.\n    \n    Parameters:\n        epochs: A integer to run the training and testing stage. \n        model: A pytorch model for training and testing.\n        train_dataloader: A pytorch dataloader for training.\n        test_dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        fabric: A Fabric function to setup device for the tensors and gradients.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of accumaleted results in dict and total training time in float datatype.\n    \"\"\"\n    # Create empty result\n    results = {'train_loss': [],\n               'train_acc': [],\n               'test_loss': [],\n               'test_acc': [],\n               'train_epoch_time(min)': [],\n               'test_epoch_time(min)': []}\n    \n    # Loop through training and testing steps\n    model_train_start_time = timer()\n    for epoch in tqdm(range(epochs), desc=f'Training and Evaluation for {epochs} Epochs', unit='epochs'):\n        # Training the model and timing it.\n        train_epoch_start_time = timer()\n        train_loss, train_acc = train_step(model=model, \n                                           dataloader=train_dataloader, \n                                           loss_fn=loss_fn, \n                                           optimizer=optimizer, \n                                           fabric=fabric, # New by Fabric\n                                           num_classes=num_classes)\n        train_epoch_stop_time = timer()\n        train_epoch_time = (train_epoch_stop_time - train_epoch_start_time)/60\n        \n        # Testing the model and timing it\n        test_epoch_start_time = timer()\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        fabric=fabric, # New by Fabric\n                                        num_classes=num_classes)\n        test_epoch_stop_time = timer()\n        test_epoch_time = (test_epoch_stop_time - test_epoch_start_time)/60\n        \n        # Print the model result\n        print(f'Epoch: [{epoch+1}/{epochs}] | train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | train_time: {train_epoch_time:.4f} min | '\n              f'test loss: {test_loss:.4f} | test_acc: {test_acc:.4f} | test_time: {test_epoch_time:.4f} min')\n        \n        # Saving the results\n        results['train_loss'].append(train_loss)\n        results['train_acc'].append(train_acc.detach().cpu().item())\n        results['test_loss'].append(test_loss)\n        results['test_acc'].append(test_acc.detach().cpu().item())\n        results['train_epoch_time(min)'].append(train_epoch_time)\n        results['test_epoch_time(min)'].append(test_epoch_time)\n        \n    # Calculating total model training time\n    model_train_end_time = timer()\n    total_train_time = (model_train_end_time - model_train_start_time)/60\n    print(f'\\nTotal Model Training Time: {total_train_time:.4f} min')\n    return results, total_train_time\n\n\nExp 5: Fabric Lightning\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric()\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp5_results, exp5_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5801 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5829 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.5967 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5939 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5900 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.6074 min\n\nTotal Model Training Time: 6.5512 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp5_results_filename = 'exp5_results.csv'\nexp5_results_df = pd.DataFrame(exp5_results)\nexp5_results_df.to_csv(os.path.join(RESULTS_DIR, exp5_results_filename), index=False)\nexp5_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.580098\n0.582881\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.596745\n0.593947\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.590004\n0.607377\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric Lightning', exp5_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n\n\n\n\n\n\n\nExp 6: Fabric Lightning - Compile mode\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric()\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp6_results, exp6_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n[2023-04-06 07:12:28,856] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.1276 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0502 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.7023 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5721 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.5845 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5688 min\n\nTotal Model Training Time: 7.6056 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp6_results_filename = 'exp6_results.csv'\nexp6_results_df = pd.DataFrame(exp6_results)\nexp6_results_df.to_csv(os.path.join(RESULTS_DIR, exp6_results_filename), index=False)\nexp6_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.127585\n1.050178\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.702294\n0.572100\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.584458\n0.568837\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric + Compile', exp6_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n\n\n\n\n\n\n\nExp 7: Fabric + Precision\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric(precision='16-mixed')\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp7_results, exp7_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nUsing 16-bit Automatic Mixed Precision (AMP)\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5438 min | test loss: 2.6781 | test_acc: 0.4519 | test_time: 0.5552 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4168 | train_time: 1.5081 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5554 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5786 min | test loss: 2.2094 | test_acc: 0.5817 | test_time: 0.5744 min\n\nTotal Model Training Time: 6.3157 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp7_results_filename = 'exp7_results.csv'\nexp7_results_df = pd.DataFrame(exp7_results)\nexp7_results_df.to_csv(os.path.join(RESULTS_DIR, exp7_results_filename), index=False)\nexp7_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853492\n0.198623\n2.678145\n0.451923\n1.543753\n0.555179\n\n\n1\n2.599140\n0.416843\n2.427695\n0.540064\n1.508068\n0.555399\n\n\n2\n2.360222\n0.523835\n2.209447\n0.581731\n1.578585\n0.574411\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric + Precision', exp7_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n\n\n\n\n\n\n\nExp 8: Fabric + Precision + Compile\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric(precision='16-mixed')\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp8_results, exp8_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nUsing 16-bit Automatic Mixed Precision (AMP)\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n[2023-04-06 07:26:29,375] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 3.0326 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.1394 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.5740 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5764 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5095 | train_time: 1.5562 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5639 min\n\nTotal Model Training Time: 8.4427 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp8_results_filename = 'exp8_results.csv'\nexp8_results_df = pd.DataFrame(exp8_results)\nexp8_results_df.to_csv(os.path.join(RESULTS_DIR, exp8_results_filename), index=False)\nexp8_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845062\n0.201271\n2.678693\n0.450321\n3.032648\n1.139419\n\n\n1\n2.590762\n0.445975\n2.427031\n0.548077\n1.574046\n0.576357\n\n\n2\n2.366282\n0.509534\n2.209138\n0.580128\n1.556243\n0.563871\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric + Precision + Compile', exp8_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n7\nFabric + Precision + Compile\n8.442741\n\n\n\n\n\n\n\n\n# plot the train time\nmodel_train_time_df.plot.barh(x='model', \n                              y='training_time(min)', \n                              xlabel='Training Time(min)', \n                              ylabel='Training Method', \n                              legend=False)\nplt.title(f'MVit V2 Model Training Time for {NUM_EPOCHS} Epochs')\nplt.savefig('model_train_time.png')"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "href": "docs/posts/2023-04-01_pytorch_&_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Training and Testing on Full Dataset and Deploying the Model",
    "text": "Training and Testing on Full Dataset and Deploying the Model\n\nCreating the model\n\nNUM_CLASSES = 51\nSEQUENCE_LENGTH = 16\nBATCH_SIZE = 16\nWORKERS = os.cpu_count()\n\n\n# Creating the model and transforms using the function.\nmodel, transforms = create_model(num_classes=NUM_CLASSES, device=device)\n\n\n\nGetting the Full Dataset and Dataloaders\n\n# Creating the dataset.\ndataset = HumanActionDataset(video_dir=DATA_PATH, \n                             seq_len=SEQUENCE_LENGTH,\n                             num_classes=NUM_CLASSES,\n                             transform=transforms)\ntrain_dataset, test_dataset = random_split(dataset=dataset,\n                                           lengths=[0.75, 0.25], \n                                           generator=torch.Generator().manual_seed(42))\nprint(f'Train Dataset Length: {len(train_dataset)} and Test Dataset length: {len(test_dataset)}')\n\n# Creating the dataloaders\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\nTrain Dataset Length: 5075 and Test Dataset length: 1691\nDataloaders Length: 317 for a batch size of: 16\nDataloaders Length: 105 for a batch size of: 16\n\n\n\n\nTraining Model for Full Dataset\n\nset_seed(42)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n# Training the model using the pytorch function \nNUM_EPOCHS = 10\nresults, total_train_time = model_train(epochs=NUM_EPOCHS,\n                                        model=model, \n                                        train_dataloader=train_dataloader, \n                                        test_dataloader=test_dataloader, \n                                        optimizer=optimizer, \n                                        loss_fn=loss_fn, \n                                        device=device, \n                                        num_classes=len(dataset.classes))\n\n\n\n\n\n\n\n\n\n\nEpoch: [1/10] | train_loss: 2.6020 | train_acc: 0.4720 | train_time: 4.8366 min | test loss: 1.8906 | test_acc: 0.6238 | test_time: 1.6950 min\nEpoch: [2/10] | train_loss: 1.8662 | train_acc: 0.6327 | train_time: 5.4221 min | test loss: 1.7076 | test_acc: 0.6607 | test_time: 1.5391 min\nEpoch: [3/10] | train_loss: 1.7319 | train_acc: 0.6642 | train_time: 4.8773 min | test loss: 1.6484 | test_acc: 0.6756 | test_time: 1.5405 min\nEpoch: [4/10] | train_loss: 1.6819 | train_acc: 0.6733 | train_time: 4.5695 min | test loss: 1.6199 | test_acc: 0.6887 | test_time: 1.5152 min\nEpoch: [5/10] | train_loss: 1.6426 | train_acc: 0.6936 | train_time: 4.3718 min | test loss: 1.6051 | test_acc: 0.6970 | test_time: 1.5019 min\nEpoch: [6/10] | train_loss: 1.6085 | train_acc: 0.6981 | train_time: 4.7581 min | test loss: 1.5919 | test_acc: 0.7000 | test_time: 1.5833 min\nEpoch: [7/10] | train_loss: 1.5967 | train_acc: 0.7049 | train_time: 4.9686 min | test loss: 1.5807 | test_acc: 0.7071 | test_time: 1.5994 min\nEpoch: [8/10] | train_loss: 1.5680 | train_acc: 0.7173 | train_time: 6.3247 min | test loss: 1.5763 | test_acc: 0.7137 | test_time: 1.5196 min\nEpoch: [9/10] | train_loss: 1.5577 | train_acc: 0.7179 | train_time: 5.6291 min | test loss: 1.5734 | test_acc: 0.7083 | test_time: 1.5351 min\nEpoch: [10/10] | train_loss: 1.5540 | train_acc: 0.7194 | train_time: 5.6172 min | test loss: 1.5605 | test_acc: 0.7185 | test_time: 1.5643 min\n\nTotal Model Training Time: 66.9688 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStoring Files for Demo App.\n\n# Creating a directory for the app\ndemo_path = 'demo'\ndemo_project_path = os.path.join(demo_path, 'human_action_recognition')\ndemo_example_path = os.path.join(demo_project_path, 'examples')\n\nif os.path.exists(demo_path):\n    print(f'[INFO] \"{demo_path}\" already exists.')\nelse:\n    os.mkdir(demo_path)\n    os.mkdir(demo_project_path)\n    os.mkdir(demo_example_path)\n    print(f'[INFO] \"{demo_project_path}\" and \"{demo_example_path}\" directory is been created.')\n\n[INFO] \"demo/human_action_recognition\" and \"demo/human_action_recognition/examples\" directory is been created.\n\n\n\n# Saving some example files\nexample_files = ['data/shoot_bow/6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_3.avi',\n                 'data/golf/Golf_Tips_-_Hit_The_Driver_300+_Yards!!!_golf_f_nm_np1_fr_med_0.avi',\n                 'data/climb/(HQ)_Rock_Climbing_-_Free_Solo_Speed_Climb_-_Dan_Osman_climb_f_cm_np1_le_med_2.avi',\n                 'data/punch/AdvancedBoxingTechniquesandExercises_punch_u_nm_np1_ba_med_23.avi',\n                 'data/swing_baseball/BaseballHitinSlowMotion_swing_baseball_f_nm_np1_fr_bad_0.avi']\n\nfor i in example_files:\n    # Copying the files\n    shutil.copy2(src=i, dst='demo/human_action_recognition/examples')\n    print(f'{i} File is been copied.')\n\ndata/shoot_bow/6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_3.avi File is been copied.\ndata/golf/Golf_Tips_-_Hit_The_Driver_300+_Yards!!!_golf_f_nm_np1_fr_med_0.avi File is been copied.\ndata/climb/(HQ)_Rock_Climbing_-_Free_Solo_Speed_Climb_-_Dan_Osman_climb_f_cm_np1_le_med_2.avi File is been copied.\ndata/punch/AdvancedBoxingTechniquesandExercises_punch_u_nm_np1_ba_med_23.avi File is been copied.\ndata/swing_baseball/BaseballHitinSlowMotion_swing_baseball_f_nm_np1_fr_bad_0.avi File is been copied.\n\n\n\n# Saving the classes names to text file\nclass_names_path = os.path.join(demo_project_path, 'class_names.txt')\n\nwith open(class_names_path, 'w') as f:\n    f.write('\\n'.join(dataset.classes))\n    print(f'[INFO] Saving class names to: \"{class_names_path}\"')\n\n[INFO] Saving class names to: \"demo/human_action_recognition/class_names.txt\"\n\n\n\n# Reading the text file\nwith open(class_names_path, 'r') as f:\n    class_names_read = [i.strip() for i in f.readlines()]\n\nclass_names_read[:5]\n\n['smoke', 'dive', 'cartwheel', 'throw', 'hit']\n\n\n\n\nSaving and Loading the Model\n\nmodel_save_name = 'mvit_v2_pretrained_model_hmdb51.pth'\nmodel_save_path = os.path.join(demo_project_path, model_save_name)\n\n\n# saving the model state\ntorch.save(obj=model.state_dict(), \n           f=model_save_path)\nprint(f'[INFO] Model is been saved to: \"{model_save_path}\"')\n\n[INFO] Model is been saved to: \"demo/human_action_recognition/mvit_v2_pretrained_model_hmdb51.pth\"\n\n\n\n# loading the model state\nmodel, transforms = create_model(num_classes=NUM_CLASSES, device=device)\nmodel.load_state_dict(torch.load(model_save_path))\n\n&lt;All keys matched successfully&gt;\n\n\n\n\nCreating a text file for Modules and Packages\n\n%%writefile demo/human_action_recognition/requirements.txt\nav==10.0.0\ntorch==2.0.0\ntorchvision==0.15.1\ntorchaudio==2.0.1\ngradio==3.24.1\n\nWriting demo/human_action_recognition/requirements.txt\n\n\n\n\nCreating a Python Script to Preprocess the Data\n\n%%writefile demo/human_action_recognition/utils.py\nimport torch\nimport torchvision\n\ndef preprocess_video(video: str):\n    \"\"\"\n    A function to preprocess the video file before going into the model.\n    Parameters: \n        video: str, A string for the video file path.\n    Returns: selected_frame: torch.Tensor, A tensor of shape 'TCHW'.\n    \"\"\"\n    # Reading the video file\n    vframes, _, _ = torchvision.io.read_video(filename=video, pts_unit='sec', output_format='TCHW')\n    vframes = vframes.type(torch.float32)\n    vframes_count = len(vframes)\n    \n    # Selecting frames at certain interval\n    skip_frames = max(int(vframes_count/16), 1)\n    \n    # Selecting the first frame\n    selected_frame = vframes[0].unsqueeze(0)\n    \n    # Creating a new sequence of frames upto the defined sequence length.\n    for i in range(1, 16):\n        selected_frame = torch.concat((selected_frame, vframes[i * skip_frames].unsqueeze(0)))\n    return selected_frame\n\nWriting demo/human_action_recognition/utils.py\n\n\n\n\nCreating a Python Script to Create the Model\n\n%%writefile demo/human_action_recognition/model.py\nimport torch\nimport torchvision\n\ndef create_model(num_classes: int, seed: int = 42):\n    \"\"\"\n    A function to create a model.\n    Parameters:\n        num_classes: int, A integer for toal number of classes.\n        seed: int(default: 42), A random seed value.\n    Returns: \n        model: A feature extracted model for video classification.\n        transforms: A torchvision transform is returned which was used in the pretrained model.    \n    \"\"\"\n    # Creating model, weights and transforms\n    weights = torchvision.models.video.MViT_V2_S_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.video.mvit_v2_s(weights=weights)\n    \n    # Freezing the model layers\n    for params in model.parameters():\n        params.requires_grad = False\n        \n    # Changing the fully Conncected head layer\n    torch.manual_seed(seed)\n    dropout_layer = model.head[0]\n    in_features = model.head[1].in_features\n    model.head = torch.nn.Sequential(\n        dropout_layer,\n        torch.nn.Linear(in_features=in_features, out_features=num_classes, bias=True)\n    )\n    return model, transforms\n\nWriting demo/human_action_recognition/model.py\n\n\n\n\nCreating a Python Script for Gradio App\n\n%%writefile demo/human_action_recognition/app.py\nimport gradio as gr\nimport os\nimport torch\nimport torchvision\nfrom timeit import default_timer as timer\nfrom model import create_model\nfrom utils import preprocess_video\n\n# Configuring the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Setting class names using the text file\nwith open('class_names.txt', 'r') as f:\n    class_names = [i.strip() for i in f.readlines()]\n\n# Setting the model and transforms\nmodel, transforms = create_model(num_classes=len(class_names), seed=42)\nmodel.load_state_dict(torch.load(f='mvit_v2_pretrained_model_hmdb51.pth', map_location=device))\n\n# Creating a function to predict the video\ndef predict(video_file):\n    \"\"\"\n    A function to predict the video using the model.\n    Parameters: \n        video_file: str, A video file path as a string.\n    Returns: \n        pred_labels_probs: A dict file containing the class names and confidence values.\n        pred_time: Time taken to predict in seconds.\n    \"\"\"\n    # Preprocessing the video file\n    frames = preprocess_video(video=video_file)\n    \n    # transforming the frames\n    mod_frames = transforms(frames).unsqueeze(dim=0)\n    \n    # Starting the timer and predicting using the model\n    start_time = timer()\n    model.eval()\n    \n    # forward pass\n    with torch.no_grad():\n        logits = model(mod_frames.to(device)) \n        pred_probs = torch.softmax(logits, dim=1).squeeze(dim=0)\n    \n    # Creating a dict with class names and their predicted probabilities\n    pred_labels_probs = {class_names[i]: float(pred_probs[i]) for i in range(len(class_names))}\n        \n    # Ending the timer and calculating the prediction time.\n    end_time = timer()\n    pred_time = round(end_time - start_time, 5)\n    \n    # Returning the labels and time for gradio output.\n    return pred_labels_probs, pred_time\n\n# Pre-information for interface\ntitle = 'Human Activity Recognition(HAR)'\ndescription = 'A Gradio demo web application for video classification using the MViT V2 Pretrained Model and trained on the HMDB51 Dataset.'\narticle = 'Created by John'\nexample_list = [['examples/'+ i] for i in os.listdir('examples')]\n\n# Building the Gradio interface\ndemo = gr.Interface(fn=predict,\n                    inputs=gr.Video(label='Video'),\n                    outputs=[gr.Label(num_top_classes=5, label='Predictions'),\n                             gr.Number(label='Prediction Time (sec)')],\n                    examples=example_list,\n                    title=title,\n                    description=description,\n                    article=article)\n\n# Launching the gradio interface\ndemo.launch()\n\nWriting demo/human_action_recognition/app.py\n\n\n\n\nDeploying on HuggingFace\n\nIFrame(src='https://hf.space/embed/JohnPinto/Human_Activity_Recognition-HAR-Video_Classification-HMDB51-Dataset',\n       width=1050,\n       height=700)"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/blog_figures.html",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/blog_figures.html",
    "title": "John Pinto",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nexp1, exp2 = pd.read_csv('/content/exp1_results.csv'), pd.read_csv('/content/exp2_results.csv')\nexp3, exp4 = pd.read_csv('/content/lightning.csv'), pd.read_csv('/content/lightning_compile.csv')\nexp5, exp6 = pd.read_csv('/content/exp5_results.csv'), pd.read_csv('/content/exp6_results.csv')\nexp7, exp8 = pd.read_csv('/content/exp7_results.csv'), pd.read_csv('/content/exp8_results.csv')\n\n\nepochs = np.array(range(len(exp1)))\nlabel_fs = 14\ntitle_fs = 16\nstitle_fs = 18\n\n\nplt.figure(figsize=(25, 8))\nplt.subplot(1, 3, 1)\nplt.plot(epochs, exp1['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp1['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp2['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp2['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 2)\nplt.plot(epochs, exp1['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp1['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp2['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp2['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 3)\nplt.plot(epochs, exp1['train_epoch_time(min)'], label='eager_train_time')\nplt.plot(epochs, exp1['test_epoch_time(min)'], label='eager_val_time')\nplt.plot(epochs, exp2['train_epoch_time(min)'], label='compile_train_time')\nplt.plot(epochs, exp2['test_epoch_time(min)'], label='compile_val_time')\nplt.title('Time (min)', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend()\nplt.suptitle('PyTorch Eager and Compile Mode', fontsize=stitle_fs, fontweight='bold');\n\n\n\n\nFigure 1: Benchmark Comparison between PyTorch Eager and Compile Mode for 3 Epochs.\n\n\n\n\n\nexp3_dict = []\nfor i, dfg in exp3.groupby('epoch'):\n    agg = dict(dfg.mean())\n    agg['epoch'] = i\n    exp3_dict.append(agg)\nexp4_dict = []\nfor i, dfg in exp3.groupby('epoch'):\n    agg = dict(dfg.mean())\n    agg['epoch'] = i\n    exp4_dict.append(agg)\nexp3, exp4 = pd.DataFrame(exp3_dict), pd.DataFrame(exp4_dict)\n\n\nplt.figure(figsize=(20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, exp3['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp3['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp4['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp4['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=label_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 2, 2)\nplt.plot(epochs, exp3['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp3['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp4['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp4['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=label_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.suptitle('PyTorch Lightning Eager and Compile Mode', fontsize=title_fs, fontweight='bold');\n\n\n\n\nFigure 2: Benchmark Comparison between PyTorch Lightning Eager and Compile Mode for 3 Epochs.\n\n\n\n\n\nplt.figure(figsize=(25, 8))\nplt.subplot(1, 3, 1)\nplt.plot(epochs, exp5['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp5['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp6['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp6['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 2)\nplt.plot(epochs, exp5['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp5['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp6['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp6['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 3)\nplt.plot(epochs, exp5['train_epoch_time(min)'], label='eager_train_time')\nplt.plot(epochs, exp5['test_epoch_time(min)'], label='eager_val_time')\nplt.plot(epochs, exp6['train_epoch_time(min)'], label='compile_train_time')\nplt.plot(epochs, exp6['test_epoch_time(min)'], label='compile_val_time')\nplt.title('Time (min)', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend()\nplt.suptitle('Lightning Fabric Eager and Compile Mode', fontsize=stitle_fs, fontweight='bold');\n\n\n\n\nFigure 3: Benchmark Comparison between Lightning Fabric Eager and Compile Mode for 3 Epochs.\n\n\n\n\n\nplt.figure(figsize=(25, 8))\nplt.subplot(1, 3, 1)\nplt.plot(epochs, exp7['train_loss'], label='eager_train_loss')\nplt.plot(epochs, exp7['test_loss'], label='eager_val_loss')\nplt.plot(epochs, exp8['train_loss'], label='compile_train_loss')\nplt.plot(epochs, exp8['test_loss'], label='compile_val_loss')\nplt.title('Loss', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 2)\nplt.plot(epochs, exp7['train_acc'], label='eager_train_acc')\nplt.plot(epochs, exp7['test_acc'], label='eager_val_acc')\nplt.plot(epochs, exp8['train_acc'], label='compile_train_acc')\nplt.plot(epochs, exp8['test_acc'], label='compile_val_acc')\nplt.title('Accuracy', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend();\nplt.subplot(1, 3, 3)\nplt.plot(epochs, exp7['train_epoch_time(min)'], label='eager_train_time')\nplt.plot(epochs, exp7['test_epoch_time(min)'], label='eager_val_time')\nplt.plot(epochs, exp8['train_epoch_time(min)'], label='compile_train_time')\nplt.plot(epochs, exp8['test_epoch_time(min)'], label='compile_val_time')\nplt.title('Time (min)', fontsize=title_fs)\nplt.xlabel('Epochs', fontsize=label_fs)\nplt.legend()\nplt.suptitle('Lightning Fabric (TF16 Mixed Precision) Eager and Compile Mode', fontsize=stitle_fs, fontweight='bold');\n\n\n\n\nFigure 4: Benchmark Comparison between Lightning Fabric (TF16 Mixed Precision) Eager and Compile Mode for 3 Epochs."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "",
    "text": "!pip install -U -q -r requirements.txt\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport os\nimport shutil\nimport rarfile\nimport random\nimport requests\nimport numpy as np\nimport pandas as pd\nimport gradio as gr\nfrom tqdm.auto import tqdm\nfrom timeit import default_timer as timer\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\nimport cv2\nfrom IPython.display import Video, IFrame\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torchmetrics\nfrom torchinfo import summary\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, utils\nfrom torchvision.io import read_video\nfrom torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights\nimport lightning as L\nfrom lightning.fabric import Fabric\n\n\nprint(f'Torch version: {torch.__version__}')\nprint(f'Torchvision version: {torchvision.__version__}')\nprint(f'Lightning version: {L.__version__}')\nprint(f'GPU score: {torch.cuda.get_device_capability()}')\n\nTorch version: 2.0.0+cu117\nTorchvision version: 0.15.1+cu117\nLightning version: 2.0.1\nGPU score: (8, 6)\n\n\n\n# Setting seed for randomness\ndef set_seed(seed: int = 42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\nset_seed(42)\n\n# setting the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cuda')"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#importing-libraries",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#importing-libraries",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "",
    "text": "!pip install -U -q -r requirements.txt\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport os\nimport shutil\nimport rarfile\nimport random\nimport requests\nimport numpy as np\nimport pandas as pd\nimport gradio as gr\nfrom tqdm.auto import tqdm\nfrom timeit import default_timer as timer\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\nimport cv2\nfrom IPython.display import Video, IFrame\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torchmetrics\nfrom torchinfo import summary\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, utils\nfrom torchvision.io import read_video\nfrom torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights\nimport lightning as L\nfrom lightning.fabric import Fabric\n\n\nprint(f'Torch version: {torch.__version__}')\nprint(f'Torchvision version: {torchvision.__version__}')\nprint(f'Lightning version: {L.__version__}')\nprint(f'GPU score: {torch.cuda.get_device_capability()}')\n\nTorch version: 2.0.0+cu117\nTorchvision version: 0.15.1+cu117\nLightning version: 2.0.1\nGPU score: (8, 6)\n\n\n\n# Setting seed for randomness\ndef set_seed(seed: int = 42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\nset_seed(42)\n\n# setting the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cuda')"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#downloading-the-dataset",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#downloading-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Downloading the Dataset",
    "text": "Downloading the Dataset\nDownloading the data from the dataset origin.\n\nDATA_URL = 'http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar'\nRAW_DATA_PATH = 'raw_data'\nDATA_PATH = 'data'\nHMDB_DATA_PATH = os.path.join(DATA_PATH, 'HMDB51')\n\n\nif os.path.isfile('hmdb51_org.rar'):\n    print('[INFO] Data already exists!')\nelse:\n    print('[INFO] Downloading data.')\n    r = requests.get(DATA_URL)\n    with open('hmdb51_org.rar', 'wb') as file:\n        file.write(r.content)\n        file.close()\n    print('[INFO] Data is been downloaded!')\n\n[INFO] Downloading data.\n[INFO] Data is been downloaded!\n\n\nExtracting the data in the raw_data directory\n\nif os.path.exists(RAW_DATA_PATH):\n    print('[INFO] Data path already exists!')\nelse:\n    print('[INFO] Extracting data.')\n    os.mkdir(RAW_DATA_PATH)\n    r = rarfile.RarFile('hmdb51_org.rar')\n    r.extractall(RAW_DATA_PATH)\n    r.close()\n    print('[INFO] Data extraction done!')\n\n[INFO] Extracting data.\n[INFO] Data extraction done!\n\n\nExtracting all the raw data in the data directory and deleting the raw data from the system..\n\nif os.path.exists(DATA_PATH):\n    print('[INFO] Data path already exists!')\nelse:\n    print('[INFO] Extracting all the classes data.')\n    os.mkdir(DATA_PATH)\n    for data in os.listdir(RAW_DATA_PATH):\n        r = rarfile.RarFile(os.path.join(RAW_DATA_PATH, data))\n        r.extractall(DATA_PATH)\n        r.close()\n        print(f'[INFO] Data extraction done for: \"{os.path.join(RAW_DATA_PATH, data)}\"!')\n    os.remove('hmdb51_org.rar')\n    shutil.rmtree(RAW_DATA_PATH)\n    print(f'\\n[INFO] Deleted raw data.')\n\n[INFO] Extracting all the classes data.\n[INFO] Data extraction done for: \"raw_data/clap.rar\"!\n[INFO] Data extraction done for: \"raw_data/talk.rar\"!\n[INFO] Data extraction done for: \"raw_data/smoke.rar\"!\n[INFO] Data extraction done for: \"raw_data/draw_sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/flic_flac.rar\"!\n[INFO] Data extraction done for: \"raw_data/dive.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_bow.rar\"!\n[INFO] Data extraction done for: \"raw_data/drink.rar\"!\n[INFO] Data extraction done for: \"raw_data/hug.rar\"!\n[INFO] Data extraction done for: \"raw_data/pour.rar\"!\n[INFO] Data extraction done for: \"raw_data/pushup.rar\"!\n[INFO] Data extraction done for: \"raw_data/hit.rar\"!\n[INFO] Data extraction done for: \"raw_data/wave.rar\"!\n[INFO] Data extraction done for: \"raw_data/run.rar\"!\n[INFO] Data extraction done for: \"raw_data/walk.rar\"!\n[INFO] Data extraction done for: \"raw_data/fall_floor.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_horse.rar\"!\n[INFO] Data extraction done for: \"raw_data/turn.rar\"!\n[INFO] Data extraction done for: \"raw_data/situp.rar\"!\n[INFO] Data extraction done for: \"raw_data/jump.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/dribble.rar\"!\n[INFO] Data extraction done for: \"raw_data/sit.rar\"!\n[INFO] Data extraction done for: \"raw_data/chew.rar\"!\n[INFO] Data extraction done for: \"raw_data/pick.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick.rar\"!\n[INFO] Data extraction done for: \"raw_data/eat.rar\"!\n[INFO] Data extraction done for: \"raw_data/fencing.rar\"!\n[INFO] Data extraction done for: \"raw_data/swing_baseball.rar\"!\n[INFO] Data extraction done for: \"raw_data/stand.rar\"!\n[INFO] Data extraction done for: \"raw_data/handstand.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb.rar\"!\n[INFO] Data extraction done for: \"raw_data/kiss.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/push.rar\"!\n[INFO] Data extraction done for: \"raw_data/throw.rar\"!\n[INFO] Data extraction done for: \"raw_data/cartwheel.rar\"!\n[INFO] Data extraction done for: \"raw_data/pullup.rar\"!\n[INFO] Data extraction done for: \"raw_data/brush_hair.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb_stairs.rar\"!\n[INFO] Data extraction done for: \"raw_data/laugh.rar\"!\n[INFO] Data extraction done for: \"raw_data/punch.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_bike.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword_exercise.rar\"!\n[INFO] Data extraction done for: \"raw_data/somersault.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_gun.rar\"!\n[INFO] Data extraction done for: \"raw_data/catch.rar\"!\n[INFO] Data extraction done for: \"raw_data/smile.rar\"!\n[INFO] Data extraction done for: \"raw_data/golf.rar\"!\n[INFO] Data extraction done for: \"raw_data/shake_hands.rar\"!\n\n[INFO] Deleted raw data.\n\n\nChecking some info on the data that is been extracted.\n\nfor dirpath, dirnames, filenames in os.walk(DATA_PATH):\n    print(f'There are {len(dirnames)} directories and {len(filenames)} files in \"{dirpath}\".')\n\nThere are 51 directories and 0 files in \"data\".\nThere are 0 directories and 143 files in \"data/swing_baseball\".\nThere are 0 directories and 232 files in \"data/run\".\nThere are 0 directories and 105 files in \"data/situp\".\nThere are 0 directories and 162 files in \"data/shake_hands\".\nThere are 0 directories and 107 files in \"data/flic_flac\".\nThere are 0 directories and 127 files in \"data/hit\".\nThere are 0 directories and 102 files in \"data/kiss\".\nThere are 0 directories and 120 files in \"data/talk\".\nThere are 0 directories and 164 files in \"data/drink\".\nThere are 0 directories and 104 files in \"data/wave\".\nThere are 0 directories and 140 files in \"data/somersault\".\nThere are 0 directories and 116 files in \"data/push\".\nThere are 0 directories and 116 files in \"data/ride_horse\".\nThere are 0 directories and 128 files in \"data/kick_ball\".\nThere are 0 directories and 105 files in \"data/golf\".\nThere are 0 directories and 112 files in \"data/shoot_bow\".\nThere are 0 directories and 116 files in \"data/fencing\".\nThere are 0 directories and 102 files in \"data/smile\".\nThere are 0 directories and 145 files in \"data/dribble\".\nThere are 0 directories and 154 files in \"data/stand\".\nThere are 0 directories and 130 files in \"data/clap\".\nThere are 0 directories and 548 files in \"data/walk\".\nThere are 0 directories and 102 files in \"data/throw\".\nThere are 0 directories and 151 files in \"data/jump\".\nThere are 0 directories and 103 files in \"data/shoot_gun\".\nThere are 0 directories and 131 files in \"data/shoot_ball\".\nThere are 0 directories and 102 files in \"data/catch\".\nThere are 0 directories and 103 files in \"data/draw_sword\".\nThere are 0 directories and 142 files in \"data/sit\".\nThere are 0 directories and 127 files in \"data/sword_exercise\".\nThere are 0 directories and 108 files in \"data/climb\".\nThere are 0 directories and 104 files in \"data/pullup\".\nThere are 0 directories and 126 files in \"data/punch\".\nThere are 0 directories and 103 files in \"data/pushup\".\nThere are 0 directories and 127 files in \"data/sword\".\nThere are 0 directories and 108 files in \"data/eat\".\nThere are 0 directories and 109 files in \"data/chew\".\nThere are 0 directories and 128 files in \"data/laugh\".\nThere are 0 directories and 103 files in \"data/ride_bike\".\nThere are 0 directories and 118 files in \"data/hug\".\nThere are 0 directories and 107 files in \"data/cartwheel\".\nThere are 0 directories and 106 files in \"data/pick\".\nThere are 0 directories and 130 files in \"data/kick\".\nThere are 0 directories and 113 files in \"data/handstand\".\nThere are 0 directories and 240 files in \"data/turn\".\nThere are 0 directories and 112 files in \"data/climb_stairs\".\nThere are 0 directories and 127 files in \"data/dive\".\nThere are 0 directories and 107 files in \"data/brush_hair\".\nThere are 0 directories and 106 files in \"data/pour\".\nThere are 0 directories and 109 files in \"data/smoke\".\nThere are 0 directories and 136 files in \"data/fall_floor\"."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#visualizing-the-dataset",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#visualizing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Visualizing the dataset",
    "text": "Visualizing the dataset\nNow that the data is downloaded, lets view the data with the labels.\n\nplt.figure(figsize=(20, 20))\nCLASSES = sorted(os.listdir(DATA_PATH))\n\n# Selecting 20 random categories for visualization\nrandom_range = random.sample(range(len(CLASSES)), 20)\n\nfor i, rand_i in enumerate(random_range, 1):\n    class_name = CLASSES[rand_i]\n\n    # Looking for the video files in the category and selecting it randomly\n    video_files_list = os.listdir(os.path.join(DATA_PATH, class_name))\n    video_file_name = random.choice(video_files_list)\n\n    # Reading the video file and displaying the first frame with label.\n    video_reader = cv2.VideoCapture(os.path.join(DATA_PATH, class_name, video_file_name))\n    _, frame = video_reader.read()\n    video_reader.release()\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    cv2.putText(rgb_frame, class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n    plt.subplot(5, 4, i)\n    plt.title(f'Category: {class_name},\\nShape: {rgb_frame.shape}')\n    plt.imshow(rgb_frame)\n    plt.axis(False);"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#preprocessing-the-dataset",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#preprocessing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Preprocessing the Dataset",
    "text": "Preprocessing the Dataset\nCreating pytorch Dataset.\nIn this steps we will be creating a custom dataset class using the functionality provided by PyTorch. 1. Read the video file. 2. Extracting the frames after every distance for a max sequence length. 3. Assigning label index for all the classes. 4. Performing torchvision.transform on the video data. 5. Returning tuple of video and label data as a tensor.\n\n# Defining variables\nSEQUENCE_LENGTH = 16 # Frames feeded to the model as a single sequence\nNUM_CLASSES = 20 # Selecting 20 classes from 51 classes\n\nBefore creating the dataset, first lets clean it. Here we will check whether the videos are readable and the frame count needs to be more than the Sequence Length frames.\n\nfiles_list = [os.path.join(DATA_PATH, dir, video) for dir in CLASSES for video in os.listdir(DATA_PATH + \"/\" + dir)]\nremove_file_list = []\nfor file in files_list:\n    vframes, _, _ = read_video(filename=file, pts_unit='sec', output_format='TCHW')\n    if len(vframes) &lt;= SEQUENCE_LENGTH:\n        print(f'File Name: {file} and Total Frames Count: {len(vframes)}')\n        remove_file_list.append(file)\nremove_file_list\n\n[]\n\n\n\n# Deleting the video files\nif len(remove_file_list) &gt; 0:\n    for file_path in remove_file_list:\n        try: \n            os.remove(path=file_path)\n            print(f'File \"{file_path}\" is been deleted')\n        except FileNotFoundError:\n            print('File is missing or is been already deleted')\nelse:\n    print('There are no files to delete.')\n\nThere are no files to delete.\n\n\n\nclass HumanActionDataset(Dataset):\n    \"\"\"\n    A class to create a PyTorch dataset using the video files directory,\n    transforming the video data and returning the data and labels for every index.\n    \n    Parameters:\n        video_dir: str, A string containing the path for all the video categories or classes.\n        seq_len: int(default: 20), A integer specifing the required total sequence frames.\n        num_classes: int(default: 20), Selecting random classes and the max allowed is 51 classes for this dataset.\n        transform: default: None, A torchvision transforms for applying multiple transformation on every frame.\n    Returns:\n        selected_frame, label: tuple, A tuple containing the video \"TCHW\" and label data in tensor format. \n    \"\"\"\n    def __init__(self, video_dir: str, seq_len: int, num_classes: int, transform=None):\n        self.video_dir = video_dir\n        self.seq_len = seq_len\n        self.num_classes = num_classes\n        self.transform = transform \n        \n        # Creating a list of all the video files path and classes.\n        set_seed(seed=42)\n        if self.num_classes &gt; 51:\n            self.num_classes = 51\n        self.classes = random.sample(sorted(os.listdir(video_dir)), k=self.num_classes)\n        self.files_list = [os.path.join(video_dir, dir, video) for dir in self.classes for video in os.listdir(video_dir + \"/\" + dir)]\n        \n    def __len__(self):\n        return len(self.files_list)\n    \n    def __getitem__(self, index):\n        video_path = self.files_list[index]\n        \n        # Reading the video file\n        vframes, _, _ = read_video(filename=video_path, pts_unit='sec', output_format='TCHW')\n        vframes = vframes.type(torch.float32)\n        vframes_count = len(vframes)\n        \n        # Selecting frames at certain interval\n        skip_frames = max(int(vframes_count/self.seq_len), 1)\n        selected_frame = vframes[0].unsqueeze(0)\n        \n        # Creating a new sequence of frames upto the defined sequence length.\n        for i in range(1, self.seq_len):\n            selected_frame = torch.concat((selected_frame, vframes[i * skip_frames].unsqueeze(0)))\n            \n        # Video label as per the classes list.\n        label = torch.tensor(self.classes.index(video_path.split('/')[1]))\n        \n        # Applying transformation to the frames\n        if self.transform:\n            return self.transform(selected_frame), label\n        else:\n            return selected_frame, label\n\n\n# Creating the dataset\ndataset = HumanActionDataset(video_dir=DATA_PATH, seq_len=SEQUENCE_LENGTH, num_classes=NUM_CLASSES)\nprint(f'Length of the dataset: {len(dataset)}')\n\n# Checking the dataset shape\nframes, label = dataset[2000]\nprint(f'Video tensor(first frame):\\n{frames[0]}\\nlabel tensor: {label}')\nprint(f'Shape of the video tensor: {frames.shape} and shape of the label: {label.shape}')\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n         [ 2.,  2.,  0.,  ..., 11., 11., 11.],\n         ...,\n         [ 2.,  2.,  0.,  ..., 55., 55., 55.],\n         [ 5.,  5.,  3.,  ..., 62., 62., 62.],\n         [ 5.,  5.,  2.,  ..., 34., 34., 34.]],\n\n        [[ 4.,  4.,  5.,  ...,  2.,  1.,  1.],\n         [ 5.,  5.,  5.,  ...,  3.,  3.,  3.],\n         [ 4.,  4.,  5.,  ..., 17., 17., 17.],\n         ...,\n         [ 2.,  2.,  2.,  ..., 41., 41., 41.],\n         [ 0.,  0.,  1.,  ..., 49., 49., 49.],\n         [ 0.,  0.,  0.,  ..., 21., 21., 21.]],\n\n        [[ 3.,  3.,  4.,  ...,  2.,  1.,  1.],\n         [ 4.,  4.,  4.,  ...,  3.,  3.,  3.],\n         [ 1.,  1.,  1.,  ...,  8.,  8.,  8.],\n         ...,\n         [ 0.,  0.,  0.,  ..., 16., 14., 14.],\n         [ 0.,  0.,  0.,  ..., 28., 28., 28.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([16, 3, 240, 320]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-a-pretrained-model",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-a-pretrained-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating A Pretrained Model",
    "text": "Creating A Pretrained Model\n\ndef create_model(num_classes: int, device: torch.device, seed: int = 42):\n    \"\"\"\n    A function to create a model.\n    Parameters:\n        num_classes: int, A integer for toal number of classes.\n        device: torch.device, A torch device to set the tensors to cpu or cuda.\n        seed: int(default: 42), A random seed value.\n    Returns: \n        model: A feature extracted model for video classification.\n        transforms: A torchvision transform is returned which was used in the pretrained model.    \n    \"\"\"\n    # Creating model, weights and transforms\n    weights = MViT_V2_S_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = mvit_v2_s(weights=weights)\n    \n    # Freezing the model layers\n    for params in model.parameters():\n        params.requires_grad = False\n        \n    # Changing the fully Conncected head layer\n    set_seed(seed)\n    dropout_layer = model.head[0]\n    in_features = model.head[1].in_features\n    model.head = torch.nn.Sequential(\n        dropout_layer,\n        torch.nn.Linear(in_features=in_features, out_features=num_classes, bias=True, device=device)\n    )\n    return model.to(device), transforms\n\n\n# Creating the model and transforms using the function.\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\n\n\nsummary(model, \n        input_size=(1, 3, 16, 224, 224),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])\n\n/usr/local/lib/python3.9/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  action_fn=lambda data: sys.getsizeof(data.storage()),\n/usr/local/lib/python3.9/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return super().__sizeof__() + self.nbytes()\n\n\n==================================================================================================================================\nLayer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n==================================================================================================================================\nMViT (MViT)                                        [1, 3, 16, 224, 224] [1, 20]              --                   Partial\n├─Conv3d (conv_proj)                               [1, 3, 16, 224, 224] [1, 96, 8, 56, 56]   (42,432)             False\n├─PositionalEncoding (pos_encoding)                [1, 25088, 96]       [1, 25089, 96]       (96)                 False\n├─ModuleList (blocks)                              --                   --                   --                   False\n│    └─MultiscaleBlock (0)                         [1, 25089, 96]       [1, 25089, 96]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 25089, 96]       (68,352)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MLP (mlp)                              [1, 25089, 96]       [1, 25089, 96]       (74,208)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    └─MultiscaleBlock (1)                         [1, 25089, 96]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 6273, 192]       (113,280)            False\n│    │    └─Linear (project)                       [1, 25089, 96]       [1, 25089, 192]      (18,624)             False\n│    │    └─Pool (pool_skip)                       [1, 25089, 192]      [1, 6273, 192]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (2)                         [1, 6273, 192]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 6273, 192]       (168,576)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (3)                         [1, 6273, 192]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 1569, 384]       (385,152)            False\n│    │    └─Linear (project)                       [1, 6273, 192]       [1, 6273, 384]       (74,112)             False\n│    │    └─Pool (pool_skip)                       [1, 6273, 384]       [1, 1569, 384]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (4)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (5)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (6)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (7)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (8)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (9)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (10)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (11)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (12)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (13)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (14)                        [1, 1569, 384]       [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 393, 768]        (1,492,608)          False\n│    │    └─Linear (project)                       [1, 1569, 384]       [1, 1569, 768]       (295,680)            False\n│    │    └─Pool (pool_skip)                       [1, 1569, 768]       [1, 393, 768]        --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    └─MultiscaleBlock (15)                        [1, 393, 768]        [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MultiscaleAttention (attn)             [1, 393, 768]        [1, 393, 768]        (2,374,656)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n├─LayerNorm (norm)                                 [1, 393, 768]        [1, 393, 768]        (1,536)              False\n├─Sequential (head)                                [1, 768]             [1, 20]              --                   True\n│    └─Dropout (0)                                 [1, 768]             [1, 768]             --                   --\n│    └─Linear (1)                                  [1, 768]             [1, 20]              15,380               True\n==================================================================================================================================\nTotal params: 34,245,524\nTrainable params: 15,380\nNon-trainable params: 34,230,144\nTotal mult-adds (G): 1.64\n==================================================================================================================================\nInput size (MB): 9.63\nForward/backward pass size (MB): 1658.93\nParams size (MB): 136.46\nEstimated Total Size (MB): 1805.02\n==================================================================================================================================\n\n\n\n# Checking the model transforms and applying it on the dataset\nprint(f'Pretrained Model Transforms:\\n{transforms}\\n')\ndataset = HumanActionDataset(video_dir=DATA_PATH, seq_len=SEQUENCE_LENGTH, num_classes=NUM_CLASSES, transform=transforms)\nprint(f'Length of the dataset: {len(dataset)}')\n\n# Checking the dataset and shape\nframes, label = dataset[2000]\nprint(f'Video tensor(first frame):\\n{frames[0]}\\nlabel tensor: {label}')\nprint(f'Shape of the video tensor: {frames.shape} and shape of the label: {label.shape}')\n\nPretrained Model Transforms:\nVideoClassification(\n    crop_size=[224, 224]\n    resize_size=[256]\n    mean=[0.45, 0.45, 0.45]\n    std=[0.225, 0.225, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 2.4604e+02,  2.3502e+02,  2.2911e+02,  ...,  2.8675e+02,\n           2.8675e+02,  2.8684e+02],\n         [ 2.3070e+02,  2.2561e+02,  2.1813e+02,  ...,  3.1508e+02,\n           3.1508e+02,  3.1508e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1425e+02,\n           3.1425e+02,  3.1425e+02],\n         ...,\n         [ 3.2401e+00,  2.9817e+00, -2.0000e+00,  ...,  1.6333e+02,\n           1.2992e+02,  9.1900e+01],\n         [-1.5833e+00, -1.7232e+00, -2.0000e+00,  ...,  4.4867e+02,\n           3.7443e+02,  2.8330e+02],\n         [ 2.4444e+00,  9.5210e-01, -2.0000e+00,  ...,  6.6754e+02,\n           6.3805e+02,  5.6134e+02]],\n\n        [[ 2.2467e+02,  2.2616e+02,  2.2911e+02,  ...,  3.0883e+02,\n           3.0883e+02,  3.0902e+02],\n         [ 2.2869e+02,  2.2478e+02,  2.1813e+02,  ...,  3.1314e+02,\n           3.1314e+02,  3.1581e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1356e+02,\n           3.1356e+02,  3.1402e+02],\n         ...,\n         [ 9.1056e+00,  6.2105e+00, -2.0000e+00,  ...,  9.9700e+01,\n           7.6676e+01,  2.8844e+01],\n         [ 1.9746e+01,  1.7561e+01, -1.1667e+00,  ...,  2.1729e+02,\n           1.6955e+02,  1.1592e+02],\n         [ 6.6111e+00,  6.6111e+00,  6.6111e+00,  ...,  5.4941e+02,\n           5.2508e+02,  4.3940e+02]],\n\n        [[-2.0000e+00,  5.3684e+00,  2.1125e+01,  ...,  3.2244e+02,\n           3.1922e+02,  3.2077e+02],\n         [-3.9952e-01,  4.0798e+00,  9.3575e+00,  ...,  3.2330e+02,\n           3.1922e+02,  3.1025e+02],\n         [ 2.0259e+00,  2.6776e+00,  5.3857e+00,  ...,  3.3464e+02,\n           3.3047e+02,  3.1790e+02],\n         ...,\n         [ 1.7578e+02,  1.7578e+02,  1.7719e+02,  ...,  2.3105e+02,\n           2.2658e+02,  2.1946e+02],\n         [ 1.7803e+02,  1.7884e+02,  1.8227e+02,  ...,  2.3562e+02,\n           2.2931e+02,  2.2306e+02],\n         [ 1.9990e+02,  2.0851e+02,  2.2140e+02,  ...,  2.2817e+02,\n           2.1964e+02,  2.2187e+02]],\n\n        ...,\n\n        [[ 4.2491e+02,  4.3142e+02,  4.2619e+02,  ...,  1.5639e+01,\n           1.5639e+01,  1.5639e+01],\n         [ 4.0218e+02,  4.0793e+02,  4.1056e+02,  ...,  6.0095e+00,\n           7.7222e+00,  2.3716e+00],\n         [ 4.0126e+02,  4.0792e+02,  4.0992e+02,  ...,  8.7490e+00,\n           1.0639e+01,  2.2439e+00],\n         ...,\n         [ 1.2787e+02,  1.2839e+02,  1.3253e+02,  ...,  9.5269e-01,\n           1.7500e+00,  1.7500e+00],\n         [ 1.3349e+02,  1.2883e+02,  1.3904e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 1.3694e+02,  1.3320e+02,  1.3960e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.5630e+02,  4.6579e+02,  4.6640e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3329e+02,  4.3661e+02,  4.3776e+02,  ...,  1.1714e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4439e+02,  4.4481e+02,  4.4500e+02,  ...,  1.4995e+00,\n          -7.8133e-01, -2.0000e+00],\n         ...,\n         [ 1.4467e+02,  1.5315e+02,  1.7425e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.3056e+00],\n         [ 1.4625e+02,  1.5174e+02,  1.6260e+02,  ..., -2.0000e+00,\n           9.2336e-01,  2.0278e+00],\n         [ 1.6148e+02,  1.6227e+02,  1.5224e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.2527e+02,  4.4359e+02,  4.6669e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3037e+02,  4.3994e+02,  4.5392e+02,  ...,  2.0278e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4064e+02,  4.4451e+02,  4.5152e+02,  ...,  2.4444e+00,\n          -7.8133e-01,  4.9081e-01],\n         ...,\n         [ 1.4709e+02,  1.5459e+02,  1.5781e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.7668e+00],\n         [ 1.4016e+02,  1.5057e+02,  1.5565e+02,  ..., -1.5833e+00,\n           1.3400e+00,  4.5890e-02],\n         [ 1.3139e+02,  1.3590e+02,  1.4467e+02,  ...,  2.4444e+00,\n           2.4444e+00,  5.3965e+00]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([3, 16, 224, 224]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-dataloaders",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-dataloaders",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating DataLoaders",
    "text": "Creating DataLoaders\n\n# Spliting the dataset in train and test dataset for ratio of 75:25.\ndataset = HumanActionDataset(video_dir=DATA_PATH, \n                             seq_len=SEQUENCE_LENGTH,\n                             num_classes=NUM_CLASSES,\n                             transform=transforms)\ntrain_dataset, test_dataset = random_split(dataset=dataset,\n                                           lengths=[0.75, 0.25], \n                                           generator=torch.Generator().manual_seed(42))\nprint(f'Train Dataset Length: {len(train_dataset)} and Test Dataset length: {len(test_dataset)}')\n\nTrain Dataset Length: 1898 and Test Dataset length: 632\n\n\n\n# Creating train and test dataloaders.\ndef create_dataloaders(dataset, batch, shuffle, workers):\n    \"\"\"\n    A function to create pytorch dataloaders.\n    \n    Parameters:\n        dataset: A pytorch dataset.\n        batch: A integer for batch size.\n        shuffle: A boolean for shuffling the data.\n        workers: A integer to set the workers in dataloaders.\n        \n    Returns: dataloader.\n    \"\"\"\n    dataloader = DataLoader(dataset=dataset, \n                            batch_size=batch, \n                            shuffle=shuffle, \n                            num_workers=workers,\n                            pin_memory=True,\n                            drop_last=True)\n    print(f'Dataloaders Length: {len(dataloader)} for a batch size of: {batch}')\n    return dataloader\n\nset_seed(42)\nBATCH_SIZE = 16\nWORKERS = os.cpu_count()\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\n\n\nLabel Distribution and Zero Rule Benchmark\n\ntrain_counter = Counter()\nfor frames, labels in train_dataloader:\n    train_counter.update(labels.tolist())\nprint('Training Dataset Distribution:\\n', sorted(train_counter.items()))\n\nTraining Dataset Distribution:\n [(0, 78), (1, 92), (2, 83), (3, 79), (4, 94), (5, 72), (6, 77), (7, 79), (8, 85), (9, 103), (10, 90), (11, 106), (12, 88), (13, 78), (14, 180), (15, 70), (16, 87), (17, 100), (18, 171), (19, 76)]\n\n\n\ntest_counter = Counter()\nfor frames, labels in test_dataloader:\n    test_counter.update(labels.tolist())\nprint('Testing Dataset Distribution:\\n', sorted(test_counter.items()))\n\nTesting Dataset Distribution:\n [(0, 30), (1, 32), (2, 24), (3, 23), (4, 29), (5, 32), (6, 30), (7, 24), (8, 27), (9, 27), (10, 17), (11, 35), (12, 37), (13, 24), (14, 58), (15, 33), (16, 29), (17, 26), (18, 60), (19, 27)]\n\n\n\n# Zero Rule Benchmark\nmajor_class = test_counter.most_common(1)[0]\nprint(f'The Major class in Testing DataLoader is: Class Name: {CLASSES[major_class[0]]}, Class Index: {major_class[0]} and Total Number of Data: {major_class[1]}')\n\nzero_rule_acc = (major_class[1] / sum(test_counter.values())) * 100\nprint(f'Zero Rule Basline Benchmark Accuracy for predicting the major class: {zero_rule_acc:.2f}%')\n\nThe Major class in Testing DataLoader is: Class Name: hug, Class Index: 18 and Total Number of Data: 60\nZero Rule Basline Benchmark Accuracy for predicting the major class: 9.62%"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch - Model Training and Testing",
    "text": "PyTorch - Model Training and Testing\n\ndef train_step(model, dataloader, loss_fn, optimizer, device, num_classes):\n    \"\"\"\n    Trains a pytorch model by going into train mode and applying forward pass,\n    loss calculation and optimizer step.\n    \n    Parameters:\n        model: A pytorch model for training.\n        dataloader: A pytorch dataloader for training.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        device: A torch device to allocate tensors on 'cpu' or 'cuda'.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of training loss and training accuracy.\n        \n    \"\"\"\n    # Model on training mode\n    model.train()\n    \n    # Setting train loss and accuracy \n    train_loss = 0\n    train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(device)\n    \n    # Looping the dataloaders\n    for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Training', total=len(dataloader), unit='batch'):\n        X, y = X.to(device), y.to(device)\n        \n        # 5 step to train a model\n        y_pred = model(X) # 1. Forward pass\n        loss = loss_fn(y_pred, y) # 2. Calculate loss\n        train_loss += loss.item() \n        optimizer.zero_grad() # 3. Initiate optimizer\n        loss.backward() # 4. Backward pass\n        optimizer.step() # 5. Updating the model parameters\n        \n        # Calculating the training accuracy\n        y_pred_labels = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc.compute()\n    return train_loss, train_acc\n\ndef test_step(model, dataloader, loss_fn, device, num_classes):\n    \"\"\"\n    Test a pytorch model by going into eval mode and applying forward pass,\n    and loss calculation.\n    \n    Parameters:\n        model: A pytorch model for testing.\n        dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        device: A torch device to allocate tensors on 'cpu' or 'cuda'.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of testing loss and testing accuracy.\n    \"\"\"\n    # Model on evaluation mode\n    model.eval()\n    \n    # Setting train loss and accuracy \n    test_loss = 0\n    test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(device)\n    \n    # Using inference mode\n    with torch.no_grad():\n        # Looping the dataloaders\n        for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Evaluation', total=len(dataloader), unit='batch'):\n            X, y = X.to(device), y.to(device)\n            \n            # Forward pass\n            y_pred = model(X)\n            \n            # Calculate loss\n            loss = loss_fn(y_pred, y)\n            test_loss += loss.item()\n            \n            # Calculate accuracy\n            y_pred_labels = y_pred.argmax(dim=1)\n            test_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc.compute()\n    return test_loss, test_acc\n\ndef model_train(epochs, model, train_dataloader, test_dataloader, optimizer, loss_fn, device, num_classes):\n    \"\"\"\n    Trains a pytorch model for a certain number of epochs going through the model training \n    and testing stage, and accumalating the loss, accuracy, and training and testing time.\n    \n    Parameters:\n        epochs: A integer to run the training and testing stage. \n        model: A pytorch model for training and testing.\n        train_dataloader: A pytorch dataloader for training.\n        test_dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        device: A torch device to allocate tensors on 'cpu' or 'cuda'.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of accumaleted results in dict and total training time in float datatype.\n    \"\"\"\n    # Create empty result\n    results = {'train_loss': [],\n               'train_acc': [],\n               'test_loss': [],\n               'test_acc': [],\n               'train_epoch_time(min)': [],\n               'test_epoch_time(min)': []}\n    \n    # Loop through training and testing steps\n    model_train_start_time = timer()\n    for epoch in tqdm(range(epochs), desc=f'Training and Evaluation for {epochs} Epochs', unit='epochs'):\n        # Training the model and timing it.\n        train_epoch_start_time = timer()\n        train_loss, train_acc = train_step(model=model, \n                                           dataloader=train_dataloader, \n                                           loss_fn=loss_fn, \n                                           optimizer=optimizer, \n                                           device=device, \n                                           num_classes=num_classes)\n        train_epoch_stop_time = timer()\n        train_epoch_time = (train_epoch_stop_time - train_epoch_start_time)/60\n        \n        # Testing the model and timing it\n        test_epoch_start_time = timer()\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        device=device,\n                                        num_classes=num_classes)\n        test_epoch_stop_time = timer()\n        test_epoch_time = (test_epoch_stop_time - test_epoch_start_time)/60\n        \n        # Print the model result\n        print(f'Epoch: [{epoch+1}/{epochs}] | train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | train_time: {train_epoch_time:.4f} min | '\n              f'test loss: {test_loss:.4f} | test_acc: {test_acc:.4f} | test_time: {test_epoch_time:.4f} min')\n        \n        # Saving the results\n        results['train_loss'].append(train_loss)\n        results['train_acc'].append(train_acc.detach().cpu().item())\n        results['test_loss'].append(test_loss)\n        results['test_acc'].append(test_acc.detach().cpu().item())\n        results['train_epoch_time(min)'].append(train_epoch_time)\n        results['test_epoch_time(min)'].append(test_epoch_time)\n        \n    # Calculating total model training time\n    model_train_end_time = timer()\n    total_train_time = (model_train_end_time - model_train_start_time)/60\n    print(f'\\nTotal Model Training Time: {total_train_time:.4f} min')\n    return results, total_train_time\n\n\nExp 1: PyTorch Before 2.0 (PyTorch Eager Mode)\n\nset_seed(42)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\nmodel.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp1_results, exp1_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  device=device, \n                                                  num_classes=len(dataset.classes))\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.4238 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5183 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.4201 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5181 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.3959 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.5176 min\n\nTotal Model Training Time: 5.7940 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Creating a result directory\nRESULTS_DIR = 'results'\nif os.path.exists(RESULTS_DIR):\n    print('[INFO] results directory exists.')\nelse:\n    os.mkdir(RESULTS_DIR)\n    print('[INFO] results directory is been created.')\n\n# Saving the result in csv format\nexp1_results_filename = 'exp1_results.csv'\nexp1_results_df = pd.DataFrame(exp1_results)\nexp1_results_df.to_csv(os.path.join(RESULTS_DIR, exp1_results_filename), index=False)\nexp1_results_df\n\n[INFO] results directory is been created.\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.423781\n0.518263\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.420147\n0.518100\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.395912\n0.517590\n\n\n\n\n\n\n\n\n# Creating and saving a dataframe for model train time \nmodel_train_time_filename = 'model_train_time.csv'\nmodel_train_time_df = pd.DataFrame({'model': ['Pytorch Eager'], 'training_time(min)': [exp1_total_train_time]})\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n\n\n\n\n\n\n\nExp 2: PyTorch 2.0 (PyTorch Compile Mode)\n\nset_seed(42)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\nmodel.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp2_results, exp2_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model,\n                                                  train_dataloader=train_dataloader,\n                                                  test_dataloader=test_dataloader,\n                                                  optimizer=optimizer,\n                                                  loss_fn=loss_fn,\n                                                  device=device,\n                                                  num_classes=len(dataset.classes))\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.3885 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0454 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.6187 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5678 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.6008 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5694 min\n\nTotal Model Training Time: 7.7908 min\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.9/dist-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n  warnings.warn(\n[2023-04-06 06:39:50,385] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp2_results_filename = 'exp2_results.csv'\nexp2_results_df = pd.DataFrame(exp2_results)\nexp2_results_df.to_csv(os.path.join(RESULTS_DIR, exp2_results_filename), index=False)\nexp2_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.388515\n1.045425\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.618742\n0.567790\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.600783\n0.569392\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Pytorch Compile', exp2_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch-lightning---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch-lightning---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch Lightning - Model Training and Testing",
    "text": "PyTorch Lightning - Model Training and Testing\n\nclass PyLightHMDB51(L.LightningModule):\n    \"\"\"\n    A Lightning Module containing Model training and validation step.\n    Parameters: \n        model: A PyTorch Model.\n        loss_fn: A PyTorch loss function.\n        optimizer: A Pytorch Optimizer.\n        num_classes: A integer for total number of classes in the dataset.\n    \"\"\"\n    def __init__(self, model, loss_fn, optimizer, num_classes):\n        super().__init__()\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.num_classes = num_classes\n        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n        self.test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def training_step(self, train_batch, batch_idx):\n        X, y = train_batch\n        y_preds = self.forward(X)\n        loss = self.loss_fn(y_preds, y)\n        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n        y_pred_labels = torch.argmax(torch.softmax(y_preds, dim=1), dim=1)\n        self.train_acc.update(y_pred_labels, y)\n        self.log('train_acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n        return loss\n    \n    def validation_step(self, val_batch, batch_idx):\n        X, y = val_batch\n        y_preds = self.forward(X)\n        loss = self.loss_fn(y_preds, y)\n        self.log('test_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n        y_pred_labels = torch.argmax(torch.softmax(y_preds, dim=1), dim=1)\n        self.test_acc.update(y_pred_labels, y)\n        self.log('test_acc', self.test_acc, prog_bar=True, on_step=False, on_epoch=True)\n    \n    def configure_optimizers(self):\n        optimizers = self.optimizer\n        return optimizers\n\n\nExp 3: PyTorch Lightning\n\nset_seed(42)\n\n# Creating the pytorch lightning trainer\nNUM_EPOCHS = 3\nlogger = L.pytorch.loggers.CSVLogger(save_dir=RESULTS_DIR, \n                                     name=\"pytorch_lightning\")\ntrainer = L.Trainer(max_epochs=NUM_EPOCHS, \n                    logger=logger)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Initializing the lightning module class\nmodel = PyLightHMDB51(model=model, \n                      loss_fn=loss_fn, \n                      optimizer=optimizer, \n                      num_classes=len(dataset.classes))\n\n# Fiting the model to trainer.\nstart_time = timer()\ntrainer.fit(model=model, \n            train_dataloaders=train_dataloader, \n            val_dataloaders=test_dataloader)\nend_time = timer()\nexp3_total_train_time = (end_time - start_time)/60\nprint(f'Total Time to train the model: {exp3_total_train_time:.4f} min')\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nMissing logger folder: results/pytorch_lightning\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type               | Params\n-------------------------------------------------\n0 | model     | MViT               | 34.2 M\n1 | loss_fn   | CrossEntropyLoss   | 0     \n2 | train_acc | MulticlassAccuracy | 0     \n3 | test_acc  | MulticlassAccuracy | 0     \n-------------------------------------------------\n15.4 K    Trainable params\n34.2 M    Non-trainable params\n34.2 M    Total params\n136.982   Total estimated model params size (MB)\n`Trainer.fit` stopped: `max_epochs=3` reached.\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 7.6887 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Pytorch Lightning', exp3_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n\n\n\n\n\n\n\nExp 4: PyTorch Lightning - Compile Mode\n\nset_seed(42)\n\n# Creating the pytorch lightning trainer\nNUM_EPOCHS = 3\nlogger = L.pytorch.loggers.CSVLogger(save_dir=RESULTS_DIR, \n                                     name=\"pytorch_lightning_compile_mode\")\ntrainer = L.Trainer(max_epochs=NUM_EPOCHS, \n                    logger=logger)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Initializing the lightning module class\nmodel = PyLightHMDB51(model=model, \n                      loss_fn=loss_fn, \n                      optimizer=optimizer, \n                      num_classes=len(dataset.classes))\n\n# Fiting the model to trainer.\nstart_time = timer()\ntrainer.fit(model=model, \n            train_dataloaders=train_dataloader, \n            val_dataloaders=test_dataloader)\nend_time = timer()\nexp4_total_train_time = (end_time - start_time)/60\nprint(f'Total Time to train the model: {exp4_total_train_time:.4f} min')\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nMissing logger folder: results/pytorch_lightning_compile_mode\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type               | Params\n-------------------------------------------------\n0 | model     | OptimizedModule    | 34.2 M\n1 | loss_fn   | CrossEntropyLoss   | 0     \n2 | train_acc | MulticlassAccuracy | 0     \n3 | test_acc  | MulticlassAccuracy | 0     \n-------------------------------------------------\n15.4 K    Trainable params\n34.2 M    Non-trainable params\n34.2 M    Total params\n136.982   Total estimated model params size (MB)\n[2023-04-06 06:56:20,261] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n`Trainer.fit` stopped: `max_epochs=3` reached.\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 10.5425 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['PyTorch Lightning + Compile', exp4_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#lightning-fabric---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#lightning-fabric---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Lightning Fabric - Model training and Testing",
    "text": "Lightning Fabric - Model training and Testing\n\ndef train_step(model, dataloader, loss_fn, optimizer, fabric, num_classes):\n    \"\"\"\n    Trains a pytorch model by going into train mode and applying forward pass,\n    loss calculation and optimizer step.\n    \n    Parameters:\n        model: A pytorch model for training.\n        dataloader: A pytorch dataloader for training.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        fabric: A Fabric function to setup device for the tensors and gradients.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of training loss and training accuracy.\n        \n    \"\"\"\n    # Model on training mode\n    model.train()\n    \n    # Setting train loss and accuracy \n    train_loss = 0\n    train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(fabric.device) # New by Fabric\n    \n    # Looping the dataloaders\n    for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Training', total=len(dataloader), unit='batch'):\n        # X, y = X.to(device), y.to(device) # New by Fabric\n        \n        # 5 step to train a model\n        y_pred = model(X) # 1. Forward pass\n        loss = loss_fn(y_pred, y) # 2. Calculate loss\n        train_loss += loss.item() \n        optimizer.zero_grad() # 3. Initiate optimizer\n        #loss.backward() # 4. Backward pass\n        fabric.backward(loss) # New by Fabric\n        optimizer.step() # 5. Updating the model parameters\n        \n        # Calculating the training accuracy\n        y_pred_labels = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc.compute()\n    return train_loss, train_acc\n\ndef test_step(model, dataloader, loss_fn, fabric, num_classes):\n    \"\"\"\n    Test a pytorch model by going into eval mode and applying forward pass,\n    and loss calculation.\n    \n    Parameters:\n        model: A pytorch model for testing.\n        dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        fabric: A Fabric function to setup device for the tensors and gradients.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of testing loss and testing accuracy.\n    \"\"\"\n    # Model on evaluation mode\n    model.eval()\n    \n    # Setting train loss and accuracy \n    test_loss = 0\n    test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(fabric.device) # New by Fabric\n    \n    # Using inference mode\n    with torch.no_grad():\n        # Looping the dataloaders\n        for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Evaluation', total=len(dataloader), unit='batch'):\n            # X, y = X.to(device), y.to(device) # New by Fabric\n            \n            # Forward pass\n            y_pred = model(X)\n            \n            # Calculate loss\n            loss = loss_fn(y_pred, y)\n            test_loss += loss.item()\n            \n            # Calculate accuracy\n            y_pred_labels = y_pred.argmax(dim=1)\n            test_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc.compute()\n    return test_loss, test_acc\n\ndef model_train(epochs, model, train_dataloader, test_dataloader, optimizer, loss_fn, fabric, num_classes):\n    \"\"\"\n    Trains a pytorch model for a certain number of epochs going through the model training \n    and testing stage, and accumalating the loss, accuracy, and training and testing time.\n    \n    Parameters:\n        epochs: A integer to run the training and testing stage. \n        model: A pytorch model for training and testing.\n        train_dataloader: A pytorch dataloader for training.\n        test_dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        fabric: A Fabric function to setup device for the tensors and gradients.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of accumaleted results in dict and total training time in float datatype.\n    \"\"\"\n    # Create empty result\n    results = {'train_loss': [],\n               'train_acc': [],\n               'test_loss': [],\n               'test_acc': [],\n               'train_epoch_time(min)': [],\n               'test_epoch_time(min)': []}\n    \n    # Loop through training and testing steps\n    model_train_start_time = timer()\n    for epoch in tqdm(range(epochs), desc=f'Training and Evaluation for {epochs} Epochs', unit='epochs'):\n        # Training the model and timing it.\n        train_epoch_start_time = timer()\n        train_loss, train_acc = train_step(model=model, \n                                           dataloader=train_dataloader, \n                                           loss_fn=loss_fn, \n                                           optimizer=optimizer, \n                                           fabric=fabric, # New by Fabric\n                                           num_classes=num_classes)\n        train_epoch_stop_time = timer()\n        train_epoch_time = (train_epoch_stop_time - train_epoch_start_time)/60\n        \n        # Testing the model and timing it\n        test_epoch_start_time = timer()\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        fabric=fabric, # New by Fabric\n                                        num_classes=num_classes)\n        test_epoch_stop_time = timer()\n        test_epoch_time = (test_epoch_stop_time - test_epoch_start_time)/60\n        \n        # Print the model result\n        print(f'Epoch: [{epoch+1}/{epochs}] | train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | train_time: {train_epoch_time:.4f} min | '\n              f'test loss: {test_loss:.4f} | test_acc: {test_acc:.4f} | test_time: {test_epoch_time:.4f} min')\n        \n        # Saving the results\n        results['train_loss'].append(train_loss)\n        results['train_acc'].append(train_acc.detach().cpu().item())\n        results['test_loss'].append(test_loss)\n        results['test_acc'].append(test_acc.detach().cpu().item())\n        results['train_epoch_time(min)'].append(train_epoch_time)\n        results['test_epoch_time(min)'].append(test_epoch_time)\n        \n    # Calculating total model training time\n    model_train_end_time = timer()\n    total_train_time = (model_train_end_time - model_train_start_time)/60\n    print(f'\\nTotal Model Training Time: {total_train_time:.4f} min')\n    return results, total_train_time\n\n\nExp 5: Fabric Lightning\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric()\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp5_results, exp5_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5801 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5829 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.5967 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5939 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5900 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.6074 min\n\nTotal Model Training Time: 6.5512 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp5_results_filename = 'exp5_results.csv'\nexp5_results_df = pd.DataFrame(exp5_results)\nexp5_results_df.to_csv(os.path.join(RESULTS_DIR, exp5_results_filename), index=False)\nexp5_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.580098\n0.582881\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.596745\n0.593947\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.590004\n0.607377\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric Lightning', exp5_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n\n\n\n\n\n\n\nExp 6: Fabric Lightning - Compile mode\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric()\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp6_results, exp6_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n[2023-04-06 07:12:28,856] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.1276 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0502 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.7023 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5721 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.5845 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5688 min\n\nTotal Model Training Time: 7.6056 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp6_results_filename = 'exp6_results.csv'\nexp6_results_df = pd.DataFrame(exp6_results)\nexp6_results_df.to_csv(os.path.join(RESULTS_DIR, exp6_results_filename), index=False)\nexp6_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.127585\n1.050178\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.702294\n0.572100\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.584458\n0.568837\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric + Compile', exp6_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n\n\n\n\n\n\n\nExp 7: Fabric + Precision\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric(precision='16-mixed')\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp7_results, exp7_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nUsing 16-bit Automatic Mixed Precision (AMP)\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5438 min | test loss: 2.6781 | test_acc: 0.4519 | test_time: 0.5552 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4168 | train_time: 1.5081 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5554 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5786 min | test loss: 2.2094 | test_acc: 0.5817 | test_time: 0.5744 min\n\nTotal Model Training Time: 6.3157 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp7_results_filename = 'exp7_results.csv'\nexp7_results_df = pd.DataFrame(exp7_results)\nexp7_results_df.to_csv(os.path.join(RESULTS_DIR, exp7_results_filename), index=False)\nexp7_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853492\n0.198623\n2.678145\n0.451923\n1.543753\n0.555179\n\n\n1\n2.599140\n0.416843\n2.427695\n0.540064\n1.508068\n0.555399\n\n\n2\n2.360222\n0.523835\n2.209447\n0.581731\n1.578585\n0.574411\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric + Precision', exp7_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n\n\n\n\n\n\n\nExp 8: Fabric + Precision + Compile\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric(precision='16-mixed')\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp8_results, exp8_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nUsing 16-bit Automatic Mixed Precision (AMP)\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n[2023-04-06 07:26:29,375] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 3.0326 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.1394 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.5740 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5764 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5095 | train_time: 1.5562 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5639 min\n\nTotal Model Training Time: 8.4427 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp8_results_filename = 'exp8_results.csv'\nexp8_results_df = pd.DataFrame(exp8_results)\nexp8_results_df.to_csv(os.path.join(RESULTS_DIR, exp8_results_filename), index=False)\nexp8_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845062\n0.201271\n2.678693\n0.450321\n3.032648\n1.139419\n\n\n1\n2.590762\n0.445975\n2.427031\n0.548077\n1.574046\n0.576357\n\n\n2\n2.366282\n0.509534\n2.209138\n0.580128\n1.556243\n0.563871\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric + Precision + Compile', exp8_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n7\nFabric + Precision + Compile\n8.442741\n\n\n\n\n\n\n\n\n# plot the train time\nmodel_train_time_df.plot.barh(x='model', \n                              y='training_time(min)', \n                              xlabel='Training Time(min)', \n                              ylabel='Training Method', \n                              legend=False)\nplt.title(f'MVit V2 Model Training Time for {NUM_EPOCHS} Epochs')\nplt.savefig('model_train_time.png')"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Training and Testing on Full Dataset and Deploying the Model",
    "text": "Training and Testing on Full Dataset and Deploying the Model\n\nCreating the model\n\nNUM_CLASSES = 51\nSEQUENCE_LENGTH = 16\nBATCH_SIZE = 16\nWORKERS = os.cpu_count()\n\n\n# Creating the model and transforms using the function.\nmodel, transforms = create_model(num_classes=NUM_CLASSES, device=device)\n\n\n\nGetting the Full Dataset and Dataloaders\n\n# Creating the dataset.\ndataset = HumanActionDataset(video_dir=DATA_PATH, \n                             seq_len=SEQUENCE_LENGTH,\n                             num_classes=NUM_CLASSES,\n                             transform=transforms)\ntrain_dataset, test_dataset = random_split(dataset=dataset,\n                                           lengths=[0.75, 0.25], \n                                           generator=torch.Generator().manual_seed(42))\nprint(f'Train Dataset Length: {len(train_dataset)} and Test Dataset length: {len(test_dataset)}')\n\n# Creating the dataloaders\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\nTrain Dataset Length: 5075 and Test Dataset length: 1691\nDataloaders Length: 317 for a batch size of: 16\nDataloaders Length: 105 for a batch size of: 16\n\n\n\n\nTraining Model for Full Dataset\n\nset_seed(42)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n# Training the model using the pytorch function \nNUM_EPOCHS = 10\nresults, total_train_time = model_train(epochs=NUM_EPOCHS,\n                                        model=model, \n                                        train_dataloader=train_dataloader, \n                                        test_dataloader=test_dataloader, \n                                        optimizer=optimizer, \n                                        loss_fn=loss_fn, \n                                        device=device, \n                                        num_classes=len(dataset.classes))\n\n\n\n\n\n\n\n\n\n\nEpoch: [1/10] | train_loss: 2.6020 | train_acc: 0.4720 | train_time: 4.8366 min | test loss: 1.8906 | test_acc: 0.6238 | test_time: 1.6950 min\nEpoch: [2/10] | train_loss: 1.8662 | train_acc: 0.6327 | train_time: 5.4221 min | test loss: 1.7076 | test_acc: 0.6607 | test_time: 1.5391 min\nEpoch: [3/10] | train_loss: 1.7319 | train_acc: 0.6642 | train_time: 4.8773 min | test loss: 1.6484 | test_acc: 0.6756 | test_time: 1.5405 min\nEpoch: [4/10] | train_loss: 1.6819 | train_acc: 0.6733 | train_time: 4.5695 min | test loss: 1.6199 | test_acc: 0.6887 | test_time: 1.5152 min\nEpoch: [5/10] | train_loss: 1.6426 | train_acc: 0.6936 | train_time: 4.3718 min | test loss: 1.6051 | test_acc: 0.6970 | test_time: 1.5019 min\nEpoch: [6/10] | train_loss: 1.6085 | train_acc: 0.6981 | train_time: 4.7581 min | test loss: 1.5919 | test_acc: 0.7000 | test_time: 1.5833 min\nEpoch: [7/10] | train_loss: 1.5967 | train_acc: 0.7049 | train_time: 4.9686 min | test loss: 1.5807 | test_acc: 0.7071 | test_time: 1.5994 min\nEpoch: [8/10] | train_loss: 1.5680 | train_acc: 0.7173 | train_time: 6.3247 min | test loss: 1.5763 | test_acc: 0.7137 | test_time: 1.5196 min\nEpoch: [9/10] | train_loss: 1.5577 | train_acc: 0.7179 | train_time: 5.6291 min | test loss: 1.5734 | test_acc: 0.7083 | test_time: 1.5351 min\nEpoch: [10/10] | train_loss: 1.5540 | train_acc: 0.7194 | train_time: 5.6172 min | test loss: 1.5605 | test_acc: 0.7185 | test_time: 1.5643 min\n\nTotal Model Training Time: 66.9688 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStoring Files for Demo App.\n\n# Creating a directory for the app\ndemo_path = 'demo'\ndemo_project_path = os.path.join(demo_path, 'human_action_recognition')\ndemo_example_path = os.path.join(demo_project_path, 'examples')\n\nif os.path.exists(demo_path):\n    print(f'[INFO] \"{demo_path}\" already exists.')\nelse:\n    os.mkdir(demo_path)\n    os.mkdir(demo_project_path)\n    os.mkdir(demo_example_path)\n    print(f'[INFO] \"{demo_project_path}\" and \"{demo_example_path}\" directory is been created.')\n\n[INFO] \"demo/human_action_recognition\" and \"demo/human_action_recognition/examples\" directory is been created.\n\n\n\n# Saving some example files\nexample_files = ['data/shoot_bow/6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_3.avi',\n                 'data/golf/Golf_Tips_-_Hit_The_Driver_300+_Yards!!!_golf_f_nm_np1_fr_med_0.avi',\n                 'data/climb/(HQ)_Rock_Climbing_-_Free_Solo_Speed_Climb_-_Dan_Osman_climb_f_cm_np1_le_med_2.avi',\n                 'data/punch/AdvancedBoxingTechniquesandExercises_punch_u_nm_np1_ba_med_23.avi',\n                 'data/swing_baseball/BaseballHitinSlowMotion_swing_baseball_f_nm_np1_fr_bad_0.avi']\n\nfor i in example_files:\n    # Copying the files\n    shutil.copy2(src=i, dst='demo/human_action_recognition/examples')\n    print(f'{i} File is been copied.')\n\ndata/shoot_bow/6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_3.avi File is been copied.\ndata/golf/Golf_Tips_-_Hit_The_Driver_300+_Yards!!!_golf_f_nm_np1_fr_med_0.avi File is been copied.\ndata/climb/(HQ)_Rock_Climbing_-_Free_Solo_Speed_Climb_-_Dan_Osman_climb_f_cm_np1_le_med_2.avi File is been copied.\ndata/punch/AdvancedBoxingTechniquesandExercises_punch_u_nm_np1_ba_med_23.avi File is been copied.\ndata/swing_baseball/BaseballHitinSlowMotion_swing_baseball_f_nm_np1_fr_bad_0.avi File is been copied.\n\n\n\n# Saving the classes names to text file\nclass_names_path = os.path.join(demo_project_path, 'class_names.txt')\n\nwith open(class_names_path, 'w') as f:\n    f.write('\\n'.join(dataset.classes))\n    print(f'[INFO] Saving class names to: \"{class_names_path}\"')\n\n[INFO] Saving class names to: \"demo/human_action_recognition/class_names.txt\"\n\n\n\n# Reading the text file\nwith open(class_names_path, 'r') as f:\n    class_names_read = [i.strip() for i in f.readlines()]\n\nclass_names_read[:5]\n\n['smoke', 'dive', 'cartwheel', 'throw', 'hit']\n\n\n\n\nSaving and Loading the Model\n\nmodel_save_name = 'mvit_v2_pretrained_model_hmdb51.pth'\nmodel_save_path = os.path.join(demo_project_path, model_save_name)\n\n\n# saving the model state\ntorch.save(obj=model.state_dict(), \n           f=model_save_path)\nprint(f'[INFO] Model is been saved to: \"{model_save_path}\"')\n\n[INFO] Model is been saved to: \"demo/human_action_recognition/mvit_v2_pretrained_model_hmdb51.pth\"\n\n\n\n# loading the model state\nmodel, transforms = create_model(num_classes=NUM_CLASSES, device=device)\nmodel.load_state_dict(torch.load(model_save_path))\n\n&lt;All keys matched successfully&gt;\n\n\n\n\nCreating a text file for Modules and Packages\n\n%%writefile demo/human_action_recognition/requirements.txt\nav==10.0.0\ntorch==2.0.0\ntorchvision==0.15.1\ntorchaudio==2.0.1\ngradio==3.24.1\n\nWriting demo/human_action_recognition/requirements.txt\n\n\n\n\nCreating a Python Script to Preprocess the Data\n\n%%writefile demo/human_action_recognition/utils.py\nimport torch\nimport torchvision\n\ndef preprocess_video(video: str):\n    \"\"\"\n    A function to preprocess the video file before going into the model.\n    Parameters: \n        video: str, A string for the video file path.\n    Returns: selected_frame: torch.Tensor, A tensor of shape 'TCHW'.\n    \"\"\"\n    # Reading the video file\n    vframes, _, _ = torchvision.io.read_video(filename=video, pts_unit='sec', output_format='TCHW')\n    vframes = vframes.type(torch.float32)\n    vframes_count = len(vframes)\n    \n    # Selecting frames at certain interval\n    skip_frames = max(int(vframes_count/16), 1)\n    \n    # Selecting the first frame\n    selected_frame = vframes[0].unsqueeze(0)\n    \n    # Creating a new sequence of frames upto the defined sequence length.\n    for i in range(1, 16):\n        selected_frame = torch.concat((selected_frame, vframes[i * skip_frames].unsqueeze(0)))\n    return selected_frame\n\nWriting demo/human_action_recognition/utils.py\n\n\n\n\nCreating a Python Script to Create the Model\n\n%%writefile demo/human_action_recognition/model.py\nimport torch\nimport torchvision\n\ndef create_model(num_classes: int, seed: int = 42):\n    \"\"\"\n    A function to create a model.\n    Parameters:\n        num_classes: int, A integer for toal number of classes.\n        seed: int(default: 42), A random seed value.\n    Returns: \n        model: A feature extracted model for video classification.\n        transforms: A torchvision transform is returned which was used in the pretrained model.    \n    \"\"\"\n    # Creating model, weights and transforms\n    weights = torchvision.models.video.MViT_V2_S_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.video.mvit_v2_s(weights=weights)\n    \n    # Freezing the model layers\n    for params in model.parameters():\n        params.requires_grad = False\n        \n    # Changing the fully Conncected head layer\n    torch.manual_seed(seed)\n    dropout_layer = model.head[0]\n    in_features = model.head[1].in_features\n    model.head = torch.nn.Sequential(\n        dropout_layer,\n        torch.nn.Linear(in_features=in_features, out_features=num_classes, bias=True)\n    )\n    return model, transforms\n\nWriting demo/human_action_recognition/model.py\n\n\n\n\nCreating a Python Script for Gradio App\n\n%%writefile demo/human_action_recognition/app.py\nimport gradio as gr\nimport os\nimport torch\nimport torchvision\nfrom timeit import default_timer as timer\nfrom model import create_model\nfrom utils import preprocess_video\n\n# Configuring the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Setting class names using the text file\nwith open('class_names.txt', 'r') as f:\n    class_names = [i.strip() for i in f.readlines()]\n\n# Setting the model and transforms\nmodel, transforms = create_model(num_classes=len(class_names), seed=42)\nmodel.load_state_dict(torch.load(f='mvit_v2_pretrained_model_hmdb51.pth', map_location=device))\n\n# Creating a function to predict the video\ndef predict(video_file):\n    \"\"\"\n    A function to predict the video using the model.\n    Parameters: \n        video_file: str, A video file path as a string.\n    Returns: \n        pred_labels_probs: A dict file containing the class names and confidence values.\n        pred_time: Time taken to predict in seconds.\n    \"\"\"\n    # Preprocessing the video file\n    frames = preprocess_video(video=video_file)\n    \n    # transforming the frames\n    mod_frames = transforms(frames).unsqueeze(dim=0)\n    \n    # Starting the timer and predicting using the model\n    start_time = timer()\n    model.eval()\n    \n    # forward pass\n    with torch.no_grad():\n        logits = model(mod_frames.to(device)) \n        pred_probs = torch.softmax(logits, dim=1).squeeze(dim=0)\n    \n    # Creating a dict with class names and their predicted probabilities\n    pred_labels_probs = {class_names[i]: float(pred_probs[i]) for i in range(len(class_names))}\n        \n    # Ending the timer and calculating the prediction time.\n    end_time = timer()\n    pred_time = round(end_time - start_time, 5)\n    \n    # Returning the labels and time for gradio output.\n    return pred_labels_probs, pred_time\n\n# Pre-information for interface\ntitle = 'Human Activity Recognition(HAR)'\ndescription = 'A Gradio demo web application for video classification using the MViT V2 Pretrained Model and trained on the HMDB51 Dataset.'\narticle = 'Created by John'\nexample_list = [['examples/'+ i] for i in os.listdir('examples')]\n\n# Building the Gradio interface\ndemo = gr.Interface(fn=predict,\n                    inputs=gr.Video(label='Video'),\n                    outputs=[gr.Label(num_top_classes=5, label='Predictions'),\n                             gr.Number(label='Prediction Time (sec)')],\n                    examples=example_list,\n                    title=title,\n                    description=description,\n                    article=article)\n\n# Launching the gradio interface\ndemo.launch()\n\nWriting demo/human_action_recognition/app.py\n\n\n\n\nDeploying on HuggingFace\n\nIFrame(src='https://hf.space/embed/JohnPinto/Human_Activity_Recognition-HAR-Video_Classification-HMDB51-Dataset',\n       width=1050,\n       height=700)"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#importing-libraries",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#importing-libraries",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\nTorch version: 2.0.0+cu117\nTorchvision version: 0.15.1+cu117\nLightning version: 2.0.1\nGPU score: (8, 6)\n\n\n\n\ndevice(type='cuda')"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#downloading-the-dataset",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#downloading-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Downloading the Dataset",
    "text": "Downloading the Dataset\nDownloading the data from the dataset origin.\n\n\n[INFO] Downloading data.\n[INFO] Data is been downloaded!\n\n\nExtracting the data in the raw_data directory\n\n\n[INFO] Extracting data.\n[INFO] Data extraction done!\n\n\nExtracting all the raw data in the data directory and deleting the raw data from the system..\n\n\n[INFO] Extracting all the classes data.\n[INFO] Data extraction done for: \"raw_data/clap.rar\"!\n[INFO] Data extraction done for: \"raw_data/talk.rar\"!\n[INFO] Data extraction done for: \"raw_data/smoke.rar\"!\n[INFO] Data extraction done for: \"raw_data/draw_sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/flic_flac.rar\"!\n[INFO] Data extraction done for: \"raw_data/dive.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_bow.rar\"!\n[INFO] Data extraction done for: \"raw_data/drink.rar\"!\n[INFO] Data extraction done for: \"raw_data/hug.rar\"!\n[INFO] Data extraction done for: \"raw_data/pour.rar\"!\n[INFO] Data extraction done for: \"raw_data/pushup.rar\"!\n[INFO] Data extraction done for: \"raw_data/hit.rar\"!\n[INFO] Data extraction done for: \"raw_data/wave.rar\"!\n[INFO] Data extraction done for: \"raw_data/run.rar\"!\n[INFO] Data extraction done for: \"raw_data/walk.rar\"!\n[INFO] Data extraction done for: \"raw_data/fall_floor.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_horse.rar\"!\n[INFO] Data extraction done for: \"raw_data/turn.rar\"!\n[INFO] Data extraction done for: \"raw_data/situp.rar\"!\n[INFO] Data extraction done for: \"raw_data/jump.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/dribble.rar\"!\n[INFO] Data extraction done for: \"raw_data/sit.rar\"!\n[INFO] Data extraction done for: \"raw_data/chew.rar\"!\n[INFO] Data extraction done for: \"raw_data/pick.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick.rar\"!\n[INFO] Data extraction done for: \"raw_data/eat.rar\"!\n[INFO] Data extraction done for: \"raw_data/fencing.rar\"!\n[INFO] Data extraction done for: \"raw_data/swing_baseball.rar\"!\n[INFO] Data extraction done for: \"raw_data/stand.rar\"!\n[INFO] Data extraction done for: \"raw_data/handstand.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb.rar\"!\n[INFO] Data extraction done for: \"raw_data/kiss.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/push.rar\"!\n[INFO] Data extraction done for: \"raw_data/throw.rar\"!\n[INFO] Data extraction done for: \"raw_data/cartwheel.rar\"!\n[INFO] Data extraction done for: \"raw_data/pullup.rar\"!\n[INFO] Data extraction done for: \"raw_data/brush_hair.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb_stairs.rar\"!\n[INFO] Data extraction done for: \"raw_data/laugh.rar\"!\n[INFO] Data extraction done for: \"raw_data/punch.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_bike.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword_exercise.rar\"!\n[INFO] Data extraction done for: \"raw_data/somersault.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_gun.rar\"!\n[INFO] Data extraction done for: \"raw_data/catch.rar\"!\n[INFO] Data extraction done for: \"raw_data/smile.rar\"!\n[INFO] Data extraction done for: \"raw_data/golf.rar\"!\n[INFO] Data extraction done for: \"raw_data/shake_hands.rar\"!\n\n[INFO] Deleted raw data.\n\n\nChecking some info on the data that is been extracted.\n\n\nThere are 51 directories and 0 files in \"data\".\nThere are 0 directories and 143 files in \"data/swing_baseball\".\nThere are 0 directories and 232 files in \"data/run\".\nThere are 0 directories and 105 files in \"data/situp\".\nThere are 0 directories and 162 files in \"data/shake_hands\".\nThere are 0 directories and 107 files in \"data/flic_flac\".\nThere are 0 directories and 127 files in \"data/hit\".\nThere are 0 directories and 102 files in \"data/kiss\".\nThere are 0 directories and 120 files in \"data/talk\".\nThere are 0 directories and 164 files in \"data/drink\".\nThere are 0 directories and 104 files in \"data/wave\".\nThere are 0 directories and 140 files in \"data/somersault\".\nThere are 0 directories and 116 files in \"data/push\".\nThere are 0 directories and 116 files in \"data/ride_horse\".\nThere are 0 directories and 128 files in \"data/kick_ball\".\nThere are 0 directories and 105 files in \"data/golf\".\nThere are 0 directories and 112 files in \"data/shoot_bow\".\nThere are 0 directories and 116 files in \"data/fencing\".\nThere are 0 directories and 102 files in \"data/smile\".\nThere are 0 directories and 145 files in \"data/dribble\".\nThere are 0 directories and 154 files in \"data/stand\".\nThere are 0 directories and 130 files in \"data/clap\".\nThere are 0 directories and 548 files in \"data/walk\".\nThere are 0 directories and 102 files in \"data/throw\".\nThere are 0 directories and 151 files in \"data/jump\".\nThere are 0 directories and 103 files in \"data/shoot_gun\".\nThere are 0 directories and 131 files in \"data/shoot_ball\".\nThere are 0 directories and 102 files in \"data/catch\".\nThere are 0 directories and 103 files in \"data/draw_sword\".\nThere are 0 directories and 142 files in \"data/sit\".\nThere are 0 directories and 127 files in \"data/sword_exercise\".\nThere are 0 directories and 108 files in \"data/climb\".\nThere are 0 directories and 104 files in \"data/pullup\".\nThere are 0 directories and 126 files in \"data/punch\".\nThere are 0 directories and 103 files in \"data/pushup\".\nThere are 0 directories and 127 files in \"data/sword\".\nThere are 0 directories and 108 files in \"data/eat\".\nThere are 0 directories and 109 files in \"data/chew\".\nThere are 0 directories and 128 files in \"data/laugh\".\nThere are 0 directories and 103 files in \"data/ride_bike\".\nThere are 0 directories and 118 files in \"data/hug\".\nThere are 0 directories and 107 files in \"data/cartwheel\".\nThere are 0 directories and 106 files in \"data/pick\".\nThere are 0 directories and 130 files in \"data/kick\".\nThere are 0 directories and 113 files in \"data/handstand\".\nThere are 0 directories and 240 files in \"data/turn\".\nThere are 0 directories and 112 files in \"data/climb_stairs\".\nThere are 0 directories and 127 files in \"data/dive\".\nThere are 0 directories and 107 files in \"data/brush_hair\".\nThere are 0 directories and 106 files in \"data/pour\".\nThere are 0 directories and 109 files in \"data/smoke\".\nThere are 0 directories and 136 files in \"data/fall_floor\"."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#visualizing-the-dataset",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#visualizing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Visualizing the dataset",
    "text": "Visualizing the dataset\nNow that the data is downloaded, lets view the data with the labels."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#preprocessing-the-dataset",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#preprocessing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Preprocessing the Dataset",
    "text": "Preprocessing the Dataset\nCreating pytorch Dataset.\nIn this steps we will be creating a custom dataset class using the functionality provided by PyTorch. 1. Read the video file. 2. Extracting the frames after every distance for a max sequence length. 3. Assigning label index for all the classes. 4. Performing torchvision.transform on the video data. 5. Returning tuple of video and label data as a tensor.\nBefore creating the dataset, first lets clean it. Here we will check whether the videos are readable and the frame count needs to be more than the Sequence Length frames.\n\n\n[]\n\n\n\n\nThere are no files to delete.\n\n\n\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n         [ 2.,  2.,  0.,  ..., 11., 11., 11.],\n         ...,\n         [ 2.,  2.,  0.,  ..., 55., 55., 55.],\n         [ 5.,  5.,  3.,  ..., 62., 62., 62.],\n         [ 5.,  5.,  2.,  ..., 34., 34., 34.]],\n\n        [[ 4.,  4.,  5.,  ...,  2.,  1.,  1.],\n         [ 5.,  5.,  5.,  ...,  3.,  3.,  3.],\n         [ 4.,  4.,  5.,  ..., 17., 17., 17.],\n         ...,\n         [ 2.,  2.,  2.,  ..., 41., 41., 41.],\n         [ 0.,  0.,  1.,  ..., 49., 49., 49.],\n         [ 0.,  0.,  0.,  ..., 21., 21., 21.]],\n\n        [[ 3.,  3.,  4.,  ...,  2.,  1.,  1.],\n         [ 4.,  4.,  4.,  ...,  3.,  3.,  3.],\n         [ 1.,  1.,  1.,  ...,  8.,  8.,  8.],\n         ...,\n         [ 0.,  0.,  0.,  ..., 16., 14., 14.],\n         [ 0.,  0.,  0.,  ..., 28., 28., 28.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([16, 3, 240, 320]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#creating-a-pretrained-model",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#creating-a-pretrained-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating A Pretrained Model",
    "text": "Creating A Pretrained Model\n\n\n==================================================================================================================================\nLayer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n==================================================================================================================================\nMViT (MViT)                                        [1, 3, 16, 224, 224] [1, 20]              --                   Partial\n├─Conv3d (conv_proj)                               [1, 3, 16, 224, 224] [1, 96, 8, 56, 56]   (42,432)             False\n├─PositionalEncoding (pos_encoding)                [1, 25088, 96]       [1, 25089, 96]       (96)                 False\n├─ModuleList (blocks)                              --                   --                   --                   False\n│    └─MultiscaleBlock (0)                         [1, 25089, 96]       [1, 25089, 96]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 25089, 96]       (68,352)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MLP (mlp)                              [1, 25089, 96]       [1, 25089, 96]       (74,208)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    └─MultiscaleBlock (1)                         [1, 25089, 96]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 6273, 192]       (113,280)            False\n│    │    └─Linear (project)                       [1, 25089, 96]       [1, 25089, 192]      (18,624)             False\n│    │    └─Pool (pool_skip)                       [1, 25089, 192]      [1, 6273, 192]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (2)                         [1, 6273, 192]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 6273, 192]       (168,576)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (3)                         [1, 6273, 192]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 1569, 384]       (385,152)            False\n│    │    └─Linear (project)                       [1, 6273, 192]       [1, 6273, 384]       (74,112)             False\n│    │    └─Pool (pool_skip)                       [1, 6273, 384]       [1, 1569, 384]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (4)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (5)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (6)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (7)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (8)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (9)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (10)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (11)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (12)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (13)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (14)                        [1, 1569, 384]       [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 393, 768]        (1,492,608)          False\n│    │    └─Linear (project)                       [1, 1569, 384]       [1, 1569, 768]       (295,680)            False\n│    │    └─Pool (pool_skip)                       [1, 1569, 768]       [1, 393, 768]        --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    └─MultiscaleBlock (15)                        [1, 393, 768]        [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MultiscaleAttention (attn)             [1, 393, 768]        [1, 393, 768]        (2,374,656)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n├─LayerNorm (norm)                                 [1, 393, 768]        [1, 393, 768]        (1,536)              False\n├─Sequential (head)                                [1, 768]             [1, 20]              --                   True\n│    └─Dropout (0)                                 [1, 768]             [1, 768]             --                   --\n│    └─Linear (1)                                  [1, 768]             [1, 20]              15,380               True\n==================================================================================================================================\nTotal params: 34,245,524\nTrainable params: 15,380\nNon-trainable params: 34,230,144\nTotal mult-adds (G): 1.64\n==================================================================================================================================\nInput size (MB): 9.63\nForward/backward pass size (MB): 1658.93\nParams size (MB): 136.46\nEstimated Total Size (MB): 1805.02\n==================================================================================================================================\n\n\n\n\nPretrained Model Transforms:\nVideoClassification(\n    crop_size=[224, 224]\n    resize_size=[256]\n    mean=[0.45, 0.45, 0.45]\n    std=[0.225, 0.225, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 2.4604e+02,  2.3502e+02,  2.2911e+02,  ...,  2.8675e+02,\n           2.8675e+02,  2.8684e+02],\n         [ 2.3070e+02,  2.2561e+02,  2.1813e+02,  ...,  3.1508e+02,\n           3.1508e+02,  3.1508e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1425e+02,\n           3.1425e+02,  3.1425e+02],\n         ...,\n         [ 3.2401e+00,  2.9817e+00, -2.0000e+00,  ...,  1.6333e+02,\n           1.2992e+02,  9.1900e+01],\n         [-1.5833e+00, -1.7232e+00, -2.0000e+00,  ...,  4.4867e+02,\n           3.7443e+02,  2.8330e+02],\n         [ 2.4444e+00,  9.5210e-01, -2.0000e+00,  ...,  6.6754e+02,\n           6.3805e+02,  5.6134e+02]],\n\n        [[ 2.2467e+02,  2.2616e+02,  2.2911e+02,  ...,  3.0883e+02,\n           3.0883e+02,  3.0902e+02],\n         [ 2.2869e+02,  2.2478e+02,  2.1813e+02,  ...,  3.1314e+02,\n           3.1314e+02,  3.1581e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1356e+02,\n           3.1356e+02,  3.1402e+02],\n         ...,\n         [ 9.1056e+00,  6.2105e+00, -2.0000e+00,  ...,  9.9700e+01,\n           7.6676e+01,  2.8844e+01],\n         [ 1.9746e+01,  1.7561e+01, -1.1667e+00,  ...,  2.1729e+02,\n           1.6955e+02,  1.1592e+02],\n         [ 6.6111e+00,  6.6111e+00,  6.6111e+00,  ...,  5.4941e+02,\n           5.2508e+02,  4.3940e+02]],\n\n        [[-2.0000e+00,  5.3684e+00,  2.1125e+01,  ...,  3.2244e+02,\n           3.1922e+02,  3.2077e+02],\n         [-3.9952e-01,  4.0798e+00,  9.3575e+00,  ...,  3.2330e+02,\n           3.1922e+02,  3.1025e+02],\n         [ 2.0259e+00,  2.6776e+00,  5.3857e+00,  ...,  3.3464e+02,\n           3.3047e+02,  3.1790e+02],\n         ...,\n         [ 1.7578e+02,  1.7578e+02,  1.7719e+02,  ...,  2.3105e+02,\n           2.2658e+02,  2.1946e+02],\n         [ 1.7803e+02,  1.7884e+02,  1.8227e+02,  ...,  2.3562e+02,\n           2.2931e+02,  2.2306e+02],\n         [ 1.9990e+02,  2.0851e+02,  2.2140e+02,  ...,  2.2817e+02,\n           2.1964e+02,  2.2187e+02]],\n\n        ...,\n\n        [[ 4.2491e+02,  4.3142e+02,  4.2619e+02,  ...,  1.5639e+01,\n           1.5639e+01,  1.5639e+01],\n         [ 4.0218e+02,  4.0793e+02,  4.1056e+02,  ...,  6.0095e+00,\n           7.7222e+00,  2.3716e+00],\n         [ 4.0126e+02,  4.0792e+02,  4.0992e+02,  ...,  8.7490e+00,\n           1.0639e+01,  2.2439e+00],\n         ...,\n         [ 1.2787e+02,  1.2839e+02,  1.3253e+02,  ...,  9.5269e-01,\n           1.7500e+00,  1.7500e+00],\n         [ 1.3349e+02,  1.2883e+02,  1.3904e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 1.3694e+02,  1.3320e+02,  1.3960e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.5630e+02,  4.6579e+02,  4.6640e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3329e+02,  4.3661e+02,  4.3776e+02,  ...,  1.1714e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4439e+02,  4.4481e+02,  4.4500e+02,  ...,  1.4995e+00,\n          -7.8133e-01, -2.0000e+00],\n         ...,\n         [ 1.4467e+02,  1.5315e+02,  1.7425e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.3056e+00],\n         [ 1.4625e+02,  1.5174e+02,  1.6260e+02,  ..., -2.0000e+00,\n           9.2336e-01,  2.0278e+00],\n         [ 1.6148e+02,  1.6227e+02,  1.5224e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.2527e+02,  4.4359e+02,  4.6669e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3037e+02,  4.3994e+02,  4.5392e+02,  ...,  2.0278e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4064e+02,  4.4451e+02,  4.5152e+02,  ...,  2.4444e+00,\n          -7.8133e-01,  4.9081e-01],\n         ...,\n         [ 1.4709e+02,  1.5459e+02,  1.5781e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.7668e+00],\n         [ 1.4016e+02,  1.5057e+02,  1.5565e+02,  ..., -1.5833e+00,\n           1.3400e+00,  4.5890e-02],\n         [ 1.3139e+02,  1.3590e+02,  1.4467e+02,  ...,  2.4444e+00,\n           2.4444e+00,  5.3965e+00]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([3, 16, 224, 224]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#creating-dataloaders",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#creating-dataloaders",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating DataLoaders",
    "text": "Creating DataLoaders\n\n\nTrain Dataset Length: 1898 and Test Dataset length: 632\n\n\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\n\n\nLabel Distribution and Zero Rule Benchmark\n\n\nTraining Dataset Distribution:\n [(0, 78), (1, 92), (2, 83), (3, 79), (4, 94), (5, 72), (6, 77), (7, 79), (8, 85), (9, 103), (10, 90), (11, 106), (12, 88), (13, 78), (14, 180), (15, 70), (16, 87), (17, 100), (18, 171), (19, 76)]\n\n\n\n\nTesting Dataset Distribution:\n [(0, 30), (1, 32), (2, 24), (3, 23), (4, 29), (5, 32), (6, 30), (7, 24), (8, 27), (9, 27), (10, 17), (11, 35), (12, 37), (13, 24), (14, 58), (15, 33), (16, 29), (17, 26), (18, 60), (19, 27)]\n\n\n\n\nThe Major class in Testing DataLoader is: Class Name: hug, Class Index: 18 and Total Number of Data: 60\nZero Rule Basline Benchmark Accuracy for predicting the major class: 9.62%"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#pytorch---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#pytorch---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch - Model Training and Testing",
    "text": "PyTorch - Model Training and Testing\n\nExp 1: PyTorch Before 2.0 (PyTorch Eager Mode)\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.4238 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5183 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.4201 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5181 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.3959 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.5176 min\n\nTotal Model Training Time: 5.7940 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[INFO] results directory is been created.\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.423781\n0.518263\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.420147\n0.518100\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.395912\n0.517590\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n\n\n\n\n\n\n\nExp 2: PyTorch 2.0 (PyTorch Compile Mode)\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.3885 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0454 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.6187 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5678 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.6008 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5694 min\n\nTotal Model Training Time: 7.7908 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.388515\n1.045425\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.618742\n0.567790\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.600783\n0.569392\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#pytorch-lightning---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#pytorch-lightning---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch Lightning - Model Training and Testing",
    "text": "PyTorch Lightning - Model Training and Testing\n\nExp 3: PyTorch Lightning\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 7.6887 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n\n\n\n\n\n\n\nExp 4: PyTorch Lightning - Compile Mode\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 10.5425 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#lightning-fabric---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#lightning-fabric---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Lightning Fabric - Model training and Testing",
    "text": "Lightning Fabric - Model training and Testing\n\nExp 5: Fabric Lightning\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5801 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5829 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.5967 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5939 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5900 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.6074 min\n\nTotal Model Training Time: 6.5512 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.580098\n0.582881\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.596745\n0.593947\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.590004\n0.607377\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n\n\n\n\n\n\n\nExp 6: Fabric Lightning - Compile mode\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.1276 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0502 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.7023 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5721 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.5845 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5688 min\n\nTotal Model Training Time: 7.6056 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.127585\n1.050178\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.702294\n0.572100\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.584458\n0.568837\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n\n\n\n\n\n\n\nExp 7: Fabric + Precision\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5438 min | test loss: 2.6781 | test_acc: 0.4519 | test_time: 0.5552 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4168 | train_time: 1.5081 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5554 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5786 min | test loss: 2.2094 | test_acc: 0.5817 | test_time: 0.5744 min\n\nTotal Model Training Time: 6.3157 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853492\n0.198623\n2.678145\n0.451923\n1.543753\n0.555179\n\n\n1\n2.599140\n0.416843\n2.427695\n0.540064\n1.508068\n0.555399\n\n\n2\n2.360222\n0.523835\n2.209447\n0.581731\n1.578585\n0.574411\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n\n\n\n\n\n\n\nExp 8: Fabric + Precision + Compile\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 3.0326 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.1394 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.5740 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5764 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5095 | train_time: 1.5562 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5639 min\n\nTotal Model Training Time: 8.4427 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845062\n0.201271\n2.678693\n0.450321\n3.032648\n1.139419\n\n\n1\n2.590762\n0.445975\n2.427031\n0.548077\n1.574046\n0.576357\n\n\n2\n2.366282\n0.509534\n2.209138\n0.580128\n1.556243\n0.563871\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n7\nFabric + Precision + Compile\n8.442741"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "href": "posts/2023-04-01_pytorch_and_lightning_2.0/index.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Training and Testing on Full Dataset and Deploying the Model",
    "text": "Training and Testing on Full Dataset and Deploying the Model\n\nCreating the model\n\n\nGetting the Full Dataset and Dataloaders\n\n\nTrain Dataset Length: 5075 and Test Dataset length: 1691\nDataloaders Length: 317 for a batch size of: 16\nDataloaders Length: 105 for a batch size of: 16\n\n\n\n\nTraining Model for Full Dataset\n\n\n\n\n\n\n\n\n\n\n\nEpoch: [1/10] | train_loss: 2.6020 | train_acc: 0.4720 | train_time: 4.8366 min | test loss: 1.8906 | test_acc: 0.6238 | test_time: 1.6950 min\nEpoch: [2/10] | train_loss: 1.8662 | train_acc: 0.6327 | train_time: 5.4221 min | test loss: 1.7076 | test_acc: 0.6607 | test_time: 1.5391 min\nEpoch: [3/10] | train_loss: 1.7319 | train_acc: 0.6642 | train_time: 4.8773 min | test loss: 1.6484 | test_acc: 0.6756 | test_time: 1.5405 min\nEpoch: [4/10] | train_loss: 1.6819 | train_acc: 0.6733 | train_time: 4.5695 min | test loss: 1.6199 | test_acc: 0.6887 | test_time: 1.5152 min\nEpoch: [5/10] | train_loss: 1.6426 | train_acc: 0.6936 | train_time: 4.3718 min | test loss: 1.6051 | test_acc: 0.6970 | test_time: 1.5019 min\nEpoch: [6/10] | train_loss: 1.6085 | train_acc: 0.6981 | train_time: 4.7581 min | test loss: 1.5919 | test_acc: 0.7000 | test_time: 1.5833 min\nEpoch: [7/10] | train_loss: 1.5967 | train_acc: 0.7049 | train_time: 4.9686 min | test loss: 1.5807 | test_acc: 0.7071 | test_time: 1.5994 min\nEpoch: [8/10] | train_loss: 1.5680 | train_acc: 0.7173 | train_time: 6.3247 min | test loss: 1.5763 | test_acc: 0.7137 | test_time: 1.5196 min\nEpoch: [9/10] | train_loss: 1.5577 | train_acc: 0.7179 | train_time: 5.6291 min | test loss: 1.5734 | test_acc: 0.7083 | test_time: 1.5351 min\nEpoch: [10/10] | train_loss: 1.5540 | train_acc: 0.7194 | train_time: 5.6172 min | test loss: 1.5605 | test_acc: 0.7185 | test_time: 1.5643 min\n\nTotal Model Training Time: 66.9688 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStoring Files for Demo App.\n\n\n[INFO] \"demo/human_action_recognition\" and \"demo/human_action_recognition/examples\" directory is been created.\n\n\n\n\ndata/shoot_bow/6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_3.avi File is been copied.\ndata/golf/Golf_Tips_-_Hit_The_Driver_300+_Yards!!!_golf_f_nm_np1_fr_med_0.avi File is been copied.\ndata/climb/(HQ)_Rock_Climbing_-_Free_Solo_Speed_Climb_-_Dan_Osman_climb_f_cm_np1_le_med_2.avi File is been copied.\ndata/punch/AdvancedBoxingTechniquesandExercises_punch_u_nm_np1_ba_med_23.avi File is been copied.\ndata/swing_baseball/BaseballHitinSlowMotion_swing_baseball_f_nm_np1_fr_bad_0.avi File is been copied.\n\n\n\n\n[INFO] Saving class names to: \"demo/human_action_recognition/class_names.txt\"\n\n\n\n\n['smoke', 'dive', 'cartwheel', 'throw', 'hit']\n\n\n\n\nSaving and Loading the Model\n\n\n[INFO] Model is been saved to: \"demo/human_action_recognition/mvit_v2_pretrained_model_hmdb51.pth\"\n\n\n\n\n&lt;All keys matched successfully&gt;\n\n\n\n\nCreating a text file for Modules and Packages\n\n\nWriting demo/human_action_recognition/requirements.txt\n\n\n\n\nCreating a Python Script to Preprocess the Data\n\n\nWriting demo/human_action_recognition/utils.py\n\n\n\n\nCreating a Python Script to Create the Model\n\n\nWriting demo/human_action_recognition/model.py\n\n\n\n\nCreating a Python Script for Gradio App\n\n\nWriting demo/human_action_recognition/app.py\n\n\n\n\nDeploying on HuggingFace"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "",
    "text": "!pip install -U -q -r requirements.txt\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport os\nimport shutil\nimport rarfile\nimport random\nimport requests\nimport numpy as np\nimport pandas as pd\nimport gradio as gr\nfrom tqdm.auto import tqdm\nfrom timeit import default_timer as timer\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\nimport cv2\nfrom IPython.display import Video, IFrame\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torchmetrics\nfrom torchinfo import summary\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, utils\nfrom torchvision.io import read_video\nfrom torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights\nimport lightning as L\nfrom lightning.fabric import Fabric\n\n\nprint(f'Torch version: {torch.__version__}')\nprint(f'Torchvision version: {torchvision.__version__}')\nprint(f'Lightning version: {L.__version__}')\nprint(f'GPU score: {torch.cuda.get_device_capability()}')\n\nTorch version: 2.0.0+cu117\nTorchvision version: 0.15.1+cu117\nLightning version: 2.0.1\nGPU score: (8, 6)\n\n\n\n# Setting seed for randomness\ndef set_seed(seed: int = 42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\nset_seed(42)\n\n# setting the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cuda')"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#importing-libraries",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#importing-libraries",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "",
    "text": "!pip install -U -q -r requirements.txt\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nimport os\nimport shutil\nimport rarfile\nimport random\nimport requests\nimport numpy as np\nimport pandas as pd\nimport gradio as gr\nfrom tqdm.auto import tqdm\nfrom timeit import default_timer as timer\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\nimport cv2\nfrom IPython.display import Video, IFrame\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torchmetrics\nfrom torchinfo import summary\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, utils\nfrom torchvision.io import read_video\nfrom torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights\nimport lightning as L\nfrom lightning.fabric import Fabric\n\n\nprint(f'Torch version: {torch.__version__}')\nprint(f'Torchvision version: {torchvision.__version__}')\nprint(f'Lightning version: {L.__version__}')\nprint(f'GPU score: {torch.cuda.get_device_capability()}')\n\nTorch version: 2.0.0+cu117\nTorchvision version: 0.15.1+cu117\nLightning version: 2.0.1\nGPU score: (8, 6)\n\n\n\n# Setting seed for randomness\ndef set_seed(seed: int = 42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\nset_seed(42)\n\n# setting the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cuda')"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#downloading-the-dataset",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#downloading-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Downloading the Dataset",
    "text": "Downloading the Dataset\nDownloading the data from the dataset origin.\n\nDATA_URL = 'http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar'\nRAW_DATA_PATH = 'raw_data'\nDATA_PATH = 'data'\nHMDB_DATA_PATH = os.path.join(DATA_PATH, 'HMDB51')\n\n\nif os.path.isfile('hmdb51_org.rar'):\n    print('[INFO] Data already exists!')\nelse:\n    print('[INFO] Downloading data.')\n    r = requests.get(DATA_URL)\n    with open('hmdb51_org.rar', 'wb') as file:\n        file.write(r.content)\n        file.close()\n    print('[INFO] Data is been downloaded!')\n\n[INFO] Downloading data.\n[INFO] Data is been downloaded!\n\n\nExtracting the data in the raw_data directory\n\nif os.path.exists(RAW_DATA_PATH):\n    print('[INFO] Data path already exists!')\nelse:\n    print('[INFO] Extracting data.')\n    os.mkdir(RAW_DATA_PATH)\n    r = rarfile.RarFile('hmdb51_org.rar')\n    r.extractall(RAW_DATA_PATH)\n    r.close()\n    print('[INFO] Data extraction done!')\n\n[INFO] Extracting data.\n[INFO] Data extraction done!\n\n\nExtracting all the raw data in the data directory and deleting the raw data from the system..\n\nif os.path.exists(DATA_PATH):\n    print('[INFO] Data path already exists!')\nelse:\n    print('[INFO] Extracting all the classes data.')\n    os.mkdir(DATA_PATH)\n    for data in os.listdir(RAW_DATA_PATH):\n        r = rarfile.RarFile(os.path.join(RAW_DATA_PATH, data))\n        r.extractall(DATA_PATH)\n        r.close()\n        print(f'[INFO] Data extraction done for: \"{os.path.join(RAW_DATA_PATH, data)}\"!')\n    os.remove('hmdb51_org.rar')\n    shutil.rmtree(RAW_DATA_PATH)\n    print(f'\\n[INFO] Deleted raw data.')\n\n[INFO] Extracting all the classes data.\n[INFO] Data extraction done for: \"raw_data/clap.rar\"!\n[INFO] Data extraction done for: \"raw_data/talk.rar\"!\n[INFO] Data extraction done for: \"raw_data/smoke.rar\"!\n[INFO] Data extraction done for: \"raw_data/draw_sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/flic_flac.rar\"!\n[INFO] Data extraction done for: \"raw_data/dive.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_bow.rar\"!\n[INFO] Data extraction done for: \"raw_data/drink.rar\"!\n[INFO] Data extraction done for: \"raw_data/hug.rar\"!\n[INFO] Data extraction done for: \"raw_data/pour.rar\"!\n[INFO] Data extraction done for: \"raw_data/pushup.rar\"!\n[INFO] Data extraction done for: \"raw_data/hit.rar\"!\n[INFO] Data extraction done for: \"raw_data/wave.rar\"!\n[INFO] Data extraction done for: \"raw_data/run.rar\"!\n[INFO] Data extraction done for: \"raw_data/walk.rar\"!\n[INFO] Data extraction done for: \"raw_data/fall_floor.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_horse.rar\"!\n[INFO] Data extraction done for: \"raw_data/turn.rar\"!\n[INFO] Data extraction done for: \"raw_data/situp.rar\"!\n[INFO] Data extraction done for: \"raw_data/jump.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/dribble.rar\"!\n[INFO] Data extraction done for: \"raw_data/sit.rar\"!\n[INFO] Data extraction done for: \"raw_data/chew.rar\"!\n[INFO] Data extraction done for: \"raw_data/pick.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick.rar\"!\n[INFO] Data extraction done for: \"raw_data/eat.rar\"!\n[INFO] Data extraction done for: \"raw_data/fencing.rar\"!\n[INFO] Data extraction done for: \"raw_data/swing_baseball.rar\"!\n[INFO] Data extraction done for: \"raw_data/stand.rar\"!\n[INFO] Data extraction done for: \"raw_data/handstand.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb.rar\"!\n[INFO] Data extraction done for: \"raw_data/kiss.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/push.rar\"!\n[INFO] Data extraction done for: \"raw_data/throw.rar\"!\n[INFO] Data extraction done for: \"raw_data/cartwheel.rar\"!\n[INFO] Data extraction done for: \"raw_data/pullup.rar\"!\n[INFO] Data extraction done for: \"raw_data/brush_hair.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb_stairs.rar\"!\n[INFO] Data extraction done for: \"raw_data/laugh.rar\"!\n[INFO] Data extraction done for: \"raw_data/punch.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_bike.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword_exercise.rar\"!\n[INFO] Data extraction done for: \"raw_data/somersault.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_gun.rar\"!\n[INFO] Data extraction done for: \"raw_data/catch.rar\"!\n[INFO] Data extraction done for: \"raw_data/smile.rar\"!\n[INFO] Data extraction done for: \"raw_data/golf.rar\"!\n[INFO] Data extraction done for: \"raw_data/shake_hands.rar\"!\n\n[INFO] Deleted raw data.\n\n\nChecking some info on the data that is been extracted.\n\nfor dirpath, dirnames, filenames in os.walk(DATA_PATH):\n    print(f'There are {len(dirnames)} directories and {len(filenames)} files in \"{dirpath}\".')\n\nThere are 51 directories and 0 files in \"data\".\nThere are 0 directories and 143 files in \"data/swing_baseball\".\nThere are 0 directories and 232 files in \"data/run\".\nThere are 0 directories and 105 files in \"data/situp\".\nThere are 0 directories and 162 files in \"data/shake_hands\".\nThere are 0 directories and 107 files in \"data/flic_flac\".\nThere are 0 directories and 127 files in \"data/hit\".\nThere are 0 directories and 102 files in \"data/kiss\".\nThere are 0 directories and 120 files in \"data/talk\".\nThere are 0 directories and 164 files in \"data/drink\".\nThere are 0 directories and 104 files in \"data/wave\".\nThere are 0 directories and 140 files in \"data/somersault\".\nThere are 0 directories and 116 files in \"data/push\".\nThere are 0 directories and 116 files in \"data/ride_horse\".\nThere are 0 directories and 128 files in \"data/kick_ball\".\nThere are 0 directories and 105 files in \"data/golf\".\nThere are 0 directories and 112 files in \"data/shoot_bow\".\nThere are 0 directories and 116 files in \"data/fencing\".\nThere are 0 directories and 102 files in \"data/smile\".\nThere are 0 directories and 145 files in \"data/dribble\".\nThere are 0 directories and 154 files in \"data/stand\".\nThere are 0 directories and 130 files in \"data/clap\".\nThere are 0 directories and 548 files in \"data/walk\".\nThere are 0 directories and 102 files in \"data/throw\".\nThere are 0 directories and 151 files in \"data/jump\".\nThere are 0 directories and 103 files in \"data/shoot_gun\".\nThere are 0 directories and 131 files in \"data/shoot_ball\".\nThere are 0 directories and 102 files in \"data/catch\".\nThere are 0 directories and 103 files in \"data/draw_sword\".\nThere are 0 directories and 142 files in \"data/sit\".\nThere are 0 directories and 127 files in \"data/sword_exercise\".\nThere are 0 directories and 108 files in \"data/climb\".\nThere are 0 directories and 104 files in \"data/pullup\".\nThere are 0 directories and 126 files in \"data/punch\".\nThere are 0 directories and 103 files in \"data/pushup\".\nThere are 0 directories and 127 files in \"data/sword\".\nThere are 0 directories and 108 files in \"data/eat\".\nThere are 0 directories and 109 files in \"data/chew\".\nThere are 0 directories and 128 files in \"data/laugh\".\nThere are 0 directories and 103 files in \"data/ride_bike\".\nThere are 0 directories and 118 files in \"data/hug\".\nThere are 0 directories and 107 files in \"data/cartwheel\".\nThere are 0 directories and 106 files in \"data/pick\".\nThere are 0 directories and 130 files in \"data/kick\".\nThere are 0 directories and 113 files in \"data/handstand\".\nThere are 0 directories and 240 files in \"data/turn\".\nThere are 0 directories and 112 files in \"data/climb_stairs\".\nThere are 0 directories and 127 files in \"data/dive\".\nThere are 0 directories and 107 files in \"data/brush_hair\".\nThere are 0 directories and 106 files in \"data/pour\".\nThere are 0 directories and 109 files in \"data/smoke\".\nThere are 0 directories and 136 files in \"data/fall_floor\"."
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#visualizing-the-dataset",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#visualizing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Visualizing the dataset",
    "text": "Visualizing the dataset\nNow that the data is downloaded, lets view the data with the labels.\n\nplt.figure(figsize=(20, 20))\nCLASSES = sorted(os.listdir(DATA_PATH))\n\n# Selecting 20 random categories for visualization\nrandom_range = random.sample(range(len(CLASSES)), 20)\n\nfor i, rand_i in enumerate(random_range, 1):\n    class_name = CLASSES[rand_i]\n\n    # Looking for the video files in the category and selecting it randomly\n    video_files_list = os.listdir(os.path.join(DATA_PATH, class_name))\n    video_file_name = random.choice(video_files_list)\n\n    # Reading the video file and displaying the first frame with label.\n    video_reader = cv2.VideoCapture(os.path.join(DATA_PATH, class_name, video_file_name))\n    _, frame = video_reader.read()\n    video_reader.release()\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    cv2.putText(rgb_frame, class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n    plt.subplot(5, 4, i)\n    plt.title(f'Category: {class_name},\\nShape: {rgb_frame.shape}')\n    plt.imshow(rgb_frame)\n    plt.axis(False);"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#preprocessing-the-dataset",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#preprocessing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Preprocessing the Dataset",
    "text": "Preprocessing the Dataset\nCreating pytorch Dataset.\nIn this steps we will be creating a custom dataset class using the functionality provided by PyTorch. 1. Read the video file. 2. Extracting the frames after every distance for a max sequence length. 3. Assigning label index for all the classes. 4. Performing torchvision.transform on the video data. 5. Returning tuple of video and label data as a tensor.\n\n# Defining variables\nSEQUENCE_LENGTH = 16 # Frames feeded to the model as a single sequence\nNUM_CLASSES = 20 # Selecting 20 classes from 51 classes\n\nBefore creating the dataset, first lets clean it. Here we will check whether the videos are readable and the frame count needs to be more than the Sequence Length frames.\n\nfiles_list = [os.path.join(DATA_PATH, dir, video) for dir in CLASSES for video in os.listdir(DATA_PATH + \"/\" + dir)]\nremove_file_list = []\nfor file in files_list:\n    vframes, _, _ = read_video(filename=file, pts_unit='sec', output_format='TCHW')\n    if len(vframes) &lt;= SEQUENCE_LENGTH:\n        print(f'File Name: {file} and Total Frames Count: {len(vframes)}')\n        remove_file_list.append(file)\nremove_file_list\n\n[]\n\n\n\n# Deleting the video files\nif len(remove_file_list) &gt; 0:\n    for file_path in remove_file_list:\n        try: \n            os.remove(path=file_path)\n            print(f'File \"{file_path}\" is been deleted')\n        except FileNotFoundError:\n            print('File is missing or is been already deleted')\nelse:\n    print('There are no files to delete.')\n\nThere are no files to delete.\n\n\n\nclass HumanActionDataset(Dataset):\n    \"\"\"\n    A class to create a PyTorch dataset using the video files directory,\n    transforming the video data and returning the data and labels for every index.\n    \n    Parameters:\n        video_dir: str, A string containing the path for all the video categories or classes.\n        seq_len: int(default: 20), A integer specifing the required total sequence frames.\n        num_classes: int(default: 20), Selecting random classes and the max allowed is 51 classes for this dataset.\n        transform: default: None, A torchvision transforms for applying multiple transformation on every frame.\n    Returns:\n        selected_frame, label: tuple, A tuple containing the video \"TCHW\" and label data in tensor format. \n    \"\"\"\n    def __init__(self, video_dir: str, seq_len: int, num_classes: int, transform=None):\n        self.video_dir = video_dir\n        self.seq_len = seq_len\n        self.num_classes = num_classes\n        self.transform = transform \n        \n        # Creating a list of all the video files path and classes.\n        set_seed(seed=42)\n        if self.num_classes &gt; 51:\n            self.num_classes = 51\n        self.classes = random.sample(sorted(os.listdir(video_dir)), k=self.num_classes)\n        self.files_list = [os.path.join(video_dir, dir, video) for dir in self.classes for video in os.listdir(video_dir + \"/\" + dir)]\n        \n    def __len__(self):\n        return len(self.files_list)\n    \n    def __getitem__(self, index):\n        video_path = self.files_list[index]\n        \n        # Reading the video file\n        vframes, _, _ = read_video(filename=video_path, pts_unit='sec', output_format='TCHW')\n        vframes = vframes.type(torch.float32)\n        vframes_count = len(vframes)\n        \n        # Selecting frames at certain interval\n        skip_frames = max(int(vframes_count/self.seq_len), 1)\n        selected_frame = vframes[0].unsqueeze(0)\n        \n        # Creating a new sequence of frames upto the defined sequence length.\n        for i in range(1, self.seq_len):\n            selected_frame = torch.concat((selected_frame, vframes[i * skip_frames].unsqueeze(0)))\n            \n        # Video label as per the classes list.\n        label = torch.tensor(self.classes.index(video_path.split('/')[1]))\n        \n        # Applying transformation to the frames\n        if self.transform:\n            return self.transform(selected_frame), label\n        else:\n            return selected_frame, label\n\n\n# Creating the dataset\ndataset = HumanActionDataset(video_dir=DATA_PATH, seq_len=SEQUENCE_LENGTH, num_classes=NUM_CLASSES)\nprint(f'Length of the dataset: {len(dataset)}')\n\n# Checking the dataset shape\nframes, label = dataset[2000]\nprint(f'Video tensor(first frame):\\n{frames[0]}\\nlabel tensor: {label}')\nprint(f'Shape of the video tensor: {frames.shape} and shape of the label: {label.shape}')\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n         [ 2.,  2.,  0.,  ..., 11., 11., 11.],\n         ...,\n         [ 2.,  2.,  0.,  ..., 55., 55., 55.],\n         [ 5.,  5.,  3.,  ..., 62., 62., 62.],\n         [ 5.,  5.,  2.,  ..., 34., 34., 34.]],\n\n        [[ 4.,  4.,  5.,  ...,  2.,  1.,  1.],\n         [ 5.,  5.,  5.,  ...,  3.,  3.,  3.],\n         [ 4.,  4.,  5.,  ..., 17., 17., 17.],\n         ...,\n         [ 2.,  2.,  2.,  ..., 41., 41., 41.],\n         [ 0.,  0.,  1.,  ..., 49., 49., 49.],\n         [ 0.,  0.,  0.,  ..., 21., 21., 21.]],\n\n        [[ 3.,  3.,  4.,  ...,  2.,  1.,  1.],\n         [ 4.,  4.,  4.,  ...,  3.,  3.,  3.],\n         [ 1.,  1.,  1.,  ...,  8.,  8.,  8.],\n         ...,\n         [ 0.,  0.,  0.,  ..., 16., 14., 14.],\n         [ 0.,  0.,  0.,  ..., 28., 28., 28.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([16, 3, 240, 320]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-a-pretrained-model",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-a-pretrained-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating A Pretrained Model",
    "text": "Creating A Pretrained Model\n\ndef create_model(num_classes: int, device: torch.device, seed: int = 42):\n    \"\"\"\n    A function to create a model.\n    Parameters:\n        num_classes: int, A integer for toal number of classes.\n        device: torch.device, A torch device to set the tensors to cpu or cuda.\n        seed: int(default: 42), A random seed value.\n    Returns: \n        model: A feature extracted model for video classification.\n        transforms: A torchvision transform is returned which was used in the pretrained model.    \n    \"\"\"\n    # Creating model, weights and transforms\n    weights = MViT_V2_S_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = mvit_v2_s(weights=weights)\n    \n    # Freezing the model layers\n    for params in model.parameters():\n        params.requires_grad = False\n        \n    # Changing the fully Conncected head layer\n    set_seed(seed)\n    dropout_layer = model.head[0]\n    in_features = model.head[1].in_features\n    model.head = torch.nn.Sequential(\n        dropout_layer,\n        torch.nn.Linear(in_features=in_features, out_features=num_classes, bias=True, device=device)\n    )\n    return model.to(device), transforms\n\n\n# Creating the model and transforms using the function.\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\n\n\nsummary(model, \n        input_size=(1, 3, 16, 224, 224),\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])\n\n/usr/local/lib/python3.9/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  action_fn=lambda data: sys.getsizeof(data.storage()),\n/usr/local/lib/python3.9/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return super().__sizeof__() + self.nbytes()\n\n\n==================================================================================================================================\nLayer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n==================================================================================================================================\nMViT (MViT)                                        [1, 3, 16, 224, 224] [1, 20]              --                   Partial\n├─Conv3d (conv_proj)                               [1, 3, 16, 224, 224] [1, 96, 8, 56, 56]   (42,432)             False\n├─PositionalEncoding (pos_encoding)                [1, 25088, 96]       [1, 25089, 96]       (96)                 False\n├─ModuleList (blocks)                              --                   --                   --                   False\n│    └─MultiscaleBlock (0)                         [1, 25089, 96]       [1, 25089, 96]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 25089, 96]       (68,352)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MLP (mlp)                              [1, 25089, 96]       [1, 25089, 96]       (74,208)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    └─MultiscaleBlock (1)                         [1, 25089, 96]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 6273, 192]       (113,280)            False\n│    │    └─Linear (project)                       [1, 25089, 96]       [1, 25089, 192]      (18,624)             False\n│    │    └─Pool (pool_skip)                       [1, 25089, 192]      [1, 6273, 192]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (2)                         [1, 6273, 192]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 6273, 192]       (168,576)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (3)                         [1, 6273, 192]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 1569, 384]       (385,152)            False\n│    │    └─Linear (project)                       [1, 6273, 192]       [1, 6273, 384]       (74,112)             False\n│    │    └─Pool (pool_skip)                       [1, 6273, 384]       [1, 1569, 384]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (4)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (5)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (6)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (7)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (8)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (9)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (10)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (11)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (12)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (13)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (14)                        [1, 1569, 384]       [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 393, 768]        (1,492,608)          False\n│    │    └─Linear (project)                       [1, 1569, 384]       [1, 1569, 768]       (295,680)            False\n│    │    └─Pool (pool_skip)                       [1, 1569, 768]       [1, 393, 768]        --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    └─MultiscaleBlock (15)                        [1, 393, 768]        [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MultiscaleAttention (attn)             [1, 393, 768]        [1, 393, 768]        (2,374,656)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n├─LayerNorm (norm)                                 [1, 393, 768]        [1, 393, 768]        (1,536)              False\n├─Sequential (head)                                [1, 768]             [1, 20]              --                   True\n│    └─Dropout (0)                                 [1, 768]             [1, 768]             --                   --\n│    └─Linear (1)                                  [1, 768]             [1, 20]              15,380               True\n==================================================================================================================================\nTotal params: 34,245,524\nTrainable params: 15,380\nNon-trainable params: 34,230,144\nTotal mult-adds (G): 1.64\n==================================================================================================================================\nInput size (MB): 9.63\nForward/backward pass size (MB): 1658.93\nParams size (MB): 136.46\nEstimated Total Size (MB): 1805.02\n==================================================================================================================================\n\n\n\n# Checking the model transforms and applying it on the dataset\nprint(f'Pretrained Model Transforms:\\n{transforms}\\n')\ndataset = HumanActionDataset(video_dir=DATA_PATH, seq_len=SEQUENCE_LENGTH, num_classes=NUM_CLASSES, transform=transforms)\nprint(f'Length of the dataset: {len(dataset)}')\n\n# Checking the dataset and shape\nframes, label = dataset[2000]\nprint(f'Video tensor(first frame):\\n{frames[0]}\\nlabel tensor: {label}')\nprint(f'Shape of the video tensor: {frames.shape} and shape of the label: {label.shape}')\n\nPretrained Model Transforms:\nVideoClassification(\n    crop_size=[224, 224]\n    resize_size=[256]\n    mean=[0.45, 0.45, 0.45]\n    std=[0.225, 0.225, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 2.4604e+02,  2.3502e+02,  2.2911e+02,  ...,  2.8675e+02,\n           2.8675e+02,  2.8684e+02],\n         [ 2.3070e+02,  2.2561e+02,  2.1813e+02,  ...,  3.1508e+02,\n           3.1508e+02,  3.1508e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1425e+02,\n           3.1425e+02,  3.1425e+02],\n         ...,\n         [ 3.2401e+00,  2.9817e+00, -2.0000e+00,  ...,  1.6333e+02,\n           1.2992e+02,  9.1900e+01],\n         [-1.5833e+00, -1.7232e+00, -2.0000e+00,  ...,  4.4867e+02,\n           3.7443e+02,  2.8330e+02],\n         [ 2.4444e+00,  9.5210e-01, -2.0000e+00,  ...,  6.6754e+02,\n           6.3805e+02,  5.6134e+02]],\n\n        [[ 2.2467e+02,  2.2616e+02,  2.2911e+02,  ...,  3.0883e+02,\n           3.0883e+02,  3.0902e+02],\n         [ 2.2869e+02,  2.2478e+02,  2.1813e+02,  ...,  3.1314e+02,\n           3.1314e+02,  3.1581e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1356e+02,\n           3.1356e+02,  3.1402e+02],\n         ...,\n         [ 9.1056e+00,  6.2105e+00, -2.0000e+00,  ...,  9.9700e+01,\n           7.6676e+01,  2.8844e+01],\n         [ 1.9746e+01,  1.7561e+01, -1.1667e+00,  ...,  2.1729e+02,\n           1.6955e+02,  1.1592e+02],\n         [ 6.6111e+00,  6.6111e+00,  6.6111e+00,  ...,  5.4941e+02,\n           5.2508e+02,  4.3940e+02]],\n\n        [[-2.0000e+00,  5.3684e+00,  2.1125e+01,  ...,  3.2244e+02,\n           3.1922e+02,  3.2077e+02],\n         [-3.9952e-01,  4.0798e+00,  9.3575e+00,  ...,  3.2330e+02,\n           3.1922e+02,  3.1025e+02],\n         [ 2.0259e+00,  2.6776e+00,  5.3857e+00,  ...,  3.3464e+02,\n           3.3047e+02,  3.1790e+02],\n         ...,\n         [ 1.7578e+02,  1.7578e+02,  1.7719e+02,  ...,  2.3105e+02,\n           2.2658e+02,  2.1946e+02],\n         [ 1.7803e+02,  1.7884e+02,  1.8227e+02,  ...,  2.3562e+02,\n           2.2931e+02,  2.2306e+02],\n         [ 1.9990e+02,  2.0851e+02,  2.2140e+02,  ...,  2.2817e+02,\n           2.1964e+02,  2.2187e+02]],\n\n        ...,\n\n        [[ 4.2491e+02,  4.3142e+02,  4.2619e+02,  ...,  1.5639e+01,\n           1.5639e+01,  1.5639e+01],\n         [ 4.0218e+02,  4.0793e+02,  4.1056e+02,  ...,  6.0095e+00,\n           7.7222e+00,  2.3716e+00],\n         [ 4.0126e+02,  4.0792e+02,  4.0992e+02,  ...,  8.7490e+00,\n           1.0639e+01,  2.2439e+00],\n         ...,\n         [ 1.2787e+02,  1.2839e+02,  1.3253e+02,  ...,  9.5269e-01,\n           1.7500e+00,  1.7500e+00],\n         [ 1.3349e+02,  1.2883e+02,  1.3904e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 1.3694e+02,  1.3320e+02,  1.3960e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.5630e+02,  4.6579e+02,  4.6640e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3329e+02,  4.3661e+02,  4.3776e+02,  ...,  1.1714e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4439e+02,  4.4481e+02,  4.4500e+02,  ...,  1.4995e+00,\n          -7.8133e-01, -2.0000e+00],\n         ...,\n         [ 1.4467e+02,  1.5315e+02,  1.7425e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.3056e+00],\n         [ 1.4625e+02,  1.5174e+02,  1.6260e+02,  ..., -2.0000e+00,\n           9.2336e-01,  2.0278e+00],\n         [ 1.6148e+02,  1.6227e+02,  1.5224e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.2527e+02,  4.4359e+02,  4.6669e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3037e+02,  4.3994e+02,  4.5392e+02,  ...,  2.0278e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4064e+02,  4.4451e+02,  4.5152e+02,  ...,  2.4444e+00,\n          -7.8133e-01,  4.9081e-01],\n         ...,\n         [ 1.4709e+02,  1.5459e+02,  1.5781e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.7668e+00],\n         [ 1.4016e+02,  1.5057e+02,  1.5565e+02,  ..., -1.5833e+00,\n           1.3400e+00,  4.5890e-02],\n         [ 1.3139e+02,  1.3590e+02,  1.4467e+02,  ...,  2.4444e+00,\n           2.4444e+00,  5.3965e+00]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([3, 16, 224, 224]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-dataloaders",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#creating-dataloaders",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating DataLoaders",
    "text": "Creating DataLoaders\n\n# Spliting the dataset in train and test dataset for ratio of 75:25.\ndataset = HumanActionDataset(video_dir=DATA_PATH, \n                             seq_len=SEQUENCE_LENGTH,\n                             num_classes=NUM_CLASSES,\n                             transform=transforms)\ntrain_dataset, test_dataset = random_split(dataset=dataset,\n                                           lengths=[0.75, 0.25], \n                                           generator=torch.Generator().manual_seed(42))\nprint(f'Train Dataset Length: {len(train_dataset)} and Test Dataset length: {len(test_dataset)}')\n\nTrain Dataset Length: 1898 and Test Dataset length: 632\n\n\n\n# Creating train and test dataloaders.\ndef create_dataloaders(dataset, batch, shuffle, workers):\n    \"\"\"\n    A function to create pytorch dataloaders.\n    \n    Parameters:\n        dataset: A pytorch dataset.\n        batch: A integer for batch size.\n        shuffle: A boolean for shuffling the data.\n        workers: A integer to set the workers in dataloaders.\n        \n    Returns: dataloader.\n    \"\"\"\n    dataloader = DataLoader(dataset=dataset, \n                            batch_size=batch, \n                            shuffle=shuffle, \n                            num_workers=workers,\n                            pin_memory=True,\n                            drop_last=True)\n    print(f'Dataloaders Length: {len(dataloader)} for a batch size of: {batch}')\n    return dataloader\n\nset_seed(42)\nBATCH_SIZE = 16\nWORKERS = os.cpu_count()\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\n\n\nLabel Distribution and Zero Rule Benchmark\n\ntrain_counter = Counter()\nfor frames, labels in train_dataloader:\n    train_counter.update(labels.tolist())\nprint('Training Dataset Distribution:\\n', sorted(train_counter.items()))\n\nTraining Dataset Distribution:\n [(0, 78), (1, 92), (2, 83), (3, 79), (4, 94), (5, 72), (6, 77), (7, 79), (8, 85), (9, 103), (10, 90), (11, 106), (12, 88), (13, 78), (14, 180), (15, 70), (16, 87), (17, 100), (18, 171), (19, 76)]\n\n\n\ntest_counter = Counter()\nfor frames, labels in test_dataloader:\n    test_counter.update(labels.tolist())\nprint('Testing Dataset Distribution:\\n', sorted(test_counter.items()))\n\nTesting Dataset Distribution:\n [(0, 30), (1, 32), (2, 24), (3, 23), (4, 29), (5, 32), (6, 30), (7, 24), (8, 27), (9, 27), (10, 17), (11, 35), (12, 37), (13, 24), (14, 58), (15, 33), (16, 29), (17, 26), (18, 60), (19, 27)]\n\n\n\n# Zero Rule Benchmark\nmajor_class = test_counter.most_common(1)[0]\nprint(f'The Major class in Testing DataLoader is: Class Name: {CLASSES[major_class[0]]}, Class Index: {major_class[0]} and Total Number of Data: {major_class[1]}')\n\nzero_rule_acc = (major_class[1] / sum(test_counter.values())) * 100\nprint(f'Zero Rule Basline Benchmark Accuracy for predicting the major class: {zero_rule_acc:.2f}%')\n\nThe Major class in Testing DataLoader is: Class Name: hug, Class Index: 18 and Total Number of Data: 60\nZero Rule Basline Benchmark Accuracy for predicting the major class: 9.62%"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch---model-training-and-testing",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch - Model Training and Testing",
    "text": "PyTorch - Model Training and Testing\n\ndef train_step(model, dataloader, loss_fn, optimizer, device, num_classes):\n    \"\"\"\n    Trains a pytorch model by going into train mode and applying forward pass,\n    loss calculation and optimizer step.\n    \n    Parameters:\n        model: A pytorch model for training.\n        dataloader: A pytorch dataloader for training.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        device: A torch device to allocate tensors on 'cpu' or 'cuda'.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of training loss and training accuracy.\n        \n    \"\"\"\n    # Model on training mode\n    model.train()\n    \n    # Setting train loss and accuracy \n    train_loss = 0\n    train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(device)\n    \n    # Looping the dataloaders\n    for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Training', total=len(dataloader), unit='batch'):\n        X, y = X.to(device), y.to(device)\n        \n        # 5 step to train a model\n        y_pred = model(X) # 1. Forward pass\n        loss = loss_fn(y_pred, y) # 2. Calculate loss\n        train_loss += loss.item() \n        optimizer.zero_grad() # 3. Initiate optimizer\n        loss.backward() # 4. Backward pass\n        optimizer.step() # 5. Updating the model parameters\n        \n        # Calculating the training accuracy\n        y_pred_labels = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc.compute()\n    return train_loss, train_acc\n\ndef test_step(model, dataloader, loss_fn, device, num_classes):\n    \"\"\"\n    Test a pytorch model by going into eval mode and applying forward pass,\n    and loss calculation.\n    \n    Parameters:\n        model: A pytorch model for testing.\n        dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        device: A torch device to allocate tensors on 'cpu' or 'cuda'.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of testing loss and testing accuracy.\n    \"\"\"\n    # Model on evaluation mode\n    model.eval()\n    \n    # Setting train loss and accuracy \n    test_loss = 0\n    test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(device)\n    \n    # Using inference mode\n    with torch.no_grad():\n        # Looping the dataloaders\n        for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Evaluation', total=len(dataloader), unit='batch'):\n            X, y = X.to(device), y.to(device)\n            \n            # Forward pass\n            y_pred = model(X)\n            \n            # Calculate loss\n            loss = loss_fn(y_pred, y)\n            test_loss += loss.item()\n            \n            # Calculate accuracy\n            y_pred_labels = y_pred.argmax(dim=1)\n            test_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc.compute()\n    return test_loss, test_acc\n\ndef model_train(epochs, model, train_dataloader, test_dataloader, optimizer, loss_fn, device, num_classes):\n    \"\"\"\n    Trains a pytorch model for a certain number of epochs going through the model training \n    and testing stage, and accumalating the loss, accuracy, and training and testing time.\n    \n    Parameters:\n        epochs: A integer to run the training and testing stage. \n        model: A pytorch model for training and testing.\n        train_dataloader: A pytorch dataloader for training.\n        test_dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        device: A torch device to allocate tensors on 'cpu' or 'cuda'.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of accumaleted results in dict and total training time in float datatype.\n    \"\"\"\n    # Create empty result\n    results = {'train_loss': [],\n               'train_acc': [],\n               'test_loss': [],\n               'test_acc': [],\n               'train_epoch_time(min)': [],\n               'test_epoch_time(min)': []}\n    \n    # Loop through training and testing steps\n    model_train_start_time = timer()\n    for epoch in tqdm(range(epochs), desc=f'Training and Evaluation for {epochs} Epochs', unit='epochs'):\n        # Training the model and timing it.\n        train_epoch_start_time = timer()\n        train_loss, train_acc = train_step(model=model, \n                                           dataloader=train_dataloader, \n                                           loss_fn=loss_fn, \n                                           optimizer=optimizer, \n                                           device=device, \n                                           num_classes=num_classes)\n        train_epoch_stop_time = timer()\n        train_epoch_time = (train_epoch_stop_time - train_epoch_start_time)/60\n        \n        # Testing the model and timing it\n        test_epoch_start_time = timer()\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        device=device,\n                                        num_classes=num_classes)\n        test_epoch_stop_time = timer()\n        test_epoch_time = (test_epoch_stop_time - test_epoch_start_time)/60\n        \n        # Print the model result\n        print(f'Epoch: [{epoch+1}/{epochs}] | train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | train_time: {train_epoch_time:.4f} min | '\n              f'test loss: {test_loss:.4f} | test_acc: {test_acc:.4f} | test_time: {test_epoch_time:.4f} min')\n        \n        # Saving the results\n        results['train_loss'].append(train_loss)\n        results['train_acc'].append(train_acc.detach().cpu().item())\n        results['test_loss'].append(test_loss)\n        results['test_acc'].append(test_acc.detach().cpu().item())\n        results['train_epoch_time(min)'].append(train_epoch_time)\n        results['test_epoch_time(min)'].append(test_epoch_time)\n        \n    # Calculating total model training time\n    model_train_end_time = timer()\n    total_train_time = (model_train_end_time - model_train_start_time)/60\n    print(f'\\nTotal Model Training Time: {total_train_time:.4f} min')\n    return results, total_train_time\n\n\nExp 1: PyTorch Before 2.0 (PyTorch Eager Mode)\n\nset_seed(42)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\nmodel.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp1_results, exp1_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  device=device, \n                                                  num_classes=len(dataset.classes))\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.4238 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5183 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.4201 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5181 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.3959 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.5176 min\n\nTotal Model Training Time: 5.7940 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Creating a result directory\nRESULTS_DIR = 'results'\nif os.path.exists(RESULTS_DIR):\n    print('[INFO] results directory exists.')\nelse:\n    os.mkdir(RESULTS_DIR)\n    print('[INFO] results directory is been created.')\n\n# Saving the result in csv format\nexp1_results_filename = 'exp1_results.csv'\nexp1_results_df = pd.DataFrame(exp1_results)\nexp1_results_df.to_csv(os.path.join(RESULTS_DIR, exp1_results_filename), index=False)\nexp1_results_df\n\n[INFO] results directory is been created.\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.423781\n0.518263\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.420147\n0.518100\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.395912\n0.517590\n\n\n\n\n\n\n\n\n# Creating and saving a dataframe for model train time \nmodel_train_time_filename = 'model_train_time.csv'\nmodel_train_time_df = pd.DataFrame({'model': ['Pytorch Eager'], 'training_time(min)': [exp1_total_train_time]})\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n\n\n\n\n\n\n\nExp 2: PyTorch 2.0 (PyTorch Compile Mode)\n\nset_seed(42)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\nmodel.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp2_results, exp2_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model,\n                                                  train_dataloader=train_dataloader,\n                                                  test_dataloader=test_dataloader,\n                                                  optimizer=optimizer,\n                                                  loss_fn=loss_fn,\n                                                  device=device,\n                                                  num_classes=len(dataset.classes))\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.3885 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0454 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.6187 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5678 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.6008 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5694 min\n\nTotal Model Training Time: 7.7908 min\n\n\n\n\n\n\n\n\n/usr/local/lib/python3.9/dist-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n  warnings.warn(\n[2023-04-06 06:39:50,385] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp2_results_filename = 'exp2_results.csv'\nexp2_results_df = pd.DataFrame(exp2_results)\nexp2_results_df.to_csv(os.path.join(RESULTS_DIR, exp2_results_filename), index=False)\nexp2_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.388515\n1.045425\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.618742\n0.567790\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.600783\n0.569392\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Pytorch Compile', exp2_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch-lightning---model-training-and-testing",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#pytorch-lightning---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch Lightning - Model Training and Testing",
    "text": "PyTorch Lightning - Model Training and Testing\n\nclass PyLightHMDB51(L.LightningModule):\n    \"\"\"\n    A Lightning Module containing Model training and validation step.\n    Parameters: \n        model: A PyTorch Model.\n        loss_fn: A PyTorch loss function.\n        optimizer: A Pytorch Optimizer.\n        num_classes: A integer for total number of classes in the dataset.\n    \"\"\"\n    def __init__(self, model, loss_fn, optimizer, num_classes):\n        super().__init__()\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.num_classes = num_classes\n        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n        self.test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def training_step(self, train_batch, batch_idx):\n        X, y = train_batch\n        y_preds = self.forward(X)\n        loss = self.loss_fn(y_preds, y)\n        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n        y_pred_labels = torch.argmax(torch.softmax(y_preds, dim=1), dim=1)\n        self.train_acc.update(y_pred_labels, y)\n        self.log('train_acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n        return loss\n    \n    def validation_step(self, val_batch, batch_idx):\n        X, y = val_batch\n        y_preds = self.forward(X)\n        loss = self.loss_fn(y_preds, y)\n        self.log('test_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n        y_pred_labels = torch.argmax(torch.softmax(y_preds, dim=1), dim=1)\n        self.test_acc.update(y_pred_labels, y)\n        self.log('test_acc', self.test_acc, prog_bar=True, on_step=False, on_epoch=True)\n    \n    def configure_optimizers(self):\n        optimizers = self.optimizer\n        return optimizers\n\n\nExp 3: PyTorch Lightning\n\nset_seed(42)\n\n# Creating the pytorch lightning trainer\nNUM_EPOCHS = 3\nlogger = L.pytorch.loggers.CSVLogger(save_dir=RESULTS_DIR, \n                                     name=\"pytorch_lightning\")\ntrainer = L.Trainer(max_epochs=NUM_EPOCHS, \n                    logger=logger)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Initializing the lightning module class\nmodel = PyLightHMDB51(model=model, \n                      loss_fn=loss_fn, \n                      optimizer=optimizer, \n                      num_classes=len(dataset.classes))\n\n# Fiting the model to trainer.\nstart_time = timer()\ntrainer.fit(model=model, \n            train_dataloaders=train_dataloader, \n            val_dataloaders=test_dataloader)\nend_time = timer()\nexp3_total_train_time = (end_time - start_time)/60\nprint(f'Total Time to train the model: {exp3_total_train_time:.4f} min')\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nMissing logger folder: results/pytorch_lightning\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type               | Params\n-------------------------------------------------\n0 | model     | MViT               | 34.2 M\n1 | loss_fn   | CrossEntropyLoss   | 0     \n2 | train_acc | MulticlassAccuracy | 0     \n3 | test_acc  | MulticlassAccuracy | 0     \n-------------------------------------------------\n15.4 K    Trainable params\n34.2 M    Non-trainable params\n34.2 M    Total params\n136.982   Total estimated model params size (MB)\n`Trainer.fit` stopped: `max_epochs=3` reached.\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 7.6887 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Pytorch Lightning', exp3_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n\n\n\n\n\n\n\nExp 4: PyTorch Lightning - Compile Mode\n\nset_seed(42)\n\n# Creating the pytorch lightning trainer\nNUM_EPOCHS = 3\nlogger = L.pytorch.loggers.CSVLogger(save_dir=RESULTS_DIR, \n                                     name=\"pytorch_lightning_compile_mode\")\ntrainer = L.Trainer(max_epochs=NUM_EPOCHS, \n                    logger=logger)\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Initializing the lightning module class\nmodel = PyLightHMDB51(model=model, \n                      loss_fn=loss_fn, \n                      optimizer=optimizer, \n                      num_classes=len(dataset.classes))\n\n# Fiting the model to trainer.\nstart_time = timer()\ntrainer.fit(model=model, \n            train_dataloaders=train_dataloader, \n            val_dataloaders=test_dataloader)\nend_time = timer()\nexp4_total_train_time = (end_time - start_time)/60\nprint(f'Total Time to train the model: {exp4_total_train_time:.4f} min')\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nMissing logger folder: results/pytorch_lightning_compile_mode\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type               | Params\n-------------------------------------------------\n0 | model     | OptimizedModule    | 34.2 M\n1 | loss_fn   | CrossEntropyLoss   | 0     \n2 | train_acc | MulticlassAccuracy | 0     \n3 | test_acc  | MulticlassAccuracy | 0     \n-------------------------------------------------\n15.4 K    Trainable params\n34.2 M    Non-trainable params\n34.2 M    Total params\n136.982   Total estimated model params size (MB)\n[2023-04-06 06:56:20,261] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n`Trainer.fit` stopped: `max_epochs=3` reached.\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 10.5425 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['PyTorch Lightning + Compile', exp4_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#lightning-fabric---model-training-and-testing",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#lightning-fabric---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Lightning Fabric - Model training and Testing",
    "text": "Lightning Fabric - Model training and Testing\n\ndef train_step(model, dataloader, loss_fn, optimizer, fabric, num_classes):\n    \"\"\"\n    Trains a pytorch model by going into train mode and applying forward pass,\n    loss calculation and optimizer step.\n    \n    Parameters:\n        model: A pytorch model for training.\n        dataloader: A pytorch dataloader for training.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        fabric: A Fabric function to setup device for the tensors and gradients.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of training loss and training accuracy.\n        \n    \"\"\"\n    # Model on training mode\n    model.train()\n    \n    # Setting train loss and accuracy \n    train_loss = 0\n    train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(fabric.device) # New by Fabric\n    \n    # Looping the dataloaders\n    for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Training', total=len(dataloader), unit='batch'):\n        # X, y = X.to(device), y.to(device) # New by Fabric\n        \n        # 5 step to train a model\n        y_pred = model(X) # 1. Forward pass\n        loss = loss_fn(y_pred, y) # 2. Calculate loss\n        train_loss += loss.item() \n        optimizer.zero_grad() # 3. Initiate optimizer\n        #loss.backward() # 4. Backward pass\n        fabric.backward(loss) # New by Fabric\n        optimizer.step() # 5. Updating the model parameters\n        \n        # Calculating the training accuracy\n        y_pred_labels = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc.compute()\n    return train_loss, train_acc\n\ndef test_step(model, dataloader, loss_fn, fabric, num_classes):\n    \"\"\"\n    Test a pytorch model by going into eval mode and applying forward pass,\n    and loss calculation.\n    \n    Parameters:\n        model: A pytorch model for testing.\n        dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        fabric: A Fabric function to setup device for the tensors and gradients.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of testing loss and testing accuracy.\n    \"\"\"\n    # Model on evaluation mode\n    model.eval()\n    \n    # Setting train loss and accuracy \n    test_loss = 0\n    test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(fabric.device) # New by Fabric\n    \n    # Using inference mode\n    with torch.no_grad():\n        # Looping the dataloaders\n        for batch, (X, y) in tqdm(enumerate(dataloader), desc='Model Evaluation', total=len(dataloader), unit='batch'):\n            # X, y = X.to(device), y.to(device) # New by Fabric\n            \n            # Forward pass\n            y_pred = model(X)\n            \n            # Calculate loss\n            loss = loss_fn(y_pred, y)\n            test_loss += loss.item()\n            \n            # Calculate accuracy\n            y_pred_labels = y_pred.argmax(dim=1)\n            test_acc.update(y_pred_labels, y)\n    \n    # Averaging the loss and accuracy\n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc.compute()\n    return test_loss, test_acc\n\ndef model_train(epochs, model, train_dataloader, test_dataloader, optimizer, loss_fn, fabric, num_classes):\n    \"\"\"\n    Trains a pytorch model for a certain number of epochs going through the model training \n    and testing stage, and accumalating the loss, accuracy, and training and testing time.\n    \n    Parameters:\n        epochs: A integer to run the training and testing stage. \n        model: A pytorch model for training and testing.\n        train_dataloader: A pytorch dataloader for training.\n        test_dataloader: A pytorch dataloader for testing.\n        loss_fn: A pytorch loss to calculate the model's prediction loss.\n        optimizer: A pytorch optimizer to minimize the loss function.\n        fabric: A Fabric function to setup device for the tensors and gradients.\n        num_classes: A integer that indicates total number of classes in the dataset.\n        \n    Returns: A tuple of accumaleted results in dict and total training time in float datatype.\n    \"\"\"\n    # Create empty result\n    results = {'train_loss': [],\n               'train_acc': [],\n               'test_loss': [],\n               'test_acc': [],\n               'train_epoch_time(min)': [],\n               'test_epoch_time(min)': []}\n    \n    # Loop through training and testing steps\n    model_train_start_time = timer()\n    for epoch in tqdm(range(epochs), desc=f'Training and Evaluation for {epochs} Epochs', unit='epochs'):\n        # Training the model and timing it.\n        train_epoch_start_time = timer()\n        train_loss, train_acc = train_step(model=model, \n                                           dataloader=train_dataloader, \n                                           loss_fn=loss_fn, \n                                           optimizer=optimizer, \n                                           fabric=fabric, # New by Fabric\n                                           num_classes=num_classes)\n        train_epoch_stop_time = timer()\n        train_epoch_time = (train_epoch_stop_time - train_epoch_start_time)/60\n        \n        # Testing the model and timing it\n        test_epoch_start_time = timer()\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        fabric=fabric, # New by Fabric\n                                        num_classes=num_classes)\n        test_epoch_stop_time = timer()\n        test_epoch_time = (test_epoch_stop_time - test_epoch_start_time)/60\n        \n        # Print the model result\n        print(f'Epoch: [{epoch+1}/{epochs}] | train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | train_time: {train_epoch_time:.4f} min | '\n              f'test loss: {test_loss:.4f} | test_acc: {test_acc:.4f} | test_time: {test_epoch_time:.4f} min')\n        \n        # Saving the results\n        results['train_loss'].append(train_loss)\n        results['train_acc'].append(train_acc.detach().cpu().item())\n        results['test_loss'].append(test_loss)\n        results['test_acc'].append(test_acc.detach().cpu().item())\n        results['train_epoch_time(min)'].append(train_epoch_time)\n        results['test_epoch_time(min)'].append(test_epoch_time)\n        \n    # Calculating total model training time\n    model_train_end_time = timer()\n    total_train_time = (model_train_end_time - model_train_start_time)/60\n    print(f'\\nTotal Model Training Time: {total_train_time:.4f} min')\n    return results, total_train_time\n\n\nExp 5: Fabric Lightning\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric()\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp5_results, exp5_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5801 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5829 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.5967 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5939 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5900 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.6074 min\n\nTotal Model Training Time: 6.5512 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp5_results_filename = 'exp5_results.csv'\nexp5_results_df = pd.DataFrame(exp5_results)\nexp5_results_df.to_csv(os.path.join(RESULTS_DIR, exp5_results_filename), index=False)\nexp5_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.580098\n0.582881\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.596745\n0.593947\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.590004\n0.607377\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric Lightning', exp5_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n\n\n\n\n\n\n\nExp 6: Fabric Lightning - Compile mode\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric()\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp6_results, exp6_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n[2023-04-06 07:12:28,856] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.1276 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0502 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.7023 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5721 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.5845 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5688 min\n\nTotal Model Training Time: 7.6056 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp6_results_filename = 'exp6_results.csv'\nexp6_results_df = pd.DataFrame(exp6_results)\nexp6_results_df.to_csv(os.path.join(RESULTS_DIR, exp6_results_filename), index=False)\nexp6_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.127585\n1.050178\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.702294\n0.572100\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.584458\n0.568837\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric + Compile', exp6_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n\n\n\n\n\n\n\nExp 7: Fabric + Precision\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric(precision='16-mixed')\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp7_results, exp7_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nUsing 16-bit Automatic Mixed Precision (AMP)\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5438 min | test loss: 2.6781 | test_acc: 0.4519 | test_time: 0.5552 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4168 | train_time: 1.5081 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5554 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5786 min | test loss: 2.2094 | test_acc: 0.5817 | test_time: 0.5744 min\n\nTotal Model Training Time: 6.3157 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp7_results_filename = 'exp7_results.csv'\nexp7_results_df = pd.DataFrame(exp7_results)\nexp7_results_df.to_csv(os.path.join(RESULTS_DIR, exp7_results_filename), index=False)\nexp7_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853492\n0.198623\n2.678145\n0.451923\n1.543753\n0.555179\n\n\n1\n2.599140\n0.416843\n2.427695\n0.540064\n1.508068\n0.555399\n\n\n2\n2.360222\n0.523835\n2.209447\n0.581731\n1.578585\n0.574411\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric + Precision', exp7_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n\n\n\n\n\n\n\nExp 8: Fabric + Precision + Compile\n\nset_seed(42)\n\n# Initializing Fabric # New by Fabric\nfabric = Fabric(precision='16-mixed')\n\n# Initializing the model and dataloaders\nmodel, transforms = create_model(num_classes=len(dataset.classes), device=fabric.device) # New by Fabric\n# model.to(device)\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Fabric setup # New by Fabric\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader, test_dataloader = fabric.setup_dataloaders(train_dataloader, test_dataloader)\n\n# Setting up compiled model(Introduced in PyTorch 2.0.0)\nmodel = torch.compile(model, mode='default')\n\n# Training the model using the function\nNUM_EPOCHS = 3\nexp8_results, exp8_total_train_time = model_train(epochs=NUM_EPOCHS,\n                                                  model=model, \n                                                  train_dataloader=train_dataloader, \n                                                  test_dataloader=test_dataloader, \n                                                  optimizer=optimizer, \n                                                  loss_fn=loss_fn, \n                                                  fabric=fabric, \n                                                  num_classes=len(dataset.classes))\n\nUsing 16-bit Automatic Mixed Precision (AMP)\nYou are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n[2023-04-06 07:26:29,375] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 3.0326 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.1394 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.5740 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5764 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5095 | train_time: 1.5562 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5639 min\n\nTotal Model Training Time: 8.4427 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Saving the result in csv format\nexp8_results_filename = 'exp8_results.csv'\nexp8_results_df = pd.DataFrame(exp8_results)\nexp8_results_df.to_csv(os.path.join(RESULTS_DIR, exp8_results_filename), index=False)\nexp8_results_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845062\n0.201271\n2.678693\n0.450321\n3.032648\n1.139419\n\n\n1\n2.590762\n0.445975\n2.427031\n0.548077\n1.574046\n0.576357\n\n\n2\n2.366282\n0.509534\n2.209138\n0.580128\n1.556243\n0.563871\n\n\n\n\n\n\n\n\n# Updating and saving a dataframe for model train time \nmodel_train_time_df.loc[len(model_train_time_df)] = ['Fabric + Precision + Compile', exp8_total_train_time]\nmodel_train_time_df.to_csv(os.path.join(RESULTS_DIR, model_train_time_filename), index=False)\nmodel_train_time_df\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n7\nFabric + Precision + Compile\n8.442741\n\n\n\n\n\n\n\n\n# plot the train time\nmodel_train_time_df.plot.barh(x='model', \n                              y='training_time(min)', \n                              xlabel='Training Time(min)', \n                              ylabel='Training Method', \n                              legend=False)\nplt.title(f'MVit V2 Model Training Time for {NUM_EPOCHS} Epochs')\nplt.savefig('model_train_time.png')"
  },
  {
    "objectID": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "href": "docs/posts/2023-04-01_pytorch_and_lightning_2.0/notebooks/HMDB51_human_action_recognition_pytorch.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Training and Testing on Full Dataset and Deploying the Model",
    "text": "Training and Testing on Full Dataset and Deploying the Model\n\nCreating the model\n\nNUM_CLASSES = 51\nSEQUENCE_LENGTH = 16\nBATCH_SIZE = 16\nWORKERS = os.cpu_count()\n\n\n# Creating the model and transforms using the function.\nmodel, transforms = create_model(num_classes=NUM_CLASSES, device=device)\n\n\n\nGetting the Full Dataset and Dataloaders\n\n# Creating the dataset.\ndataset = HumanActionDataset(video_dir=DATA_PATH, \n                             seq_len=SEQUENCE_LENGTH,\n                             num_classes=NUM_CLASSES,\n                             transform=transforms)\ntrain_dataset, test_dataset = random_split(dataset=dataset,\n                                           lengths=[0.75, 0.25], \n                                           generator=torch.Generator().manual_seed(42))\nprint(f'Train Dataset Length: {len(train_dataset)} and Test Dataset length: {len(test_dataset)}')\n\n# Creating the dataloaders\ntrain_dataloader = create_dataloaders(dataset=train_dataset, batch=BATCH_SIZE, shuffle=True, workers=WORKERS)\ntest_dataloader = create_dataloaders(dataset=test_dataset, batch=BATCH_SIZE, shuffle=False, workers=WORKERS)\n\nTrain Dataset Length: 5075 and Test Dataset length: 1691\nDataloaders Length: 317 for a batch size of: 16\nDataloaders Length: 105 for a batch size of: 16\n\n\n\n\nTraining Model for Full Dataset\n\nset_seed(42)\n\n# Intializing loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n# Training the model using the pytorch function \nNUM_EPOCHS = 10\nresults, total_train_time = model_train(epochs=NUM_EPOCHS,\n                                        model=model, \n                                        train_dataloader=train_dataloader, \n                                        test_dataloader=test_dataloader, \n                                        optimizer=optimizer, \n                                        loss_fn=loss_fn, \n                                        device=device, \n                                        num_classes=len(dataset.classes))\n\n\n\n\n\n\n\n\n\n\nEpoch: [1/10] | train_loss: 2.6020 | train_acc: 0.4720 | train_time: 4.8366 min | test loss: 1.8906 | test_acc: 0.6238 | test_time: 1.6950 min\nEpoch: [2/10] | train_loss: 1.8662 | train_acc: 0.6327 | train_time: 5.4221 min | test loss: 1.7076 | test_acc: 0.6607 | test_time: 1.5391 min\nEpoch: [3/10] | train_loss: 1.7319 | train_acc: 0.6642 | train_time: 4.8773 min | test loss: 1.6484 | test_acc: 0.6756 | test_time: 1.5405 min\nEpoch: [4/10] | train_loss: 1.6819 | train_acc: 0.6733 | train_time: 4.5695 min | test loss: 1.6199 | test_acc: 0.6887 | test_time: 1.5152 min\nEpoch: [5/10] | train_loss: 1.6426 | train_acc: 0.6936 | train_time: 4.3718 min | test loss: 1.6051 | test_acc: 0.6970 | test_time: 1.5019 min\nEpoch: [6/10] | train_loss: 1.6085 | train_acc: 0.6981 | train_time: 4.7581 min | test loss: 1.5919 | test_acc: 0.7000 | test_time: 1.5833 min\nEpoch: [7/10] | train_loss: 1.5967 | train_acc: 0.7049 | train_time: 4.9686 min | test loss: 1.5807 | test_acc: 0.7071 | test_time: 1.5994 min\nEpoch: [8/10] | train_loss: 1.5680 | train_acc: 0.7173 | train_time: 6.3247 min | test loss: 1.5763 | test_acc: 0.7137 | test_time: 1.5196 min\nEpoch: [9/10] | train_loss: 1.5577 | train_acc: 0.7179 | train_time: 5.6291 min | test loss: 1.5734 | test_acc: 0.7083 | test_time: 1.5351 min\nEpoch: [10/10] | train_loss: 1.5540 | train_acc: 0.7194 | train_time: 5.6172 min | test loss: 1.5605 | test_acc: 0.7185 | test_time: 1.5643 min\n\nTotal Model Training Time: 66.9688 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStoring Files for Demo App.\n\n# Creating a directory for the app\ndemo_path = 'demo'\ndemo_project_path = os.path.join(demo_path, 'human_action_recognition')\ndemo_example_path = os.path.join(demo_project_path, 'examples')\n\nif os.path.exists(demo_path):\n    print(f'[INFO] \"{demo_path}\" already exists.')\nelse:\n    os.mkdir(demo_path)\n    os.mkdir(demo_project_path)\n    os.mkdir(demo_example_path)\n    print(f'[INFO] \"{demo_project_path}\" and \"{demo_example_path}\" directory is been created.')\n\n[INFO] \"demo/human_action_recognition\" and \"demo/human_action_recognition/examples\" directory is been created.\n\n\n\n# Saving some example files\nexample_files = ['data/shoot_bow/6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_3.avi',\n                 'data/golf/Golf_Tips_-_Hit_The_Driver_300+_Yards!!!_golf_f_nm_np1_fr_med_0.avi',\n                 'data/climb/(HQ)_Rock_Climbing_-_Free_Solo_Speed_Climb_-_Dan_Osman_climb_f_cm_np1_le_med_2.avi',\n                 'data/punch/AdvancedBoxingTechniquesandExercises_punch_u_nm_np1_ba_med_23.avi',\n                 'data/swing_baseball/BaseballHitinSlowMotion_swing_baseball_f_nm_np1_fr_bad_0.avi']\n\nfor i in example_files:\n    # Copying the files\n    shutil.copy2(src=i, dst='demo/human_action_recognition/examples')\n    print(f'{i} File is been copied.')\n\ndata/shoot_bow/6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_3.avi File is been copied.\ndata/golf/Golf_Tips_-_Hit_The_Driver_300+_Yards!!!_golf_f_nm_np1_fr_med_0.avi File is been copied.\ndata/climb/(HQ)_Rock_Climbing_-_Free_Solo_Speed_Climb_-_Dan_Osman_climb_f_cm_np1_le_med_2.avi File is been copied.\ndata/punch/AdvancedBoxingTechniquesandExercises_punch_u_nm_np1_ba_med_23.avi File is been copied.\ndata/swing_baseball/BaseballHitinSlowMotion_swing_baseball_f_nm_np1_fr_bad_0.avi File is been copied.\n\n\n\n# Saving the classes names to text file\nclass_names_path = os.path.join(demo_project_path, 'class_names.txt')\n\nwith open(class_names_path, 'w') as f:\n    f.write('\\n'.join(dataset.classes))\n    print(f'[INFO] Saving class names to: \"{class_names_path}\"')\n\n[INFO] Saving class names to: \"demo/human_action_recognition/class_names.txt\"\n\n\n\n# Reading the text file\nwith open(class_names_path, 'r') as f:\n    class_names_read = [i.strip() for i in f.readlines()]\n\nclass_names_read[:5]\n\n['smoke', 'dive', 'cartwheel', 'throw', 'hit']\n\n\n\n\nSaving and Loading the Model\n\nmodel_save_name = 'mvit_v2_pretrained_model_hmdb51.pth'\nmodel_save_path = os.path.join(demo_project_path, model_save_name)\n\n\n# saving the model state\ntorch.save(obj=model.state_dict(), \n           f=model_save_path)\nprint(f'[INFO] Model is been saved to: \"{model_save_path}\"')\n\n[INFO] Model is been saved to: \"demo/human_action_recognition/mvit_v2_pretrained_model_hmdb51.pth\"\n\n\n\n# loading the model state\nmodel, transforms = create_model(num_classes=NUM_CLASSES, device=device)\nmodel.load_state_dict(torch.load(model_save_path))\n\n&lt;All keys matched successfully&gt;\n\n\n\n\nCreating a text file for Modules and Packages\n\n%%writefile demo/human_action_recognition/requirements.txt\nav==10.0.0\ntorch==2.0.0\ntorchvision==0.15.1\ntorchaudio==2.0.1\ngradio==3.24.1\n\nWriting demo/human_action_recognition/requirements.txt\n\n\n\n\nCreating a Python Script to Preprocess the Data\n\n%%writefile demo/human_action_recognition/utils.py\nimport torch\nimport torchvision\n\ndef preprocess_video(video: str):\n    \"\"\"\n    A function to preprocess the video file before going into the model.\n    Parameters: \n        video: str, A string for the video file path.\n    Returns: selected_frame: torch.Tensor, A tensor of shape 'TCHW'.\n    \"\"\"\n    # Reading the video file\n    vframes, _, _ = torchvision.io.read_video(filename=video, pts_unit='sec', output_format='TCHW')\n    vframes = vframes.type(torch.float32)\n    vframes_count = len(vframes)\n    \n    # Selecting frames at certain interval\n    skip_frames = max(int(vframes_count/16), 1)\n    \n    # Selecting the first frame\n    selected_frame = vframes[0].unsqueeze(0)\n    \n    # Creating a new sequence of frames upto the defined sequence length.\n    for i in range(1, 16):\n        selected_frame = torch.concat((selected_frame, vframes[i * skip_frames].unsqueeze(0)))\n    return selected_frame\n\nWriting demo/human_action_recognition/utils.py\n\n\n\n\nCreating a Python Script to Create the Model\n\n%%writefile demo/human_action_recognition/model.py\nimport torch\nimport torchvision\n\ndef create_model(num_classes: int, seed: int = 42):\n    \"\"\"\n    A function to create a model.\n    Parameters:\n        num_classes: int, A integer for toal number of classes.\n        seed: int(default: 42), A random seed value.\n    Returns: \n        model: A feature extracted model for video classification.\n        transforms: A torchvision transform is returned which was used in the pretrained model.    \n    \"\"\"\n    # Creating model, weights and transforms\n    weights = torchvision.models.video.MViT_V2_S_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.video.mvit_v2_s(weights=weights)\n    \n    # Freezing the model layers\n    for params in model.parameters():\n        params.requires_grad = False\n        \n    # Changing the fully Conncected head layer\n    torch.manual_seed(seed)\n    dropout_layer = model.head[0]\n    in_features = model.head[1].in_features\n    model.head = torch.nn.Sequential(\n        dropout_layer,\n        torch.nn.Linear(in_features=in_features, out_features=num_classes, bias=True)\n    )\n    return model, transforms\n\nWriting demo/human_action_recognition/model.py\n\n\n\n\nCreating a Python Script for Gradio App\n\n%%writefile demo/human_action_recognition/app.py\nimport gradio as gr\nimport os\nimport torch\nimport torchvision\nfrom timeit import default_timer as timer\nfrom model import create_model\nfrom utils import preprocess_video\n\n# Configuring the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Setting class names using the text file\nwith open('class_names.txt', 'r') as f:\n    class_names = [i.strip() for i in f.readlines()]\n\n# Setting the model and transforms\nmodel, transforms = create_model(num_classes=len(class_names), seed=42)\nmodel.load_state_dict(torch.load(f='mvit_v2_pretrained_model_hmdb51.pth', map_location=device))\n\n# Creating a function to predict the video\ndef predict(video_file):\n    \"\"\"\n    A function to predict the video using the model.\n    Parameters: \n        video_file: str, A video file path as a string.\n    Returns: \n        pred_labels_probs: A dict file containing the class names and confidence values.\n        pred_time: Time taken to predict in seconds.\n    \"\"\"\n    # Preprocessing the video file\n    frames = preprocess_video(video=video_file)\n    \n    # transforming the frames\n    mod_frames = transforms(frames).unsqueeze(dim=0)\n    \n    # Starting the timer and predicting using the model\n    start_time = timer()\n    model.eval()\n    \n    # forward pass\n    with torch.no_grad():\n        logits = model(mod_frames.to(device)) \n        pred_probs = torch.softmax(logits, dim=1).squeeze(dim=0)\n    \n    # Creating a dict with class names and their predicted probabilities\n    pred_labels_probs = {class_names[i]: float(pred_probs[i]) for i in range(len(class_names))}\n        \n    # Ending the timer and calculating the prediction time.\n    end_time = timer()\n    pred_time = round(end_time - start_time, 5)\n    \n    # Returning the labels and time for gradio output.\n    return pred_labels_probs, pred_time\n\n# Pre-information for interface\ntitle = 'Human Activity Recognition(HAR)'\ndescription = 'A Gradio demo web application for video classification using the MViT V2 Pretrained Model and trained on the HMDB51 Dataset.'\narticle = 'Created by John'\nexample_list = [['examples/'+ i] for i in os.listdir('examples')]\n\n# Building the Gradio interface\ndemo = gr.Interface(fn=predict,\n                    inputs=gr.Video(label='Video'),\n                    outputs=[gr.Label(num_top_classes=5, label='Predictions'),\n                             gr.Number(label='Prediction Time (sec)')],\n                    examples=example_list,\n                    title=title,\n                    description=description,\n                    article=article)\n\n# Launching the gradio interface\ndemo.launch()\n\nWriting demo/human_action_recognition/app.py\n\n\n\n\nDeploying on HuggingFace\n\nIFrame(src='https://hf.space/embed/JohnPinto/Human_Activity_Recognition-HAR-Video_Classification-HMDB51-Dataset',\n       width=1050,\n       height=700)"
  }
]