[
  {
    "objectID": "posts/2022-11-20_tf_modelling/index.html",
    "href": "posts/2022-11-20_tf_modelling/index.html",
    "title": "TensorFlow Modelling — Sequential Vs Functional",
    "section": "",
    "text": "“A computer system modeled on the human brain and nervous system.” You can google it and this is the answer you will get. It’s a simple answer but does not explain why it is used in Deep Learning.\n\n\n\n\n\nNeural Network also known as an Artificial Neural Network, mimics the same architecture of our human brain. It’s the core of any Deep Learning or Machine Learning Algorithm. It consists of three essential factors also called Nodes or Layers, that make up the neural network architecture. First, the Input Layer is the entry point for any data. After the data passes through the input layer it moves toward multiple Hidden Layers, this is where patterns are learned by the neural network using weights and biases. Once the network has reached its mathematical conclusion in the hidden layers it passes to the Output Layer.\nNow, that we understand a neural network, how do we practically use them? Here comes TensorFlow, an open-source Python library for Deep Learning. TensorFlow helps us in creating neural network models that are capable to solve different types of deep learning problems, due to the complexity of neural networks TensorFlow uses the Keras library to make it simple and easy to use.\nTensorflow and Keras provide us with two different methods to initiate and implement our neural network architecture, they are as follows:\n\nThe Sequential API Model\nThe Functional API Model\n\n\n\n\n\nFig 1: Sequential API Vs Functional API Architecture\n\n\n\n\n\nThe very first model that TensorFlow teaches us is the Sequential Model because it is the fastest and the simplest way to create a neural network. Now you might be wondering when to use a sequential model. You use it when you have one input and one output, just like everything moves in a sequence, one at a time.\nSequential Model Example:\nimport tensorflow as tf\n\n# Sequential Model \nmodel = tf.keras.Sequential(\n    [\n        # Input Layer\n        tf.keras.Input(shape=(224, 224, 3), name=\"input_layer\"),\n        # 4 Hidden Layers\n        tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_layer1\"),\n        tf.keras.layers.Dense(128, activation=\"relu\", name=\"hidden_layer2\"),\n        tf.keras.layers.Dense(64, activation=\"relu\", name=\"hidden_layer3\"),\n        tf.keras.layers.Dense(32, activation=\"relu\", name=\"hidden_layer4\"),\n        # Output Layer\n        tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"output_layer\")\n    ]\n)\n\n# The above code can also be written using add() method.\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(shape=(224, 224, 3), name=\"input_layer\"))\nmodel.add(tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_layer1\"))\nmodel.add(tf.keras.layers.Dense(128, activation=\"relu\", name=\"hidden_layer2\"))\nmodel.add(tf.keras.layers.Dense(64, activation=\"relu\", name=\"hidden_layer3\"))\nmodel.add(tf.keras.layers.Dense(32, activation=\"relu\", name=\"hidden_layer4\"))\nmodel.add(tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"output_layer\"))\n\n\n\n\nFig. 2: Model Architecture of the above Sequential Model example.\n\n\n\nIf you are seeing this for the first time you might get confused, and questions might arise like what shape, Dense, activation, relu, sigmoid, and multiple different numbers? Let’s understand everything line by line.\n\nThe first line initiates the sequential function within the name ‘model’. You can click and check the documentation for the sequential function.\nWithin the sequential model, there is a list with multiple layers. The first layer is the Input layer with a shape 224, 224, 3. This tells us that the data is an image array, here you can change the shape to whatever your data is been processed in.\nAfter the input layer, there are 4 fully dense hidden layers. In the sequential model, the output of the layer is the input for the next layer. Layers are basically Keras functions that apply certain mathematical calculations to learn patterns. In the hidden layer part, you can even add a sequential model as a layer, this way you can nest a model within a model.\nOnce we get an output from the last hidden layer, it goes through the output layer which is activated using sigmoid(another mathematical calculation for binary classification) this gives us an output result for our problem.\n\n\n\n\nKeras understood that the sequential model had some limitations like it couldn’t handle multiple inputs and outputs. It is not flexible enough to build a neural network that has shared layers or a non-linear branched topology. Although, sequential models were capable to produce great results. It was not capable enough to solve complex problems like object detection in computer vision, speech-to-text recognition, or vice versa, and many more. Finally, Keras came up with Functional API, here everything is isolated and combined however you see fit. A neural network that is now customizable layer-to-layer.\nFunctional API Example:\n# Taking similar example as above and making it more complex\n# importing packages\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, concatenate\n\n# Input Layer\ninputs = Input(shape=(224, 224, 3), name=\"input_layer\")\n\n# Two branches of hidden layers\n# First branch\nlayer_x1 = Dense(256, activation=\"relu\", name=\"hidden_layer1\")(inputs)\nlayer_x2 = Dense(128, activation=\"relu\", name=\"hidden_layer2\")(layer_x1)\n# Second branch\nlayer_y1 = Dense(64, activation=\"relu\", name=\"hidden_layer3\")(inputs)\nlayer_y2 = Dense(32, activation=\"relu\", name=\"hidden_layer4\")(layer_y1)\n# Concatenate layer\ncat_layer = concatenate([layer_x2, layer_y2])\n\n# Output Layer\noutputs = Dense(2, activation=\"sigmoid\", name=\"output_layer\")(cat_layer)\n\n# Defining the model\nmodel = Model(inputs = inputs, outputs = outputs)\n\n\n\n\nFig. 3: Model Architecture of the above Functional API example.\n\n\n\nNow, that you understand sequential architecture, the above functional API example should be easier for you. Most of the layers used in this example are similar to the sequential example. Just the way to represent the model has changed, but this change is powerful enough to build some of the famous pre-trained models(e.g. AlexNet, VGG, ResNet, Densenet, and many more). Let’s understand the changes that are happening in the functional API model.\n\nSimilar to the sequential model example, we initialize the model by importing the packages and creating our first layer — the input layer, but unlike the sequential model, we do not need to use any Keras function and can directly name the layers using a variable.\nNow the fun part starts, we have used 4 hidden layers separated into two different branches. In Functional API, layers communicate by letting them know which layer is connected. If you look at the hidden layer “layer_x1”, at the end of the Dense layer statement it mentions the layer connected i.e. the input layer. This way you can chain any layer with each other and forward pass the model.\nNow that we have created two branches but need the output from a single layer, we can use the concatenate layer. The last layer of the branches is the input for the concatenate layer which is finally connected with the output layer to give us the output probabilities.\nAll the layers are connected, so it is time to define the model, for this we will be using the Keras Model function. The Model function needs two Arguments inputs and outputs. It basically tells the start and end point and the flow of the model is created."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "All My Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nRiding the Waves - PyTorch and Lightning 2.0\n\n\n\n\n\n\n\nPyTorch\n\n\nLightning\n\n\nFabric\n\n\nAI\n\n\nVideo Classification\n\n\n\n\nIn this post, I will be talking about the new release by PyTorch and PyTorch Lightning. Also testing the potential of the new updates.\n\n\n\n\n\n\nApr 1, 2023\n\n\nJohn Pinto\n\n\n39 min\n\n\n\n\n\n\n  \n\n\n\n\nTensorFlow Modelling — Sequential Vs Functional\n\n\n\n\n\n\n\nTensorflow\n\n\nKeras\n\n\nAI\n\n\nNeural Network\n\n\nArchitecture\n\n\n\n\nIn this post, I introduce you to the Tensorflow neural network architectures that will help you in building AI Models.\n\n\n\n\n\n\nNov 20, 2022\n\n\nJohn Pinto\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-11-20_tf_modelling/index.html#the-sequential-api-model",
    "href": "posts/2022-11-20_tf_modelling/index.html#the-sequential-api-model",
    "title": "TensorFlow Modelling — Sequential Vs Functional",
    "section": "",
    "text": "The very first model that TensorFlow teaches us is the Sequential Model because it is the fastest and the simplest way to create a neural network. Now you might be wondering when to use a sequential model. You use it when you have one input and one output, just like everything moves in a sequence, one at a time.\nSequential Model Example:\nimport tensorflow as tf\n\n# Sequential Model \nmodel = tf.keras.Sequential(\n    [\n        # Input Layer\n        tf.keras.Input(shape=(224, 224, 3), name=\"input_layer\"),\n        # 4 Hidden Layers\n        tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_layer1\"),\n        tf.keras.layers.Dense(128, activation=\"relu\", name=\"hidden_layer2\"),\n        tf.keras.layers.Dense(64, activation=\"relu\", name=\"hidden_layer3\"),\n        tf.keras.layers.Dense(32, activation=\"relu\", name=\"hidden_layer4\"),\n        # Output Layer\n        tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"output_layer\")\n    ]\n)\n\n# The above code can also be written using add() method.\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(shape=(224, 224, 3), name=\"input_layer\"))\nmodel.add(tf.keras.layers.Dense(256, activation=\"relu\", name=\"hidden_layer1\"))\nmodel.add(tf.keras.layers.Dense(128, activation=\"relu\", name=\"hidden_layer2\"))\nmodel.add(tf.keras.layers.Dense(64, activation=\"relu\", name=\"hidden_layer3\"))\nmodel.add(tf.keras.layers.Dense(32, activation=\"relu\", name=\"hidden_layer4\"))\nmodel.add(tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"output_layer\"))\n\n\n\n\nFig. 2: Model Architecture of the above Sequential Model example.\n\n\n\nIf you are seeing this for the first time you might get confused, and questions might arise like what shape, Dense, activation, relu, sigmoid, and multiple different numbers? Let’s understand everything line by line.\n\nThe first line initiates the sequential function within the name ‘model’. You can click and check the documentation for the sequential function.\nWithin the sequential model, there is a list with multiple layers. The first layer is the Input layer with a shape 224, 224, 3. This tells us that the data is an image array, here you can change the shape to whatever your data is been processed in.\nAfter the input layer, there are 4 fully dense hidden layers. In the sequential model, the output of the layer is the input for the next layer. Layers are basically Keras functions that apply certain mathematical calculations to learn patterns. In the hidden layer part, you can even add a sequential model as a layer, this way you can nest a model within a model.\nOnce we get an output from the last hidden layer, it goes through the output layer which is activated using sigmoid(another mathematical calculation for binary classification) this gives us an output result for our problem."
  },
  {
    "objectID": "posts/2022-11-20_tf_modelling/index.html#the-functional-api-model",
    "href": "posts/2022-11-20_tf_modelling/index.html#the-functional-api-model",
    "title": "TensorFlow Modelling — Sequential Vs Functional",
    "section": "",
    "text": "Keras understood that the sequential model had some limitations like it couldn’t handle multiple inputs and outputs. It is not flexible enough to build a neural network that has shared layers or a non-linear branched topology. Although, sequential models were capable to produce great results. It was not capable enough to solve complex problems like object detection in computer vision, speech-to-text recognition, or vice versa, and many more. Finally, Keras came up with Functional API, here everything is isolated and combined however you see fit. A neural network that is now customizable layer-to-layer.\nFunctional API Example:\n# Taking similar example as above and making it more complex\n# importing packages\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, concatenate\n\n# Input Layer\ninputs = Input(shape=(224, 224, 3), name=\"input_layer\")\n\n# Two branches of hidden layers\n# First branch\nlayer_x1 = Dense(256, activation=\"relu\", name=\"hidden_layer1\")(inputs)\nlayer_x2 = Dense(128, activation=\"relu\", name=\"hidden_layer2\")(layer_x1)\n# Second branch\nlayer_y1 = Dense(64, activation=\"relu\", name=\"hidden_layer3\")(inputs)\nlayer_y2 = Dense(32, activation=\"relu\", name=\"hidden_layer4\")(layer_y1)\n# Concatenate layer\ncat_layer = concatenate([layer_x2, layer_y2])\n\n# Output Layer\noutputs = Dense(2, activation=\"sigmoid\", name=\"output_layer\")(cat_layer)\n\n# Defining the model\nmodel = Model(inputs = inputs, outputs = outputs)\n\n\n\n\nFig. 3: Model Architecture of the above Functional API example.\n\n\n\nNow, that you understand sequential architecture, the above functional API example should be easier for you. Most of the layers used in this example are similar to the sequential example. Just the way to represent the model has changed, but this change is powerful enough to build some of the famous pre-trained models(e.g. AlexNet, VGG, ResNet, Densenet, and many more). Let’s understand the changes that are happening in the functional API model.\n\nSimilar to the sequential model example, we initialize the model by importing the packages and creating our first layer — the input layer, but unlike the sequential model, we do not need to use any Keras function and can directly name the layers using a variable.\nNow the fun part starts, we have used 4 hidden layers separated into two different branches. In Functional API, layers communicate by letting them know which layer is connected. If you look at the hidden layer “layer_x1”, at the end of the Dense layer statement it mentions the layer connected i.e. the input layer. This way you can chain any layer with each other and forward pass the model.\nNow that we have created two branches but need the output from a single layer, we can use the concatenate layer. The last layer of the branches is the input for the concatenate layer which is finally connected with the output layer to give us the output probabilities.\nAll the layers are connected, so it is time to define the model, for this we will be using the Keras Model function. The Model function needs two Arguments inputs and outputs. It basically tells the start and end point and the flow of the model is created."
  },
  {
    "objectID": "posts/2022-11-20_tf_modelling/index.html#closing",
    "href": "posts/2022-11-20_tf_modelling/index.html#closing",
    "title": "TensorFlow Modelling — Sequential Vs Functional",
    "section": "",
    "text": "We have covered both the Sequential and Functional Model architecture in this article. Experimented with both and understood the pros and cons, with this you are now capable enough to create your neural network. By the way, there’s a third method that I haven’t mentioned in this article. It’s called the Model subclassing method. I do not recommend this for a beginner to begin their journey in deep learning, it is much more complex and difficult to understand however powerful enough to create your own custom layers and models.\nI hope that you enjoyed this article and that it serves you well. Please reach out to me or you can comment down if you run into any trouble with what I wrote. I am more than happy to improve my article so that it can help more people."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello and Welcome to My Blog!",
    "section": "",
    "text": "Hello and Welcome to My Blog!\n\n\n\n\n\n\n\nBlog\n\n\nI enjoy exploring Data Analytics and Machine Learning and picking up new  tricks on my way. I share what I discover and learn.\n\nStart reading\n\n\n\n\n\n\n\nProjects\n\n\nA collection of my ongoing personal data-endeavors and programming projects.\n\n Take me there"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "John Pinto",
    "section": "",
    "text": "👋 Hey there!\n\nI’m John Pinto. I’m a Management and I.T. graduate who is self-motivated and passionate about the field of Data Analytics, Data Visualization and Data Science. I have a great interest in open and reproducible research and deveploment in machine learning and deep learning using open and reproducible tools like Python and R. This is my personal blog where I share on topics that I learn and explore."
  },
  {
    "objectID": "about.html#fa-solid-address-card-about",
    "href": "about.html#fa-solid-address-card-about",
    "title": "John Pinto",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html",
    "title": "Riding the 2.0 Waves, PyTorch and Lightning 2.0",
    "section": "",
    "text": "Quick Facts on PyTorch and Lightning 2.0 Release\nPyTorch and Lightning were released on 15 March, 2023. The main focus being the improvement in the speed, even the release article title says it\n\n“PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever”\n\nyou can read it over here.\nThis speed that they mention comes with just a single line of code. After you have created your model you just use this code and the code modifies your model to perform at it’s best level.\ntorch.compile()\nThey have even mentioned the details while testing the new functionality and how the torch.compile() can improve the speed of the models from different sources [HuggingFace, TIMM and TorchBench].\nTo run this amazing compiled model, PyTorch introduces new technologies - TorchDynamo, AOTAutograd, PrimTorch and TorchInductor. All of this new technologies are working in a flow and they are broken down in three phases - Graph Acquisition, Graph Lowering and Graph Compilation. You can check all of this over here, they have explained complex system in easy to understand way.\n\n\n\n\n\n\nTip\n\n\n\nRead the above linked article, especially the section on technology overview, this will help you understand the hyper-parameters within the torch.compile()."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#now-the-caveats-part-of-the-release",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#now-the-caveats-part-of-the-release",
    "title": "Riding the 2.0 Waves, PyTorch and Lightning 2.0",
    "section": "Now the Caveats Part of the Release",
    "text": "Now the Caveats Part of the Release\nPyTorch and PyTorch Lightning 2.0 are the stable release but there are some information that needs attention.\n\nHardware:"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#caveats-part-of-the-release",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#caveats-part-of-the-release",
    "title": "Riding the Waves - PyTorch and Lightning 2.0",
    "section": "Caveats Part of the Release",
    "text": "Caveats Part of the Release\nPyTorch and PyTorch Lightning 2.0 are the stable release but there are some information that needs attention.\n\nHardware: This speed up performance that the PyTorch team speaks of are based on specific hardware, broadly they have mentioned that NVIDIA Volta and Ampere server class GPUs are capable to produce decent results. So desktop GPUs will need to wait for later releases.\nModel Saving/Exporting: Right now the compile model can only be saved using the model.state_dict() method. You won’t me able to save the object of the model, which returns an error if you try to. You can read the serialization part of the article that I have mentioned above. Along with the save part, team will also introduce torch.export() mode in the later release."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#importing-libraries",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#importing-libraries",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\nTorch version: 2.0.0+cu117\nTorchvision version: 0.15.1+cu117\nLightning version: 2.0.1\nGPU score: (8, 6)\n\n\n\n\ndevice(type='cuda')"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#downloading-the-dataset",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#downloading-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Downloading the Dataset",
    "text": "Downloading the Dataset\nDownloading the data from the dataset origin.\n\n\n[INFO] Downloading data.\n[INFO] Data is been downloaded!\n\n\nExtracting the data in the raw_data directory\n\n\n[INFO] Extracting data.\n[INFO] Data extraction done!\n\n\nExtracting all the raw data in the data directory and deleting the raw data from the system..\n\n\n[INFO] Extracting all the classes data.\n[INFO] Data extraction done for: \"raw_data/clap.rar\"!\n[INFO] Data extraction done for: \"raw_data/talk.rar\"!\n[INFO] Data extraction done for: \"raw_data/smoke.rar\"!\n[INFO] Data extraction done for: \"raw_data/draw_sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/flic_flac.rar\"!\n[INFO] Data extraction done for: \"raw_data/dive.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_bow.rar\"!\n[INFO] Data extraction done for: \"raw_data/drink.rar\"!\n[INFO] Data extraction done for: \"raw_data/hug.rar\"!\n[INFO] Data extraction done for: \"raw_data/pour.rar\"!\n[INFO] Data extraction done for: \"raw_data/pushup.rar\"!\n[INFO] Data extraction done for: \"raw_data/hit.rar\"!\n[INFO] Data extraction done for: \"raw_data/wave.rar\"!\n[INFO] Data extraction done for: \"raw_data/run.rar\"!\n[INFO] Data extraction done for: \"raw_data/walk.rar\"!\n[INFO] Data extraction done for: \"raw_data/fall_floor.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_horse.rar\"!\n[INFO] Data extraction done for: \"raw_data/turn.rar\"!\n[INFO] Data extraction done for: \"raw_data/situp.rar\"!\n[INFO] Data extraction done for: \"raw_data/jump.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/dribble.rar\"!\n[INFO] Data extraction done for: \"raw_data/sit.rar\"!\n[INFO] Data extraction done for: \"raw_data/chew.rar\"!\n[INFO] Data extraction done for: \"raw_data/pick.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick.rar\"!\n[INFO] Data extraction done for: \"raw_data/eat.rar\"!\n[INFO] Data extraction done for: \"raw_data/fencing.rar\"!\n[INFO] Data extraction done for: \"raw_data/swing_baseball.rar\"!\n[INFO] Data extraction done for: \"raw_data/stand.rar\"!\n[INFO] Data extraction done for: \"raw_data/handstand.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb.rar\"!\n[INFO] Data extraction done for: \"raw_data/kiss.rar\"!\n[INFO] Data extraction done for: \"raw_data/kick_ball.rar\"!\n[INFO] Data extraction done for: \"raw_data/push.rar\"!\n[INFO] Data extraction done for: \"raw_data/throw.rar\"!\n[INFO] Data extraction done for: \"raw_data/cartwheel.rar\"!\n[INFO] Data extraction done for: \"raw_data/pullup.rar\"!\n[INFO] Data extraction done for: \"raw_data/brush_hair.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword.rar\"!\n[INFO] Data extraction done for: \"raw_data/climb_stairs.rar\"!\n[INFO] Data extraction done for: \"raw_data/laugh.rar\"!\n[INFO] Data extraction done for: \"raw_data/punch.rar\"!\n[INFO] Data extraction done for: \"raw_data/ride_bike.rar\"!\n[INFO] Data extraction done for: \"raw_data/sword_exercise.rar\"!\n[INFO] Data extraction done for: \"raw_data/somersault.rar\"!\n[INFO] Data extraction done for: \"raw_data/shoot_gun.rar\"!\n[INFO] Data extraction done for: \"raw_data/catch.rar\"!\n[INFO] Data extraction done for: \"raw_data/smile.rar\"!\n[INFO] Data extraction done for: \"raw_data/golf.rar\"!\n[INFO] Data extraction done for: \"raw_data/shake_hands.rar\"!\n\n[INFO] Deleted raw data.\n\n\nChecking some info on the data that is been extracted.\n\n\nThere are 51 directories and 0 files in \"data\".\nThere are 0 directories and 143 files in \"data/swing_baseball\".\nThere are 0 directories and 232 files in \"data/run\".\nThere are 0 directories and 105 files in \"data/situp\".\nThere are 0 directories and 162 files in \"data/shake_hands\".\nThere are 0 directories and 107 files in \"data/flic_flac\".\nThere are 0 directories and 127 files in \"data/hit\".\nThere are 0 directories and 102 files in \"data/kiss\".\nThere are 0 directories and 120 files in \"data/talk\".\nThere are 0 directories and 164 files in \"data/drink\".\nThere are 0 directories and 104 files in \"data/wave\".\nThere are 0 directories and 140 files in \"data/somersault\".\nThere are 0 directories and 116 files in \"data/push\".\nThere are 0 directories and 116 files in \"data/ride_horse\".\nThere are 0 directories and 128 files in \"data/kick_ball\".\nThere are 0 directories and 105 files in \"data/golf\".\nThere are 0 directories and 112 files in \"data/shoot_bow\".\nThere are 0 directories and 116 files in \"data/fencing\".\nThere are 0 directories and 102 files in \"data/smile\".\nThere are 0 directories and 145 files in \"data/dribble\".\nThere are 0 directories and 154 files in \"data/stand\".\nThere are 0 directories and 130 files in \"data/clap\".\nThere are 0 directories and 548 files in \"data/walk\".\nThere are 0 directories and 102 files in \"data/throw\".\nThere are 0 directories and 151 files in \"data/jump\".\nThere are 0 directories and 103 files in \"data/shoot_gun\".\nThere are 0 directories and 131 files in \"data/shoot_ball\".\nThere are 0 directories and 102 files in \"data/catch\".\nThere are 0 directories and 103 files in \"data/draw_sword\".\nThere are 0 directories and 142 files in \"data/sit\".\nThere are 0 directories and 127 files in \"data/sword_exercise\".\nThere are 0 directories and 108 files in \"data/climb\".\nThere are 0 directories and 104 files in \"data/pullup\".\nThere are 0 directories and 126 files in \"data/punch\".\nThere are 0 directories and 103 files in \"data/pushup\".\nThere are 0 directories and 127 files in \"data/sword\".\nThere are 0 directories and 108 files in \"data/eat\".\nThere are 0 directories and 109 files in \"data/chew\".\nThere are 0 directories and 128 files in \"data/laugh\".\nThere are 0 directories and 103 files in \"data/ride_bike\".\nThere are 0 directories and 118 files in \"data/hug\".\nThere are 0 directories and 107 files in \"data/cartwheel\".\nThere are 0 directories and 106 files in \"data/pick\".\nThere are 0 directories and 130 files in \"data/kick\".\nThere are 0 directories and 113 files in \"data/handstand\".\nThere are 0 directories and 240 files in \"data/turn\".\nThere are 0 directories and 112 files in \"data/climb_stairs\".\nThere are 0 directories and 127 files in \"data/dive\".\nThere are 0 directories and 107 files in \"data/brush_hair\".\nThere are 0 directories and 106 files in \"data/pour\".\nThere are 0 directories and 109 files in \"data/smoke\".\nThere are 0 directories and 136 files in \"data/fall_floor\"."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#visualizing-the-dataset",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#visualizing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Visualizing the dataset",
    "text": "Visualizing the dataset\nNow that the data is downloaded, lets view the data with the labels."
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#preprocessing-the-dataset",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#preprocessing-the-dataset",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Preprocessing the Dataset",
    "text": "Preprocessing the Dataset\nCreating pytorch Dataset.\nIn this steps we will be creating a custom dataset class using the functionality provided by PyTorch. 1. Read the video file. 2. Extracting the frames after every distance for a max sequence length. 3. Assigning label index for all the classes. 4. Performing torchvision.transform on the video data. 5. Returning tuple of video and label data as a tensor.\nBefore creating the dataset, first lets clean it. Here we will check whether the videos are readable and the frame count needs to be more than the Sequence Length frames.\n\n\n[]\n\n\n\n\nThere are no files to delete.\n\n\n\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n         [ 2.,  2.,  0.,  ..., 11., 11., 11.],\n         ...,\n         [ 2.,  2.,  0.,  ..., 55., 55., 55.],\n         [ 5.,  5.,  3.,  ..., 62., 62., 62.],\n         [ 5.,  5.,  2.,  ..., 34., 34., 34.]],\n\n        [[ 4.,  4.,  5.,  ...,  2.,  1.,  1.],\n         [ 5.,  5.,  5.,  ...,  3.,  3.,  3.],\n         [ 4.,  4.,  5.,  ..., 17., 17., 17.],\n         ...,\n         [ 2.,  2.,  2.,  ..., 41., 41., 41.],\n         [ 0.,  0.,  1.,  ..., 49., 49., 49.],\n         [ 0.,  0.,  0.,  ..., 21., 21., 21.]],\n\n        [[ 3.,  3.,  4.,  ...,  2.,  1.,  1.],\n         [ 4.,  4.,  4.,  ...,  3.,  3.,  3.],\n         [ 1.,  1.,  1.,  ...,  8.,  8.,  8.],\n         ...,\n         [ 0.,  0.,  0.,  ..., 16., 14., 14.],\n         [ 0.,  0.,  0.,  ..., 28., 28., 28.],\n         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([16, 3, 240, 320]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#creating-a-pretrained-model",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#creating-a-pretrained-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating A Pretrained Model",
    "text": "Creating A Pretrained Model\n\n\n==================================================================================================================================\nLayer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n==================================================================================================================================\nMViT (MViT)                                        [1, 3, 16, 224, 224] [1, 20]              --                   Partial\n├─Conv3d (conv_proj)                               [1, 3, 16, 224, 224] [1, 96, 8, 56, 56]   (42,432)             False\n├─PositionalEncoding (pos_encoding)                [1, 25088, 96]       [1, 25089, 96]       (96)                 False\n├─ModuleList (blocks)                              --                   --                   --                   False\n│    └─MultiscaleBlock (0)                         [1, 25089, 96]       [1, 25089, 96]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 25089, 96]       (68,352)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MLP (mlp)                              [1, 25089, 96]       [1, 25089, 96]       (74,208)             False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 25089, 96]       [1, 25089, 96]       --                   --\n│    └─MultiscaleBlock (1)                         [1, 25089, 96]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 25089, 96]       [1, 25089, 96]       (192)                False\n│    │    └─MultiscaleAttention (attn)             [1, 25089, 96]       [1, 6273, 192]       (113,280)            False\n│    │    └─Linear (project)                       [1, 25089, 96]       [1, 25089, 192]      (18,624)             False\n│    │    └─Pool (pool_skip)                       [1, 25089, 192]      [1, 6273, 192]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (2)                         [1, 6273, 192]       [1, 6273, 192]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 6273, 192]       (168,576)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MLP (mlp)                              [1, 6273, 192]       [1, 6273, 192]       (295,872)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 6273, 192]       [1, 6273, 192]       --                   --\n│    └─MultiscaleBlock (3)                         [1, 6273, 192]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 6273, 192]       [1, 6273, 192]       (384)                False\n│    │    └─MultiscaleAttention (attn)             [1, 6273, 192]       [1, 1569, 384]       (385,152)            False\n│    │    └─Linear (project)                       [1, 6273, 192]       [1, 6273, 384]       (74,112)             False\n│    │    └─Pool (pool_skip)                       [1, 6273, 384]       [1, 1569, 384]       --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (4)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (5)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (6)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (7)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (8)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (9)                         [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (10)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (11)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (12)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (13)                        [1, 1569, 384]       [1, 1569, 384]       --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 1569, 384]       (606,336)            False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    │    └─LayerNorm (norm2)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MLP (mlp)                              [1, 1569, 384]       [1, 1569, 384]       (1,181,568)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 1569, 384]       [1, 1569, 384]       --                   --\n│    └─MultiscaleBlock (14)                        [1, 1569, 384]       [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 1569, 384]       [1, 1569, 384]       (768)                False\n│    │    └─MultiscaleAttention (attn)             [1, 1569, 384]       [1, 393, 768]        (1,492,608)          False\n│    │    └─Linear (project)                       [1, 1569, 384]       [1, 1569, 768]       (295,680)            False\n│    │    └─Pool (pool_skip)                       [1, 1569, 768]       [1, 393, 768]        --                   --\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    └─MultiscaleBlock (15)                        [1, 393, 768]        [1, 393, 768]        --                   False\n│    │    └─LayerNorm (norm1)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MultiscaleAttention (attn)             [1, 393, 768]        [1, 393, 768]        (2,374,656)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n│    │    └─LayerNorm (norm2)                      [1, 393, 768]        [1, 393, 768]        (1,536)              False\n│    │    └─MLP (mlp)                              [1, 393, 768]        [1, 393, 768]        (4,722,432)          False\n│    │    └─StochasticDepth (stochastic_depth)     [1, 393, 768]        [1, 393, 768]        --                   --\n├─LayerNorm (norm)                                 [1, 393, 768]        [1, 393, 768]        (1,536)              False\n├─Sequential (head)                                [1, 768]             [1, 20]              --                   True\n│    └─Dropout (0)                                 [1, 768]             [1, 768]             --                   --\n│    └─Linear (1)                                  [1, 768]             [1, 20]              15,380               True\n==================================================================================================================================\nTotal params: 34,245,524\nTrainable params: 15,380\nNon-trainable params: 34,230,144\nTotal mult-adds (G): 1.64\n==================================================================================================================================\nInput size (MB): 9.63\nForward/backward pass size (MB): 1658.93\nParams size (MB): 136.46\nEstimated Total Size (MB): 1805.02\n==================================================================================================================================\n\n\n\n\nPretrained Model Transforms:\nVideoClassification(\n    crop_size=[224, 224]\n    resize_size=[256]\n    mean=[0.45, 0.45, 0.45]\n    std=[0.225, 0.225, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\nLength of the dataset: 2530\nVideo tensor(first frame):\ntensor([[[ 2.4604e+02,  2.3502e+02,  2.2911e+02,  ...,  2.8675e+02,\n           2.8675e+02,  2.8684e+02],\n         [ 2.3070e+02,  2.2561e+02,  2.1813e+02,  ...,  3.1508e+02,\n           3.1508e+02,  3.1508e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1425e+02,\n           3.1425e+02,  3.1425e+02],\n         ...,\n         [ 3.2401e+00,  2.9817e+00, -2.0000e+00,  ...,  1.6333e+02,\n           1.2992e+02,  9.1900e+01],\n         [-1.5833e+00, -1.7232e+00, -2.0000e+00,  ...,  4.4867e+02,\n           3.7443e+02,  2.8330e+02],\n         [ 2.4444e+00,  9.5210e-01, -2.0000e+00,  ...,  6.6754e+02,\n           6.3805e+02,  5.6134e+02]],\n\n        [[ 2.2467e+02,  2.2616e+02,  2.2911e+02,  ...,  3.0883e+02,\n           3.0883e+02,  3.0902e+02],\n         [ 2.2869e+02,  2.2478e+02,  2.1813e+02,  ...,  3.1314e+02,\n           3.1314e+02,  3.1581e+02],\n         [ 2.3060e+02,  2.2712e+02,  2.1802e+02,  ...,  3.1356e+02,\n           3.1356e+02,  3.1402e+02],\n         ...,\n         [ 9.1056e+00,  6.2105e+00, -2.0000e+00,  ...,  9.9700e+01,\n           7.6676e+01,  2.8844e+01],\n         [ 1.9746e+01,  1.7561e+01, -1.1667e+00,  ...,  2.1729e+02,\n           1.6955e+02,  1.1592e+02],\n         [ 6.6111e+00,  6.6111e+00,  6.6111e+00,  ...,  5.4941e+02,\n           5.2508e+02,  4.3940e+02]],\n\n        [[-2.0000e+00,  5.3684e+00,  2.1125e+01,  ...,  3.2244e+02,\n           3.1922e+02,  3.2077e+02],\n         [-3.9952e-01,  4.0798e+00,  9.3575e+00,  ...,  3.2330e+02,\n           3.1922e+02,  3.1025e+02],\n         [ 2.0259e+00,  2.6776e+00,  5.3857e+00,  ...,  3.3464e+02,\n           3.3047e+02,  3.1790e+02],\n         ...,\n         [ 1.7578e+02,  1.7578e+02,  1.7719e+02,  ...,  2.3105e+02,\n           2.2658e+02,  2.1946e+02],\n         [ 1.7803e+02,  1.7884e+02,  1.8227e+02,  ...,  2.3562e+02,\n           2.2931e+02,  2.2306e+02],\n         [ 1.9990e+02,  2.0851e+02,  2.2140e+02,  ...,  2.2817e+02,\n           2.1964e+02,  2.2187e+02]],\n\n        ...,\n\n        [[ 4.2491e+02,  4.3142e+02,  4.2619e+02,  ...,  1.5639e+01,\n           1.5639e+01,  1.5639e+01],\n         [ 4.0218e+02,  4.0793e+02,  4.1056e+02,  ...,  6.0095e+00,\n           7.7222e+00,  2.3716e+00],\n         [ 4.0126e+02,  4.0792e+02,  4.0992e+02,  ...,  8.7490e+00,\n           1.0639e+01,  2.2439e+00],\n         ...,\n         [ 1.2787e+02,  1.2839e+02,  1.3253e+02,  ...,  9.5269e-01,\n           1.7500e+00,  1.7500e+00],\n         [ 1.3349e+02,  1.2883e+02,  1.3904e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 1.3694e+02,  1.3320e+02,  1.3960e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.5630e+02,  4.6579e+02,  4.6640e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3329e+02,  4.3661e+02,  4.3776e+02,  ...,  1.1714e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4439e+02,  4.4481e+02,  4.4500e+02,  ...,  1.4995e+00,\n          -7.8133e-01, -2.0000e+00],\n         ...,\n         [ 1.4467e+02,  1.5315e+02,  1.7425e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.3056e+00],\n         [ 1.4625e+02,  1.5174e+02,  1.6260e+02,  ..., -2.0000e+00,\n           9.2336e-01,  2.0278e+00],\n         [ 1.6148e+02,  1.6227e+02,  1.5224e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00]],\n\n        [[ 4.2527e+02,  4.4359e+02,  4.6669e+02,  ..., -2.0000e+00,\n          -2.0000e+00, -2.0000e+00],\n         [ 4.3037e+02,  4.3994e+02,  4.5392e+02,  ...,  2.0278e+00,\n          -8.9558e-01, -2.0000e+00],\n         [ 4.4064e+02,  4.4451e+02,  4.5152e+02,  ...,  2.4444e+00,\n          -7.8133e-01,  4.9081e-01],\n         ...,\n         [ 1.4709e+02,  1.5459e+02,  1.5781e+02,  ..., -2.0000e+00,\n          -1.4960e+00, -1.7668e+00],\n         [ 1.4016e+02,  1.5057e+02,  1.5565e+02,  ..., -1.5833e+00,\n           1.3400e+00,  4.5890e-02],\n         [ 1.3139e+02,  1.3590e+02,  1.4467e+02,  ...,  2.4444e+00,\n           2.4444e+00,  5.3965e+00]]])\nlabel tensor: 16\nShape of the video tensor: torch.Size([3, 16, 224, 224]) and shape of the label: torch.Size([])"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#creating-dataloaders",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#creating-dataloaders",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Creating DataLoaders",
    "text": "Creating DataLoaders\n\n\nTrain Dataset Length: 1898 and Test Dataset length: 632\n\n\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\n\n\nLabel Distribution and Zero Rule Benchmark\n\n\nTraining Dataset Distribution:\n [(0, 78), (1, 92), (2, 83), (3, 79), (4, 94), (5, 72), (6, 77), (7, 79), (8, 85), (9, 103), (10, 90), (11, 106), (12, 88), (13, 78), (14, 180), (15, 70), (16, 87), (17, 100), (18, 171), (19, 76)]\n\n\n\n\nTesting Dataset Distribution:\n [(0, 30), (1, 32), (2, 24), (3, 23), (4, 29), (5, 32), (6, 30), (7, 24), (8, 27), (9, 27), (10, 17), (11, 35), (12, 37), (13, 24), (14, 58), (15, 33), (16, 29), (17, 26), (18, 60), (19, 27)]\n\n\n\n\nThe Major class in Testing DataLoader is: Class Name: hug, Class Index: 18 and Total Number of Data: 60\nZero Rule Basline Benchmark Accuracy for predicting the major class: 9.62%"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#pytorch---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#pytorch---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch - Model Training and Testing",
    "text": "PyTorch - Model Training and Testing\n\nExp 1: PyTorch Before 2.0 (PyTorch Eager Mode)\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.4238 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5183 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.4201 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5181 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.3959 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.5176 min\n\nTotal Model Training Time: 5.7940 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[INFO] results directory is been created.\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.423781\n0.518263\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.420147\n0.518100\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.395912\n0.517590\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n\n\n\n\n\n\n\nExp 2: PyTorch 2.0 (PyTorch Compile Mode)\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.3885 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0454 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.6187 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5678 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.6008 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5694 min\n\nTotal Model Training Time: 7.7908 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.388515\n1.045425\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.618742\n0.567790\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.600783\n0.569392\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#pytorch-lightning---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#pytorch-lightning---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "PyTorch Lightning - Model Training and Testing",
    "text": "PyTorch Lightning - Model Training and Testing\n\nExp 3: PyTorch Lightning\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 7.6887 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n\n\n\n\n\n\n\nExp 4: PyTorch Lightning - Compile Mode\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nTotal Time to train the model: 10.5425 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#lightning-fabric---model-training-and-testing",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#lightning-fabric---model-training-and-testing",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Lightning Fabric - Model training and Testing",
    "text": "Lightning Fabric - Model training and Testing\n\nExp 5: Fabric Lightning\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5801 min | test loss: 2.6782 | test_acc: 0.4535 | test_time: 0.5829 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4163 | train_time: 1.5967 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5939 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5900 min | test loss: 2.2095 | test_acc: 0.5801 | test_time: 0.6074 min\n\nTotal Model Training Time: 6.5512 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853498\n0.198623\n2.678158\n0.453526\n1.580098\n0.582881\n\n\n1\n2.599148\n0.416314\n2.427746\n0.540064\n1.596745\n0.593947\n\n\n2\n2.360210\n0.523835\n2.209470\n0.580128\n1.590004\n0.607377\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n\n\n\n\n\n\n\nExp 6: Fabric Lightning - Compile mode\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 2.1276 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.0502 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.7023 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5721 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5101 | train_time: 1.5845 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5688 min\n\nTotal Model Training Time: 7.6056 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845071\n0.201271\n2.678697\n0.450321\n2.127585\n1.050178\n\n\n1\n2.590763\n0.445975\n2.427013\n0.548077\n1.702294\n0.572100\n\n\n2\n2.366281\n0.510064\n2.209109\n0.580128\n1.584458\n0.568837\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n\n\n\n\n\n\n\nExp 7: Fabric + Precision\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8535 | train_acc: 0.1986 | train_time: 1.5438 min | test loss: 2.6781 | test_acc: 0.4519 | test_time: 0.5552 min\nEpoch: [2/3] | train_loss: 2.5991 | train_acc: 0.4168 | train_time: 1.5081 min | test loss: 2.4277 | test_acc: 0.5401 | test_time: 0.5554 min\nEpoch: [3/3] | train_loss: 2.3602 | train_acc: 0.5238 | train_time: 1.5786 min | test loss: 2.2094 | test_acc: 0.5817 | test_time: 0.5744 min\n\nTotal Model Training Time: 6.3157 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.853492\n0.198623\n2.678145\n0.451923\n1.543753\n0.555179\n\n\n1\n2.599140\n0.416843\n2.427695\n0.540064\n1.508068\n0.555399\n\n\n2\n2.360222\n0.523835\n2.209447\n0.581731\n1.578585\n0.574411\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n\n\n\n\n\n\n\nExp 8: Fabric + Precision + Compile\n\n\nDataloaders Length: 118 for a batch size of: 16\nDataloaders Length: 39 for a batch size of: 16\nEpoch: [1/3] | train_loss: 2.8451 | train_acc: 0.2013 | train_time: 3.0326 min | test loss: 2.6787 | test_acc: 0.4503 | test_time: 1.1394 min\nEpoch: [2/3] | train_loss: 2.5908 | train_acc: 0.4460 | train_time: 1.5740 min | test loss: 2.4270 | test_acc: 0.5481 | test_time: 0.5764 min\nEpoch: [3/3] | train_loss: 2.3663 | train_acc: 0.5095 | train_time: 1.5562 min | test loss: 2.2091 | test_acc: 0.5801 | test_time: 0.5639 min\n\nTotal Model Training Time: 8.4427 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\ntrain_epoch_time(min)\ntest_epoch_time(min)\n\n\n\n\n0\n2.845062\n0.201271\n2.678693\n0.450321\n3.032648\n1.139419\n\n\n1\n2.590762\n0.445975\n2.427031\n0.548077\n1.574046\n0.576357\n\n\n2\n2.366282\n0.509534\n2.209138\n0.580128\n1.556243\n0.563871\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ntraining_time(min)\n\n\n\n\n0\nPytorch Eager\n5.794009\n\n\n1\nPytorch Compile\n7.790816\n\n\n2\nPytorch Lightning\n7.688747\n\n\n3\nPyTorch Lightning + Compile\n10.542457\n\n\n4\nFabric Lightning\n6.551208\n\n\n5\nFabric + Compile\n7.605610\n\n\n6\nFabric + Precision\n6.315720\n\n\n7\nFabric + Precision + Compile\n8.442741"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#training-and-testing-on-full-dataset-and-deploying-the-model",
    "title": "HMDB51 - Human Action Recognition using Pytorch",
    "section": "Training and Testing on Full Dataset and Deploying the Model",
    "text": "Training and Testing on Full Dataset and Deploying the Model\n\nCreating the model\n\n\nGetting the Full Dataset and Dataloaders\n\n\nTrain Dataset Length: 5075 and Test Dataset length: 1691\nDataloaders Length: 317 for a batch size of: 16\nDataloaders Length: 105 for a batch size of: 16\n\n\n\n\nTraining Model for Full Dataset\n\n\n\n\n\n\n\n\n\n\n\nEpoch: [1/10] | train_loss: 2.6020 | train_acc: 0.4720 | train_time: 4.8366 min | test loss: 1.8906 | test_acc: 0.6238 | test_time: 1.6950 min\nEpoch: [2/10] | train_loss: 1.8662 | train_acc: 0.6327 | train_time: 5.4221 min | test loss: 1.7076 | test_acc: 0.6607 | test_time: 1.5391 min\nEpoch: [3/10] | train_loss: 1.7319 | train_acc: 0.6642 | train_time: 4.8773 min | test loss: 1.6484 | test_acc: 0.6756 | test_time: 1.5405 min\nEpoch: [4/10] | train_loss: 1.6819 | train_acc: 0.6733 | train_time: 4.5695 min | test loss: 1.6199 | test_acc: 0.6887 | test_time: 1.5152 min\nEpoch: [5/10] | train_loss: 1.6426 | train_acc: 0.6936 | train_time: 4.3718 min | test loss: 1.6051 | test_acc: 0.6970 | test_time: 1.5019 min\nEpoch: [6/10] | train_loss: 1.6085 | train_acc: 0.6981 | train_time: 4.7581 min | test loss: 1.5919 | test_acc: 0.7000 | test_time: 1.5833 min\nEpoch: [7/10] | train_loss: 1.5967 | train_acc: 0.7049 | train_time: 4.9686 min | test loss: 1.5807 | test_acc: 0.7071 | test_time: 1.5994 min\nEpoch: [8/10] | train_loss: 1.5680 | train_acc: 0.7173 | train_time: 6.3247 min | test loss: 1.5763 | test_acc: 0.7137 | test_time: 1.5196 min\nEpoch: [9/10] | train_loss: 1.5577 | train_acc: 0.7179 | train_time: 5.6291 min | test loss: 1.5734 | test_acc: 0.7083 | test_time: 1.5351 min\nEpoch: [10/10] | train_loss: 1.5540 | train_acc: 0.7194 | train_time: 5.6172 min | test loss: 1.5605 | test_acc: 0.7185 | test_time: 1.5643 min\n\nTotal Model Training Time: 66.9688 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStoring Files for Demo App.\n\n\n[INFO] \"demo/human_action_recognition\" and \"demo/human_action_recognition/examples\" directory is been created.\n\n\n\n\ndata/shoot_bow/6arrowswithin30seconds_shoot_bow_f_nm_np1_fr_med_3.avi File is been copied.\ndata/golf/Golf_Tips_-_Hit_The_Driver_300+_Yards!!!_golf_f_nm_np1_fr_med_0.avi File is been copied.\ndata/climb/(HQ)_Rock_Climbing_-_Free_Solo_Speed_Climb_-_Dan_Osman_climb_f_cm_np1_le_med_2.avi File is been copied.\ndata/punch/AdvancedBoxingTechniquesandExercises_punch_u_nm_np1_ba_med_23.avi File is been copied.\ndata/swing_baseball/BaseballHitinSlowMotion_swing_baseball_f_nm_np1_fr_bad_0.avi File is been copied.\n\n\n\n\n[INFO] Saving class names to: \"demo/human_action_recognition/class_names.txt\"\n\n\n\n\n['smoke', 'dive', 'cartwheel', 'throw', 'hit']\n\n\n\n\nSaving and Loading the Model\n\n\n[INFO] Model is been saved to: \"demo/human_action_recognition/mvit_v2_pretrained_model_hmdb51.pth\"\n\n\n\n\n&lt;All keys matched successfully&gt;\n\n\n\n\nCreating a text file for Modules and Packages\n\n\nWriting demo/human_action_recognition/requirements.txt\n\n\n\n\nCreating a Python Script to Preprocess the Data\n\n\nWriting demo/human_action_recognition/utils.py\n\n\n\n\nCreating a Python Script to Create the Model\n\n\nWriting demo/human_action_recognition/model.py\n\n\n\n\nCreating a Python Script for Gradio App\n\n\nWriting demo/human_action_recognition/app.py\n\n\n\n\nDeploying on HuggingFace"
  },
  {
    "objectID": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#results",
    "href": "posts/2023-04-01_pytorch_&_lightning_2.0/index.html#results",
    "title": "Riding the Waves - PyTorch and Lightning 2.0",
    "section": "Results",
    "text": "Results\nSo now that all the experiments are completed, it’s time to check the overall comparison of all the experiment and which one has performed the best.\n\n\n\n\n\n\nNvidia RTX A4000\n\n\n\n\n\n\n\nNvidia Tesla T4"
  }
]