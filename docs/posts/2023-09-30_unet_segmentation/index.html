<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.302">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="John Pinto">
<meta name="dcterms.date" content="2023-09-30">
<meta name="description" content="Building blocks for a strong foundation in image segmentation journey.">

<title>John Pinto - Construct your First Multiclass U-Net in PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6DE2RCBFFM"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6DE2RCBFFM', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  });
});
</script> 
  

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="John Pinto - Construct your First Multiclass U-Net in PyTorch">
<meta property="og:description" content="Building blocks for a strong foundation in image segmentation journey.">
<meta property="og:image" content="https://johnppinto.github.io/blog/posts/2023-09-30_unet_segmentation/featured.jpg">
<meta property="og:site-name" content="John Pinto">
<meta name="twitter:title" content="John Pinto - Construct your First Multiclass U-Net in PyTorch">
<meta name="twitter:description" content="Building blocks for a strong foundation in image segmentation journey.">
<meta name="twitter:image" content="https://johnppinto.github.io/blog/posts/2023-09-30_unet_segmentation/featured.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">John Pinto</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://johnppinto.github.io/" rel="" target="_blank">
 <span class="menu-text">Portfolio</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/JohnPPinto" rel="" target="_blank"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/john-p-pinto/" rel="" target="_blank"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/john77272002" rel="" target="_blank"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml" rel="" target="_blank"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-right">
      <h1 class="title">Construct your First Multiclass U-Net in PyTorch</h1>
                  <div>
        <div class="description">
          Building blocks for a strong foundation in image segmentation journey.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">PyTorch</div>
                <div class="quarto-category">AI</div>
                <div class="quarto-category">Computer Vision</div>
                <div class="quarto-category">Image Segmentation</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>John Pinto </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 30, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-semantic-segmentation-and-u-net" id="toc-what-is-semantic-segmentation-and-u-net" class="nav-link active" data-scroll-target="#what-is-semantic-segmentation-and-u-net">What is Semantic Segmentation and U-Net?</a>
  <ul class="collapse">
  <li><a href="#how-does-it-look" id="toc-how-does-it-look" class="nav-link" data-scroll-target="#how-does-it-look">How does it look?</a>
  <ul class="collapse">
  <li><a href="#encodercontraction-path" id="toc-encodercontraction-path" class="nav-link" data-scroll-target="#encodercontraction-path">Encoder(Contraction Path)</a></li>
  <li><a href="#decoderexpansion-path" id="toc-decoderexpansion-path" class="nav-link" data-scroll-target="#decoderexpansion-path">Decoder(Expansion Path)</a></li>
  <li><a href="#complete-u-net-architecture" id="toc-complete-u-net-architecture" class="nav-link" data-scroll-target="#complete-u-net-architecture">Complete U-Net Architecture</a></li>
  </ul></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a>
  <ul class="collapse">
  <li><a href="#loss" id="toc-loss" class="nav-link" data-scroll-target="#loss">Loss</a></li>
  <li><a href="#accuracy" id="toc-accuracy" class="nav-link" data-scroll-target="#accuracy">Accuracy</a></li>
  <li><a href="#iou-and-dice" id="toc-iou-and-dice" class="nav-link" data-scroll-target="#iou-and-dice">IoU and Dice</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-right" id="quarto-document-content">




<style>
.quarto-figure-center > figure {
  text-align: center;
}
</style>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="featured.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="featured.jpg" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="what-is-semantic-segmentation-and-u-net" class="level1">
<h1>What is Semantic Segmentation and U-Net?</h1>
<p>The field of Computer Vision is quite vast and the general idea is to make the machine able to perceive and understand real-world elements. All the tasks within the field of computer vision have some method to let the machines see the world, among these tasks is semantic segmentation.</p>
<p>Semantic segmentation has a simple objective, to learn and understand each and every pixel that the camera has captured. If you have already been reading and learning about machine learning, then you might know numbers are everything in this field. So, what makes semantic segmentation special is the way it represents the pixel. All the information of the pixel is labeled with a class, whether it’s a binary(0 or 1) or a multiclass(more than 2) problem.</p>
<p>Learning about semantic segmentation and applying this method to solve problems was one of the difficult tasks traditionally until deep learning methods showed up, one of the starting methods to boost this field was called U-Net. U-Net is a deep learning architecture, that was introduced for a task in <a href="https://arxiv.org/pdf/1505.04597.pdf">Biomedical Image Segmentation</a>, the model has given amazing results and even to date it can handle most of the segmentation problem.</p>
<p>In this article, we will learn more about U-Net and construct our own PyTorch implementation of the U-Net architecture.</p>
<section id="how-does-it-look" class="level2">
<h2 class="anchored" data-anchor-id="how-does-it-look">How does it look?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/U-Net.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="images/U-Net.jpg" class="img-fluid figure-img" width="600"></a></p>
</figure>
</div>
<p>The model has a representation same as the name states. A ‘U’ shape model. The reason for the ‘U’ shape comes from the working of the model, the model starts from the left-hand side and moves towards the center compressing the data and keeping only the features of the image, this part of the model is even called an encoder. Then the data moves toward the right-hand corner where the data is reconstructed with all the known features and provided in the form of a mask, this part of the model is called the decoder.</p>
<section id="encodercontraction-path" class="level3">
<h3 class="anchored" data-anchor-id="encodercontraction-path">Encoder(Contraction Path)</h3>
<p>The encoder path takes in the image data and progressively starts performing the downsampling method, this is performed with the help of convolution and pooling layers.</p>
<p>If you see the architecture you might notice a pattern, all the downsample layers have two convolution layers. we will build a class of <code>torch.nn.Module</code> having two convolution layers. While using the convolution layers, we won’t change the spatial dimension by applying padding and along with convolution a batch normalization for bias and ReLU activation to make it non-linear.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">class</span> DoubleConvLayer(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co">    Creates two convolution layers with batch normalization</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co">    and relu activation.</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">    These convolution layers do not change the spatial dimension</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">    and only affects the feature dimension.</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">    If you check the architecture dig., this class creates the</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">    layer indicated by the blue arrow.</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co">    """</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels):</span>
<span id="cb1-11"><a href="#cb1-11"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-12"><a href="#cb1-12"></a>        <span class="va">self</span>.double_conv <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-13"><a href="#cb1-13"></a>            <span class="co"># First convolution layer</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>            nn.Conv2d(in_channels<span class="op">=</span>in_channels,</span>
<span id="cb1-15"><a href="#cb1-15"></a>                      out_channels<span class="op">=</span>out_channels,</span>
<span id="cb1-16"><a href="#cb1-16"></a>                      kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb1-17"><a href="#cb1-17"></a>                      stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-18"><a href="#cb1-18"></a>                      padding<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-19"><a href="#cb1-19"></a>                      bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb1-20"><a href="#cb1-20"></a>            nn.BatchNorm2d(num_features<span class="op">=</span>out_channels),</span>
<span id="cb1-21"><a href="#cb1-21"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb1-22"><a href="#cb1-22"></a></span>
<span id="cb1-23"><a href="#cb1-23"></a>            <span class="co"># Second convolution layer</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>            nn.Conv2d(in_channels<span class="op">=</span>out_channels,</span>
<span id="cb1-25"><a href="#cb1-25"></a>                      out_channels<span class="op">=</span>out_channels,</span>
<span id="cb1-26"><a href="#cb1-26"></a>                      kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb1-27"><a href="#cb1-27"></a>                      stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-28"><a href="#cb1-28"></a>                      padding<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-29"><a href="#cb1-29"></a>                      bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb1-30"><a href="#cb1-30"></a>            nn.BatchNorm2d(num_features<span class="op">=</span>out_channels),</span>
<span id="cb1-31"><a href="#cb1-31"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-32"><a href="#cb1-32"></a>        )</span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-35"><a href="#cb1-35"></a>        <span class="cf">return</span> <span class="va">self</span>.double_conv(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>After the data passes through the convolution layers, it needs to move down, this is done by using the max pooling layer. Once, the data reaches down it can go through the same convolution layer that we created above, a simple model of passing data.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">class</span> DownSampling(nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co">    This class implements the downsampling part of the architecture.</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co">    If you check the architecture dig., the left path displays the red arrow</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co">    indicating the downsampled layer using the max pool.</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co">    """</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels):</span>
<span id="cb2-8"><a href="#cb2-8"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-9"><a href="#cb2-9"></a>        <span class="va">self</span>.down_sample <span class="op">=</span> nn.Sequential(</span>
<span id="cb2-10"><a href="#cb2-10"></a>            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb2-11"><a href="#cb2-11"></a></span>
<span id="cb2-12"><a href="#cb2-12"></a>            <span class="co"># Initializing the layer using the double convolution layer class</span></span>
<span id="cb2-13"><a href="#cb2-13"></a>            DoubleConvLayer(in_channels<span class="op">=</span>in_channels,</span>
<span id="cb2-14"><a href="#cb2-14"></a>                            out_channels<span class="op">=</span>out_channels)</span>
<span id="cb2-15"><a href="#cb2-15"></a>        )</span>
<span id="cb2-16"><a href="#cb2-16"></a></span>
<span id="cb2-17"><a href="#cb2-17"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-18"><a href="#cb2-18"></a>        <span class="cf">return</span> <span class="va">self</span>.down_sample(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>While using downsampling, the model learns the features within the image, but also loses spatial information. So to get the lost data back we need to perform the decoder after this.</p>
</div>
</div>
</section>
<section id="decoderexpansion-path" class="level3">
<h3 class="anchored" data-anchor-id="decoderexpansion-path">Decoder(Expansion Path)</h3>
<p>Once the data reaches the bottom, it needs to move up to increase the spatial dimension and start to select only the most important features, to perform this method we will be using the transposed convolution and following that we will concatenate the downsampling layer with the double convolution. This way the model learns the features and brings them back to the original input image shape.</p>
<p>If you have read the paper, it will be mentioned that they have used the upsampling method which is also great, but the recent method for decoding in segmentation and GAN’s is been done using transposed convolution. so we will stick with the present best method.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">class</span> UpSampling(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co">    This class implements the upsampling part of the architecture.</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co">    If you check the architecture dig., the right path displays the green arrow</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co">    indicating the upsampled layer.</span></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co">    """</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels):</span>
<span id="cb3-8"><a href="#cb3-8"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-9"><a href="#cb3-9"></a>        <span class="co"># Using Transposed convolution for upsampling</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>        <span class="va">self</span>.up_sample <span class="op">=</span> nn.ConvTranspose2d(in_channels<span class="op">=</span>in_channels,</span>
<span id="cb3-11"><a href="#cb3-11"></a>                                            out_channels<span class="op">=</span>out_channels,</span>
<span id="cb3-12"><a href="#cb3-12"></a>                                            kernel_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb3-13"><a href="#cb3-13"></a>                                            stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-14"><a href="#cb3-14"></a>        <span class="va">self</span>.double_conv <span class="op">=</span> DoubleConvLayer(in_channels<span class="op">=</span>in_channels,</span>
<span id="cb3-15"><a href="#cb3-15"></a>                                           out_channels<span class="op">=</span>out_channels)</span>
<span id="cb3-16"><a href="#cb3-16"></a></span>
<span id="cb3-17"><a href="#cb3-17"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x1, x2):</span>
<span id="cb3-18"><a href="#cb3-18"></a>        <span class="co">"""</span></span>
<span id="cb3-19"><a href="#cb3-19"></a><span class="co">        x1 is the output tensor of the previous layer, which will be upsampled.</span></span>
<span id="cb3-20"><a href="#cb3-20"></a><span class="co">        x2 is the skip connection tensor that was generated during</span></span>
<span id="cb3-21"><a href="#cb3-21"></a><span class="co">        downsampling.</span></span>
<span id="cb3-22"><a href="#cb3-22"></a><span class="co">        """</span></span>
<span id="cb3-23"><a href="#cb3-23"></a>        x1 <span class="op">=</span> <span class="va">self</span>.up_sample(x1)</span>
<span id="cb3-24"><a href="#cb3-24"></a></span>
<span id="cb3-25"><a href="#cb3-25"></a>        <span class="co"># Correcting the shape after upsampling</span></span>
<span id="cb3-26"><a href="#cb3-26"></a>        <span class="cf">if</span> x1.shape <span class="op">!=</span> x2.shape:</span>
<span id="cb3-27"><a href="#cb3-27"></a>            x1 <span class="op">=</span> TF.resize(img<span class="op">=</span>x1, size<span class="op">=</span>x2.shape[<span class="dv">2</span>:], antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-28"><a href="#cb3-28"></a></span>
<span id="cb3-29"><a href="#cb3-29"></a>        x <span class="op">=</span> torch.cat((x2, x1), <span class="dv">1</span>)</span>
<span id="cb3-30"><a href="#cb3-30"></a>        <span class="cf">return</span> <span class="va">self</span>.double_conv(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="complete-u-net-architecture" class="level3">
<h3 class="anchored" data-anchor-id="complete-u-net-architecture">Complete U-Net Architecture</h3>
<p>Now, that we have created both the downsampling and upsampling of the model, we can complete the model by assembling the pieces and completing the puzzle.</p>
<ul>
<li>We start with downsampling for 4 layers(including the input layer), every layer increases the features by 2x and decreases the spatial dimension by 2x.</li>
<li>Then we create the bottom convolution layer, which is just meant to pass the data to the upsampling stage.</li>
<li>Once we receive the bottom layer data it moves up to upsampling along with that it takes in the skip connection data from the downsampling and moves up to 4 layers.</li>
<li>Finally reaching the last layers, the data goes through the final layer where the features data is convoluted as per the total number of classes present and shaping the image as per the original shape.</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">class</span> UnetModel(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb4-3"><a href="#cb4-3"></a>                 n_classes: <span class="bu">int</span>,</span>
<span id="cb4-4"><a href="#cb4-4"></a>                 in_channels<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb4-5"><a href="#cb4-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-6"><a href="#cb4-6"></a>        features <span class="op">=</span> [<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>]</span>
<span id="cb4-7"><a href="#cb4-7"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.ModuleList()</span>
<span id="cb4-8"><a href="#cb4-8"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.ModuleList()</span>
<span id="cb4-9"><a href="#cb4-9"></a>        <span class="va">self</span>.skip_connection <span class="op">=</span> []</span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a>        <span class="co"># The input layer of the model [BCHW]</span></span>
<span id="cb4-12"><a href="#cb4-12"></a>        <span class="co"># Eg. input tensor shape: [1, 3, 572, 572]</span></span>
<span id="cb4-13"><a href="#cb4-13"></a>        <span class="va">self</span>.encoder.append(DoubleConvLayer(in_channels<span class="op">=</span>in_channels,</span>
<span id="cb4-14"><a href="#cb4-14"></a>                                            out_channels<span class="op">=</span>features[<span class="dv">0</span>]))</span>
<span id="cb4-15"><a href="#cb4-15"></a>        <span class="co"># Eg. output tensor shape: [1, 64, 572, 572]</span></span>
<span id="cb4-16"><a href="#cb4-16"></a></span>
<span id="cb4-17"><a href="#cb4-17"></a>        <span class="co"># A 3 layer downsampling</span></span>
<span id="cb4-18"><a href="#cb4-18"></a>        <span class="co"># Eg. input tensor shape: [1, 64, 572, 572]</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>        <span class="cf">for</span> feature <span class="kw">in</span> features:</span>
<span id="cb4-20"><a href="#cb4-20"></a>            <span class="va">self</span>.encoder.append(DownSampling(in_channels<span class="op">=</span>feature,</span>
<span id="cb4-21"><a href="#cb4-21"></a>                                             out_channels<span class="op">=</span>feature <span class="op">*</span> <span class="dv">2</span>))</span>
<span id="cb4-22"><a href="#cb4-22"></a>        <span class="co"># Eg. output tensor shape: [1, 512, 71, 71]</span></span>
<span id="cb4-23"><a href="#cb4-23"></a></span>
<span id="cb4-24"><a href="#cb4-24"></a>        <span class="co"># Bottom layer of the UNet Model</span></span>
<span id="cb4-25"><a href="#cb4-25"></a>        <span class="co"># Eg. input tensor shape: [1, 512, 71, 71]</span></span>
<span id="cb4-26"><a href="#cb4-26"></a>        <span class="va">self</span>.bottom_layer <span class="op">=</span> DownSampling(in_channels<span class="op">=</span>features[<span class="op">-</span><span class="dv">1</span>]<span class="op">*</span><span class="dv">2</span>,</span>
<span id="cb4-27"><a href="#cb4-27"></a>                                         out_channels<span class="op">=</span>features[<span class="op">-</span><span class="dv">1</span>]<span class="op">*</span><span class="dv">4</span>)</span>
<span id="cb4-28"><a href="#cb4-28"></a>        <span class="co"># Eg. output tensor shape: [1, 1024, 35, 35]</span></span>
<span id="cb4-29"><a href="#cb4-29"></a></span>
<span id="cb4-30"><a href="#cb4-30"></a>        <span class="co"># A 3-layer upsampling</span></span>
<span id="cb4-31"><a href="#cb4-31"></a>        <span class="co"># Eg. input tensor shape: [1, 1024, 35, 35]</span></span>
<span id="cb4-32"><a href="#cb4-32"></a>        <span class="cf">for</span> feature <span class="kw">in</span> <span class="bu">reversed</span>(features):</span>
<span id="cb4-33"><a href="#cb4-33"></a>            <span class="va">self</span>.decoder.append(UpSampling(in_channels<span class="op">=</span>feature <span class="op">*</span> <span class="dv">4</span>,</span>
<span id="cb4-34"><a href="#cb4-34"></a>                                           out_channels<span class="op">=</span>feature <span class="op">*</span> <span class="dv">2</span>))</span>
<span id="cb4-35"><a href="#cb4-35"></a>        <span class="co"># Eg. output tensor shape: [1, 128, 286, 286]</span></span>
<span id="cb4-36"><a href="#cb4-36"></a></span>
<span id="cb4-37"><a href="#cb4-37"></a>        <span class="co"># Upsampling before the final layer</span></span>
<span id="cb4-38"><a href="#cb4-38"></a>        <span class="co"># Eg. input tensor shape: [1, 128, 286, 286]</span></span>
<span id="cb4-39"><a href="#cb4-39"></a>        <span class="va">self</span>.decoder.append(UpSampling(in_channels<span class="op">=</span>features[<span class="dv">1</span>],</span>
<span id="cb4-40"><a href="#cb4-40"></a>                                       out_channels<span class="op">=</span>features[<span class="dv">0</span>]))</span>
<span id="cb4-41"><a href="#cb4-41"></a>        <span class="co"># Eg. output tensor shape: [1, 64, 572, 572]</span></span>
<span id="cb4-42"><a href="#cb4-42"></a></span>
<span id="cb4-43"><a href="#cb4-43"></a>        <span class="co"># Final layer of the model, giving the predicted mask</span></span>
<span id="cb4-44"><a href="#cb4-44"></a>        <span class="co"># Eg. input tensor shape: [1, 64, 572, 572]</span></span>
<span id="cb4-45"><a href="#cb4-45"></a>        <span class="va">self</span>.final_layer <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span>features[<span class="dv">0</span>],</span>
<span id="cb4-46"><a href="#cb4-46"></a>                                     out_channels<span class="op">=</span>n_classes,</span>
<span id="cb4-47"><a href="#cb4-47"></a>                                     kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-48"><a href="#cb4-48"></a>        <span class="co"># Eg. output tensor shape: [1, n_classes, 572, 572]</span></span>
<span id="cb4-49"><a href="#cb4-49"></a></span>
<span id="cb4-50"><a href="#cb4-50"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-51"><a href="#cb4-51"></a>        <span class="cf">for</span> encode <span class="kw">in</span> <span class="va">self</span>.encoder:</span>
<span id="cb4-52"><a href="#cb4-52"></a>            x <span class="op">=</span> encode(x)</span>
<span id="cb4-53"><a href="#cb4-53"></a>            <span class="co"># Appending all the downsampled output for skip connection</span></span>
<span id="cb4-54"><a href="#cb4-54"></a>            <span class="va">self</span>.skip_connection.append(x)</span>
<span id="cb4-55"><a href="#cb4-55"></a></span>
<span id="cb4-56"><a href="#cb4-56"></a>        x <span class="op">=</span> <span class="va">self</span>.bottom_layer(x)</span>
<span id="cb4-57"><a href="#cb4-57"></a></span>
<span id="cb4-58"><a href="#cb4-58"></a>        <span class="cf">for</span> decode, skip <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.decoder,</span>
<span id="cb4-59"><a href="#cb4-59"></a>                                <span class="bu">reversed</span>(<span class="va">self</span>.skip_connection)):</span>
<span id="cb4-60"><a href="#cb4-60"></a>            <span class="co"># Upsampling using the skip connection that was collected durning downsampling</span></span>
<span id="cb4-61"><a href="#cb4-61"></a>            x <span class="op">=</span> decode(x, skip)</span>
<span id="cb4-62"><a href="#cb4-62"></a></span>
<span id="cb4-63"><a href="#cb4-63"></a>        <span class="cf">return</span> <span class="va">self</span>.final_layer(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>We have successfully constructed the U-Net Model, and with this, our model is ready for semantic segmentation tasks.</p>
<p>To test our model and to know whether it’s capable of performing and producing multiclass masks for any image we will need to train the model on a dataset, to implement this I have already done model training in a notebook, you can check the notebook over <a href="">here</a>.</p>
<p>Implementation is similar to the other task of computer vision, with some minor changes. While building the PyTorch dataset, the mask tensor needs to be labeled encoded. This means that the pixel of the image will have an RGB value representation, this needs to be converted into label representation.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> rgbmask_to_label(rgb_mask: numpy.ndarray,</span>
<span id="cb5-2"><a href="#cb5-2"></a>                     colormap: <span class="bu">list</span>):</span>
<span id="cb5-3"><a href="#cb5-3"></a>        <span class="co">"""</span></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="co">        Converts a single RGB mask into one-hot encoding mask and finally</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co">        creates a single channel class index label.</span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="co">        Parameters:</span></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="co">            rgb_mask: An array containing the mask in RGB format and shape (HWC).</span></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co">            colormap: A list with all the RGB colors for every single class</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co">                      in the proper sequence.</span></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="co">        Returns:</span></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="co">            output: An array that is converted from RGB mask to label encoded in</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="co">                    shape (H x W).</span></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="co">        """</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>        <span class="co"># An array to fill the output later.</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>        output <span class="op">=</span> np.zeros(rgb_mask.shape[:<span class="dv">2</span>])</span>
<span id="cb5-16"><a href="#cb5-16"></a></span>
<span id="cb5-17"><a href="#cb5-17"></a>        <span class="cf">for</span> label, color <span class="kw">in</span> <span class="bu">enumerate</span>(colormap):</span>
<span id="cb5-18"><a href="#cb5-18"></a>            <span class="cf">if</span> label <span class="op">&lt;</span> <span class="bu">len</span>(colormap):</span>
<span id="cb5-19"><a href="#cb5-19"></a>                <span class="co"># Matching the mask with the colormap</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>                <span class="co"># Then replace with classes index</span></span>
<span id="cb5-21"><a href="#cb5-21"></a>                output[np.<span class="bu">all</span>(np.equal(rgb_mask, color), axis<span class="op">=-</span><span class="dv">1</span>)] <span class="op">=</span> label</span>
<span id="cb5-22"><a href="#cb5-22"></a></span>
<span id="cb5-23"><a href="#cb5-23"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>While performing the model training, you will require proper metrics that can represent the performance of the model. This indicator will help us understand, how well the model is learning and improving, I have used the following in the notebook:</p>
<section id="loss" class="level3">
<h3 class="anchored" data-anchor-id="loss">Loss</h3>
<p>While performing the model training the most important factor is to calculate the loss. Usually, the rule of thumb is to use Binary Cross-Entropy Loss for binary classification and Cross-entropy Loss for multiclass classification. In PyTorch, while using the Binary Cross-Entropy Loss there is an option to use raw data(logits) directly or with an activation function like Sigmoid. When working with multiclass you can use PyTorch Cross-Entropy Loss, here only raw data is allowed to be provided to the function, the function itself makes a Softmax calculation. <em>You can learn more about Loss function from this <a href="https://machinelearningmastery.com/loss-functions-in-pytorch-models/">article</a>.</em></p>
</section>
<section id="accuracy" class="level3">
<h3 class="anchored" data-anchor-id="accuracy">Accuracy</h3>
<p>Accuracy is the simplest way to learn about the model performance, but the real truth can be hidden when you see the values given by the accuracy metrics. The reason why accuracy can’t be trusted is because if the problem has an imbalanced dataset then it is very easy to get a higher accuracy score by having all the scores from the majority class. So using accuracy as your only metric is not good enough but want an overall look at the prediction it is better to use accuracy.</p>
<p>The Formula of accuracy is: <span class="math display">\[Accuracy = \dfrac {TP + TN}{TP + TN + FP + FN}\]</span></p>
<p><em>You can read more about accuracy from this <a href="https://deepchecks.com/how-to-check-the-accuracy-of-your-machine-learning-model/">article</a>.</em></p>
</section>
<section id="iou-and-dice" class="level3">
<h3 class="anchored" data-anchor-id="iou-and-dice">IoU and Dice</h3>
<p>Intersection over Union(IoU), also known as the Jaccard Index, is one of the most preferred metrics when working on an object detection problem. Another metric that is also preferred for classification problems is called Dice, also known as F1-Score. These metrics make a clear definition by considering only the true positive for the class it is been calculated while representing mathematically both of the metrics are quite related by they both show different representations for the same class. This is because in general IoU metric tends to penalize the bad class more than the Dice metric, even though they both agree on the same instance. So it is preferred to use one of the metrics from these two.</p>
<p>The reason can be explained by the following equation:</p>
<p>Let, <span class="math display">\[a = TP, b = TP + TN + FP\]</span></p>
<p>Then, <span class="math display">\[IoU = \dfrac {TP}{TP + FP + TN} = \dfrac {a}{b}\]</span></p>
<p>and <span class="math display">\[Dice = \dfrac {TP + TP}{TP + TP + FP + TN} = \dfrac {2a}{a + b}\]</span></p>
<p>Hence, <span class="math display">\[Dice = \dfrac {\dfrac {2a}{b}}{\dfrac {a+b}{b}} = \dfrac {2 \cdot \dfrac {a}{b}}{\dfrac {a}{b} + 1} = \dfrac {2 \cdot IoU}{ IoU + 1}\]</span></p>
<p><em>You can further read about IoU and Dice from this <a href="https://medium.datadriveninvestor.com/deep-learning-in-medical-imaging-3c1008431aaf">article</a></em></p>
<p>Now, that we have constructed the model and learned something about the metrics, it’s time to see whether the model runs successfully, below are the predicted results after running the model for 3 epochs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/prediction.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="images/prediction.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>As you can see the model is been able to identify a large pixel area mask like the paved area(purple) and grass(green) only after training the model for 3 epochs, but still, the smaller pixel area classes will need much longer training time. Due to limited training, the model is currently underfitting which can be improved by training the model for much longer.</p>
<p>You can try my <a href="">notebook</a> and train the model for much longer, and if you use the dataset you will need to create a credentials.py file to store the password provided by the owner of the dataset. You can get the password by visiting the dataset site and following the instructions - website: <a href="https://www.tugraz.at/index.php?id=22387">https://www.tugraz.at/index.php?id=22387</a>.</p>
<p>Thank you for reading my article and I hope that you have enjoyed it. If I have left out something do mention it in the GitHub repo issues or the comments of this article, I am more than happy to improve this article.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<div>
    <hr>
    <h3>Stay in touch</h3>
    <p>If you enjoyed my blog and don't want to miss out on any future articles you can subscribe to my newsletters.</p>
    <iframe src="https://embeds.beehiiv.com/e7421b93-7002-413c-b016-9c72444d77b6?slim=true" data-test-id="beehiiv-embed" height="52" frameborder="0" scrolling="no" style="margin: 0; border-radius: 0px !important; background-color: transparent;">
    </iframe>
    <h3>Click to share</h3>
    <div style="display: inline-flex; align-items: center;">
        <div style="display: flex; margin-top: 10%; margin-right: 10%;">
            <a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-size="large" data-via="john77272002" data-show-count="false">Tweet
            </a>
            <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        </div>
        <div style="display: flex;">
            <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
            <script type="IN/Share" data-url="https://www.linkedin.com"></script>
        </div>
    </div>
    <hr>
</div>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="JohnPPinto/blog" data-repo-id="R_kgDOJTakLg" data-category="General" data-category-id="DIC_kwDOJTakLs4CVlze" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">John Pinto <i class="fa-copyright fa-regular" aria-label="regular"></i> 2023 made with HTML5 <i class="fab fa-html5"></i>, CSS3 <i class="fa-brands fa-css3"></i> and <a href="https://quarto.org">Quarto</a>.</div>   
    <div class="nav-footer-center"><div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","selector":".lightbox","descPosition":"bottom","loop":true,"openEffect":"zoom"});</script>



</body></html>